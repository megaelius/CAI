Abstract. Image guidance has been widely used in radiation therapy. Correctly identifying anatomical
landmarks, like the 12th thoracic vertebra (T12), is the key to success. Until recently, the detection
of those landmarks still requires tedious manual inspections and annotations; and superior-inferior
misalignment to the wrong vertebral body is still relatively common in image guided radiation therapy.
It is necessary to develop an automated approach to detect those landmarks from images. There are
three major challenges to identify T12 vertebra automatically: 1) subtle difference in the structures
with high similarity, 2) limited annotated training data, and 3) high memory usage of 3D networks.
Abstract. In this study, we propose a novel 3D full convolutional network (FCN) that is trained to
detect anatomical structures from 3D volumetric data, requiring only a small amount of training
data. Comparing with existing approaches, the network architecture, target generation and loss
functions were significantly improved to address the challenges specific to medical images. In
our experiments, the proposed network, which was trained from a small amount of annotated images,
demonstrated the capability of accurately detecting structures with high similarity. Furthermore,
the trained network showed the capability of cross-modality learning. This is meaningful in the
situation where image annotations in one modality are easier to obtain than others. The cross-modality
learning ability also indicated that the learned features were robust to noise in different image
modalities. In summary, our approach has a great potential to be integrated into the clinical workflow
to improve the safety of image guided radiation therapy. 