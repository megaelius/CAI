Computational modeling and simulation have become essential tools in the quest to better understand
the brain's makeup and to decipher the causal interrelations of its components. The breadth of biochemical
and biophysical processes and structures in the brain has led to the development of a large variety
of model abstractions and specialized tools, often times requiring high performance computing
resource for their timely execution. What has been missing so far was an in-depth analysis of the
complexity of the computational kernels, hindering a systematic approach to identifying bottlenecks
of algorithms and hardware, and their combinations. If whole brain models are to be achieved on emerging
computer generations, models and simulation engines will have to be carefully co-designed for
the intrinsic hardware tradeoffs. For the first time, we present a systematic exploration based
on analytic performance modeling. We base our analysis on three in silico models, chosen as representative
examples of the most widely employed modeling abstractions. We identify that the synaptic formalism,
i.e. current or conductance based representations, and not the level of morphological detail,
is the most significant factor in determining the properties of memory bandwidth saturation and
shared-memory scaling of in silico models. Even though general purpose computing has, until now,
largely been able to deliver high performance, we find that for all types of abstractions, network
latency and memory bandwidth will become severe bottlenecks as the number of neurons to be simulated
grows. By adapting and extending a performance modeling approach, we deliver a first characterization
of the performance landscape of brain tissue simulations, allowing us to pinpoint current bottlenecks
in state-of-the-art in silico models, and make projections for future hardware and software requirements.
