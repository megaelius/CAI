Inverse problems are pervasive mathematical methods in inferring knowledge from observational
and experimental data by leveraging simulations and models. Unlike direct inference methods,
inverse problem approaches typically require many forward model solves usually governed by Partial
Differential Equations (PDEs). This a crucial bottleneck in determining the feasibility of such
methods. While machine learning (ML) methods, such as deep neural networks (DNNs), can be employed
to learn nonlinear forward models, designing a network architecture that preserves accuracy while
generalizing to new parameter regimes is a daunting task. Furthermore, due to the computation-expensive
nature of forward models, state-of-the-art black-box ML methods would require an unrealistic
amount of work in order to obtain an accurate surrogate model. On the other hand, standard Reduced-Order
Models (ROMs) accurately capture supposedly important physics of the forward model in the reduced
subspaces, but otherwise could be inaccurate elsewhere. In this paper, we propose to enlarge the
validity of ROMs and hence improve the accuracy outside the reduced subspaces by incorporating
a data-driven ML technique. In particular, we focus on a goal-oriented approach that substantially
improves the accuracy of reduced models by learning the error between the forward model and the ROM
outputs. Once an ML-enhanced ROM is constructed it can accelerate the performance of solving many-query
problems in parametrized forward and inverse problems. Numerical results for inverse problems
governed by elliptic PDEs and parametrized neutron transport equations will be presented to support
our approach. 