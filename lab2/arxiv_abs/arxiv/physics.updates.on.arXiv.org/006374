Neural networks have become increasingly prevalent within the geosciences, although a common
limitation of their usage has been a lack of methods to interpret what the networks learn and how they
make decisions. As such, neural networks have often been used within the geosciences to most accurately
identify a desired output given a set of inputs, with the interpretation of what the network learns
used as a secondary metric to ensure the network is making the right decision for the right reason.
Neural network interpretation techniques have become more advanced in recent years, however,
and we therefore propose that the ultimate objective of using a neural network can also be the interpretation
of what the network has learned rather than the output itself. We show that the interpretation of
neural networks can enable the discovery of scientifically meaningful connections within geoscientific
data. In particular, we use two methods for neural network interpretation called backwards optimization
and layerwise relevance propagation, both of which project the decision pathways of a network back
onto the original input dimensions. To the best of our knowledge, LRP has not yet been applied to geoscientific
research, and we believe it has great potential in this area. We show how these interpretation techniques
can be used to reliably infer scientifically meaningful information from neural networks by applying
them to common climate patterns. These results suggest that combining interpretable neural networks
with novel scientific hypotheses will open the door to many new avenues in neural network-related
geoscience research. 