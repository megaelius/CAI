Machine learning has become a widely popular and successful paradigm, including in data-driven
science and engineering. A major application problem is the forecasting of future states from a
complex dynamical, given a cache of data representing time ordered observation of states from the
system. Artificial neural networks (ANN) have evolved as a clear leader amongst many machine learning
approaches, and recurrent neural networks (RNN) are especially well suited due to an aspect of memory
associated with the concept, even if the major step of training the RNN to data which typically involves
backpropagation and optimization becomes computationally especially intensive. In this setting,
the echo state networks (ESN) or reservoir computer (RC) have emerged for their simplicity since
they are a special case of an RNN, which are not fully trained to the data. Instead only the readout
weights are trained but read-in weights and internal weights are simply selected randomly. However,
the read-out weights can be trained by a simple least squares step and so it is simple and efficient
to train. There remains an unanswered question as to why and how an RC works at all, with randomly selected
weights. To this end this work analyzes a further simplified RC. Specifically we are then able to
connect the RC to the well developed time-series literature on vector autoregressive averages
(VAR) that includes theorems on representability through the WOLD theorem. Further we can associate
this paradigm with the now widely popular dynamic mode decomposition (DMD), and thus these three
are in a sense different faces of the same thing. Our simplification is not presented for sake of tuning
or improving an RC, but rather for sake of analysis the surprise being not that it doesn't work better
but that such random methods work at all. 