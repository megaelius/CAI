The study of large and complex datasets, or big data, organized as networks has emerged as one of the
central challenges in most areas of science and technology. Cellular and molecular networks in
biology is one of the prime examples. Henceforth, a number of techniques for data dimensionality
reduction, especially in the context of networks, have been developed. Yet, current techniques
require a predefined metric upon which to minimize the data size. Here we introduce a family of parameter-free
algorithms based on (algorithmic) information theory that are designed to minimize the loss of
any (enumerable computable) property contributing to the object's algorithmic content and thus
important to preserve in a process of data dimension reduction when forcing the algorithm to delete
first the least important features. Being independent of any particular criterion, they are universal
in a fundamental mathematical sense. Using suboptimal approximations of efficient (polynomial)
estimations we demonstrate how to preserve network properties outperforming other (leading)
algorithms for network dimension reduction. Our method preserves all graph-theoretic indices
measured, ranging from degree distribution, clustering-coefficient, edge betweenness, and
degree and eigenvector centralities. We conclude and demonstrate numerically that our parameter-free,
Minimal Information Loss Sparsification (MILS) method is robust, has the potential to maximize
the preservation of all recursively enumerable features in data and networks, and achieves equal
to significantly better results than other data reduction and network sparsification methods.
