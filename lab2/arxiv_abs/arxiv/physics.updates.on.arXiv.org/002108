In this work we explore globally optimal multilayer convolution models such as feed forward neural-networks
(FFNN) for learning and predicting dynamics from transient fluid flow data. While machine learning
in general depends on data quality relative to the underlying dynamics of the system, it is important
for a given data-driven learning architecture to make the most of this available information. To
this end, we cast the suite of recently popular data-driven learning approaches that approximate
the Markovian dynamics through a linear model in a higher-dimensional feature space as a multilayer
architecture similar to neural networks, but with layer-wise locally optimal convolution mappings.
As a contrast, we also represent the traditional neural networks with some slight modifications
as a multilayer architecture, but with convolution maps optimized in response to the global learning
cost (i.e. not the cost of learning across two immediate layers). We show through examples of data-driven
learning of canonical fluid flows that globally optimal FFNN-like methods owe their success to
leveraging the extended learning parameter space available in multilayer models to achieve a common
goal of minimizing the training cost function while incorporating nonlinear function maps between
layers. On the other hand, locally optimal multilayer models also show improvement from the same
factors, but behave like shallow neural networks requiring much larger hidden layers to achieve
comparable learning and prediction accuracy. We illustrate these ideas by learning the dynamics
from snapshots of training data and predicting the temporal evolution of canonical nonlinear fluid
flows. 