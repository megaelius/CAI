Background properties in experimental particle physics are typically estimated using large collections
of events. However, both the quantum mechanical nature of the underlying physics and statistical
fluctuations can lead to different events exhibiting appreciably-different features. Although
traditional background estimation techniques based on high-statistics control samples can provide
a precise description of average background distributions, they are typically unable to describe
deviations of the shapes of probability distributions in small data sets from the corresponding
high-statistics templates. From a physics analysis point of view, not taking such deviations into
account when subtracting background can translate into increased systematic uncertainties and
into degraded resolution of observables of interest. This article proposes a novel algorithm inspired
by the Gibbs sampler that builds on a population-based view of particle physics data. Events are
treated as heterogeneous statistical populations comprising particles originating from different
processes such as a hard scattering of interest as opposed to background associated with low-energy
strong interactions. The algorithm estimates the shapes of signal and background probability
density functions from a given collection of particles by sampling from a posterior probability
distribution that encodes information on which particles are more likely to originate from either
process. Results on Monte Carlo data are presented, and the prospects for the development of tools
for intensive offline analysis of individual events at the Large Hadron Collider are discussed.
