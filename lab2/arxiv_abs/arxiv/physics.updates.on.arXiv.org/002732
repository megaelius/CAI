Because patients anatomy may change during radiation therapy, using the most up-to-date computed
tomography (CT) is essential in online adaptive radiation therapy. However, CT-on-rails are not
readily available in every clinic because of their high cost. On the other hand, cone beam computed
tomography (CBCT), which is adopted frequently during the treatment process, is used for both patient
positioning and adaptive radiation therapy. CBCT cannot be used directly for dose calculation
without being modified because of the high number of artefacts and the Hounsfield unit (HU) values
are incorrect. In contrast, if CBCT images are converted to CT images, it holds a great potential
by bringing the most up-to-date CT images in adaptive radiation therapy efficiently and economically.
Recently, deep learning has achieved great success in image-to-image translation tasks. Supervised
learning requires a paired training dataset, while unsupervised learning only needs an unpaired
training dataset. Due to the inter-fractional and intra-fractional anatomical changes, it is
very difficult to acquire paired CT and CBCT images for training. To overcome this limitation, a
cycle generative adversarial network (CycleGAN) which is an unsupervised learning method was
developed to train our CBCT-to-CT conversion model. The synthesized CT generated from CBCT through
CycleGAN obtained visual and quantitative results similar to real CT images. Dose distribution
on synthesized CT demonstrated a higher pass rate than CBCT in 3D gamma index. We also compared the
CycleGAN model with other unsupervised learning methods including deep convolutional generative
adversarial network (DCGAN) and progressive growing of GANs, showing that CycleGAN outperformed
the other two methods. 