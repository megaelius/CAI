In this paper we show that deep convolutional neural networks (CNN) can massively outperform traditional
densely connected neural networks (both deep or shallow) in predicting eigenvalue problems in
mechanics. In this sense, we strike out in a novel direction in mechanics computations with strongly
predictive neural networks whose success depends not only neural architectures being deep but
also being fundamentally different from neural architectures which have been used in mechanics
till now. To show this, we consider a model problem: predicting the eigenvalues of a 1-D phononic
crystal, however, the general observations pertaining to the predictive superiority of CNNs over
MLPs should extend to other problems in mechanics as well. In the present problem, the optimal CNN
architecture reaches $98\%$ accuracy level on unseen data when trained with just 20,000 training
samples. Fully-connected multi-layer perceptrons (MLP) - the network of choice in mechanics research
- on the other hand, does not improve beyond $85\%$ accuracy even with $100,000$ training samples.
We also show that even with a relatively small amount of training data, CNNs have the capability to
generalize well for our problems and that they automatically learn deep symmetry operations such
as translational invariance. Most importantly, however, we show how CNNs can naturally represent
mechanical material tensors and that the convolution operation of CNNs has the ability to serve
as local receptive fields which is a natural representation of mechanical response. Strategies
proposed here may be used for other problems of mechanics and may, in the future, be used to completely
sidestep certain cumbersome algorithms with a purely data driven approach based upon deep architectures
of modern neural networks such as deep CNNs. 