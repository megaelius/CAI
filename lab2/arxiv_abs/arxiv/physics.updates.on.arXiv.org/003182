Many experiments in the field of quantum foundations seek to adjudicate between quantum theory
and speculative alternatives to it. This requires one to analyze the experimental data in a manner
that does not presume the correctness of the quantum formalism. The mathematical framework of generalized
probabilistic theories (GPTs) provides a means of doing so. We present a scheme for determining
which GPTs are consistent with a given set of experimental data. It proceeds by performing tomography
on the preparations and measurements in a self-consistent manner, i.e., without presuming a prior
characterization of either. We illustrate the scheme by analyzing experimental data for a large
set of preparations and measurements on the polarization degree of freedom of a single photon. We
find that the smallest and largest GPT state spaces consistent with our data are a pair of polytopes,
each approximating the shape of the Bloch Sphere and having a volume ratio of $0.977 \pm 0.001$, which
provides a quantitative bound on the scope for deviations from quantum theory. We also demonstrate
how our scheme can be used to bound the extent to which nature might be more nonlocal than quantum theory
predicts, as well as the extent to which it might be more or less contextual. Specifically, we find
that the maximal violation of the CHSH inequality can be at most $1.3\% \pm 0.1$ greater than the quantum
prediction, and the maximal violation of a particular inequality for universal noncontextuality
can not differ from the quantum prediction by more than this factor on either side. The most significant
loophole in this sort of analysis is that the set of preparations and measurements one implements
might fail to be tomographically complete for the system of interest. 