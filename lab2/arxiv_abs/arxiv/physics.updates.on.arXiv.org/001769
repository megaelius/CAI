What are the distinct ways in which a set of predictor variables can provide information about a target
variable? When does a variable provide unique information, when do variables share redundant information,
and when do variables combine synergistically to provide complementary information? The redundancy
lattice from the partial information decomposition of Williams and Beer provided a promising glimpse
at the answer to these questions. However, this structure was constructed using a much criticised
measure of redundant information, and despite sustained research, no completely satisfactory
replacement measure has been proposed. In this paper, we take a different approach, applying the
axiomatic derivation of the redundancy lattice to a single realisation from a set of discrete variables.
To overcome the difficulty associated with signed pointwise mutual information, we apply this
decomposition separately to the unsigned entropic components of pointwise mutual information
which we refer to as the specificity and ambiguity. This yields a separate redundancy lattice for
each component. Then based upon an operational interpretation of redundancy, we define measures
of redundant specificity and ambiguity enabling us to evaluate the partial information atoms in
each lattice. These atoms can be recombined to yield the sought-after multivariate information
decomposition. We apply this framework to canonical examples from the literature and discuss the
results and the various properties of the decomposition. In particular, the pointwise decomposition
using specificity and ambiguity satisfies a chain rule over target variables, which provides new
insights into the so-called two-bit-copy example. 