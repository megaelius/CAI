We develop a data-driven framework for extracting complex spatiotemporal patterns generated
by ergodic dynamical systems. Our approach, called Vector-valued Spectral Analysis (VSA), is
based on an eigendecomposition of a kernel integral operator acting on a Hilbert space of vector-valued
observables of the system, taking values in a space of functions (scalar fields) on a spatial domain.
This operator is constructed by combining aspects of the theory of operator-valued kernels for
machine learning with delay-coordinate maps of dynamical systems. Specifically, delay-coordinate
maps performed pointwise in the spatial domain induce an operator acting on functions on that domain
for each pair of dynamical states. Unlike conventional eigendecomposition techniques, which
decompose the input data into pairs of temporal and spatial modes with a separable, tensor product
structure, the patterns recovered by VSA can be manifestly non-separable, requiring only a modest
number of modes to represent signals with intermittency in both space and time. In addition, our
kernel construction naturally quotients out any symmetries present in the data. We show that in
the limit of infinitely many delays the kernel integral operator employed in our scheme commutes
with a Koopman operator governing the evolution of vector-valued observables under the dynamics;
as a result, in that limit our recovered patterns lie in simultaneous eigenspaces of these operators
associated with the point spectrum of the dynamical system. We present applications of VSA to the
Kuramoto-Sivashinsky model, which demonstrate considerable performance gains in efficient
and meaningful decomposition over eigendecomposition techniques utilizing scalar-valued kernels.
