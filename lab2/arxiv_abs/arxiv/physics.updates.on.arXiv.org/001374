A principled approach to characterize the hidden structure of networks is to formulate generative
models, and then infer their parameters from data. When the desired structure is composed of modules
or "communities", a suitable choice for this task is the stochastic block model (SBM), where nodes
are divided into groups, and the placement of edges is conditioned on the group memberships. Here,
we present a nonparametric Bayesian method to infer the modular structure of empirical networks,
including the number of modules and their hierarchical organization. We focus on a microcanonical
variant of the SBM, where the structure is imposed via hard constraints, i.e. the generated networks
are not allowed to violate the patterns imposed by the model. We show how this simple model variation
allows simultaneously for two important improvements over more traditional inference approaches:
1. Deeper Bayesian hierarchies, with noninformative priors replaced by sequences of priors and
hyperpriors, that not only remove limitations that seriously degrade the inference on large networks,
but also reveal structures at multiple scales; 2. A very efficient inference algorithm that scales
well not only for networks with a large number of nodes and edges, but also with an unlimited number
of modules. We show also how this approach can be used to sample modular hierarchies from the posterior
distribution, as well as to perform model selection. We discuss and analyze the differences between
sampling from the posterior and simply finding the single parameter estimate that maximizes it.
Furthermore, we expose a direct equivalence between our microcanonical approach and alternative
derivations based on the canonical SBM. 