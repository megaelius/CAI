The objective of this research is to develop techniques for assimilating GOES-R Series observations
in precipitating scenes for the purpose of improving short-term convective-scale forecasts of
high impact weather hazards. Whereas one approach is radiance assimilation, the information content
of GOES-R radiances from its Advanced Baseline Imager (ABI) saturates in precipitating scenes,
and radiance assimilation does not make use of lightning observations from the GOES Lightning Mapper
(GLM). Here, a convolutional neural network (CNN) is developed to transform GOES-R radiances and
lightning into synthetic radar reflectivity fields to make use of existing radar assimilation
techniques. We find that the ability of CNNs to utilize spatial context is essential for this application
and offers breakthrough improvement in skill compared to traditional pixel-by-pixel based approaches.
To understand the improved performance, we use a novel analysis methodology that combines several
techniques, each providing different insights into the network's reasoning. Channel withholding
experiments and spatial information withholding experiments are used to show that the CNN achieves
skill at high reflectivity values from the information content in radiance gradients and the presence
of lightning. The attribution method, layer-wise relevance propagation, demonstrates that the
CNN uses radiance and lightning information synergistically, where lightning helps the CNN focus
on which neighboring locations are most important. Synthetic inputs are used to quantify the sensitivity
to radiance gradients, showing that sharper gradients produce a stronger response in predicted
reflectivity. Finally, geostationary lightning observations are found to be uniquely valuable
for their ability to pinpoint locations of strong radar echoes. 