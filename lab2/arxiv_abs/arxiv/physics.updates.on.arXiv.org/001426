What are the distinct ways in which a set of predictor variables can provide information about a target
variable? When does a variable provide unique information, when do variables share redundant information,
and when do variables combine to provide complementary information? The redundancy lattice from
the partial information decomposition of Williams and Beer provided a promising glimpse at the
answer to these questions; however, this structure was constructed using a much criticised measure
of redundant information. Despite much research effort, no satisfactory replacement measure
has been proposed. This paper takes a different approach, applying the axiomatic derivation of
the redundancy lattice to a single realisation from the set of variables. In order to do this, one
must overcome the difficulties associated with signed pointwise mutual information. This is done
by applying the decomposition separately to the non-negative entropic components of the pointwise
mutual information, which we refer to as the specificity and the ambiguity. Then, based upon an operational
interpretation of redundancy, measures of redundant specificity and ambiguity are defined. It
is shown that the decomposed specificity and ambiguity can be recombined to yield the sought-after
information decomposition. The decomposition is applied to canonical examples from the literature
and its various properties are discussed. In particular, the pointwise decomposition using specificity
and ambiguity satisfies a chain rule over target variables, which provides new insights into interpreting
the well-known two-bit copy example. 