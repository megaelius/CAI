We propose an approach to rapidly find the upper limit of separability between datasets that is directly
applicable to HEP classification problems. The most common HEP classification task is to use $n$
values (variables) for an object (event) to estimate the probability that it is signal vs. background.
Most techniques first use known samples to identify differences in how signal and background events
are distributed throughout the $n$-dimensional variable space, then use those differences to
classify events of unknown type. Qualitatively, the greater the differences, the more effectively
one can classify events of unknown type. We will show that the Mutual Information (MI) between the
$n$-dimensional signal-background mixed distribution and the answers for the known events, tells
us the upper-limit of separation for that set of $n$ variables. We will then compare that value to
the Jensen-Shannon Divergence between the output distributions from a classifier to test whether
it has extracted all possible information from the input variables. We will also discuss speed improvements
to a standard method for calculating MI. Our approach will allow one to: a) quickly measure the maximum
possible effectiveness of a large number of potential discriminating variables independent of
any specific classification algorithm, b) identify potential discriminating variables that
are redundant, and c) determine whether a classification algorithm has achieved the maximum possible
separation. We test these claims first on simple distributions and then on Monte Carlo samples generated
for Supersymmetry and Higgs searches. In all cases, we were able to a) predict the separation that
a classification algorithm would reach, b) identify variables that carried no additional discriminating
power, and c) identify whether an algorithm had reached the optimum separation. Our code is publicly
available. 