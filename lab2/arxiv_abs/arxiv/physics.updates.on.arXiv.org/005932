This is a talk given at Computers in High Energy Physics in Adelaide, South Australia, Australia
in November 2019. It is partially intended to explain the context of DUNE Computing for computing
specialists. The DUNE collaboration consists of over 180 institutions from 33 countries. The experiment
is in preparation now with commissioning of the first 10kT fiducial volume Liquid Argon TPC expected
over the period 2025-2028 and a long data taking run with 4 modules expected from 2029 and beyond.
An active prototyping program is already in place with a short test beam run with a 700T, 15,360 channel
prototype of single-phase readout at the neutrino platform at CERN in late 2018 and tests of a similar
sized dual-phase detector scheduled for mid-2019. The 2018 test beam run was a valuable live test
of our computing model. The detector produced raw data at rates of up to ~2GB/s. These data were stored
at full rate on tape at CERN and Fermilab and replicated at sites in the UK and Czech Republic. In total
1.2 PB of raw data from beam and cosmic triggers were produced and reconstructed during the six week
test beam run. Baseline predictions for the full DUNE detector data, starting in the late 2020's
are 30-60 PB of raw data per year. In contrast to traditional HEP computational problems, DUNE's
Liquid Argon TPC data consist of simple but very large (many GB) 2D data objects which share many characteristics
with astrophysical images. This presents opportunities to use advances in machine learning and
pattern recognition as a frontier user of High Performance Computing facilities capable of massively
parallel processing. 