An increasing body of evidence suggests that the trial-to-trial variability of spiking activity
in the brain is not mere noise, but rather the reflection of a sampling-based encoding scheme for
probabilistic computing. Since the precise statistical properties of neural activity are important
in this context, many models assume an ad-hoc source of well-behaved, explicit noise, either on
the input or on the output side of single neuron dynamics, most often assuming an independent Poisson
process in either case. However, these assumptions are somewhat problematic: neighboring neurons
tend to share receptive fields, rendering both their input and their output correlated; at the same
time, neurons are known to behave largely deterministically, as a function of their membrane potential
and conductance. We suggest that spiking neural networks may, in fact, have no need for noise to perform
sampling-based Bayesian inference. We study analytically the effect of auto- and cross-correlations
in functionally Bayesian spiking networks and demonstrate how their effect translates to synaptic
interaction strengths, rendering them controllable through synaptic plasticity. This allows
even small ensembles of interconnected deterministic spiking networks to simultaneously and
co-dependently shape their output activity through learning, enabling them to perform complex
Bayesian computation without any need for noise, which we demonstrate in silico, both in classical
simulation and in neuromorphic emulation. These results close a gap between the abstract models
and the biology of functionally Bayesian spiking networks, effectively reducing the architectural
constraints imposed on physical neural substrates required to perform probabilistic computing,
be they biological or artificial. 