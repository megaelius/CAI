Turbulent convection is thought to act as an effective viscosity in damping equilibrium tidal flows,
driving spin and orbital evolution in close convective binary systems. Compared to mixing-length
predictions, this viscosity ought to be reduced when the tidal frequency $|\omega_t|$ exceeds
the turnover frequency $\omega_{c\nu}$ of the dominant convective eddies, but the efficiency
of this reduction has been disputed. We reexamine this long-standing controversy using direct
numerical simulations of an idealized global model. We simulate thermal convection in a full sphere,
and externally forced by the equilibrium tidal flow, to measure the effective viscosity $\nu_E$
acting on the tidal flow when $|\omega_t|/\omega_{c\nu} \gtrsim 1$. We demonstrate that the frequency
reduction of $\nu_E$ is correlated with the frequency spectrum of the (unperturbed) convection.
For intermediate frequencies below those in the turbulent cascade ($|\omega_t|/\omega_{c\nu}
\sim 1-5$), the frequency spectrum displays an anomalous $1/\omega^\alpha$ power law that is responsible
for the frequency-reduction $\nu_E \propto 1/|\omega_t|^{\alpha}$, where $\alpha < 1$ depends
on the model parameters. We then get $|\nu_E| \propto 1/|\omega_t|^{\delta}$ with $\delta > 1$
for higher frequencies, and $\delta=2$ is obtained for a Kolmogorov turbulent cascade. A generic
$|\nu_E| \propto 1/|\omega_t|^{2}$ suppression is next found for higher frequencies within the
dissipation range of the convection (but with negative values). Our results indicate that a better
knowledge of the frequency spectrum of convection is necessary to accurately predict the efficiency
of tidal dissipation in stars and planets resulting from this mechanism. 