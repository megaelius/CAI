We consider stochastic convex optimization problems with affine constraints and develop several
methods using either primal or dual approach to solve it. In the primal case, we use special penalization
technique to make the initial problem more convenient for using optimization methods. We propose
algorithms to solve it based on Similar Triangles Method with Inexact Proximal Step for the convex
smooth and strongly convex smooth objective functions and methods based on Gradient Sliding algorithm
to solve the same problems in the non-smooth case. We prove the convergence guarantees in the smooth
convex case with deterministic first-order oracle. We propose and analyze three novel methods
to handle stochastic convex optimization problems with affine constraints: SPDSTM, R-RRMA-AC-SA$^2$
and SSTM_sc. All methods use stochastic dual oracle. SPDSTM is the stochastic primal-dual modification
of STM and it is applied for the dual problem when the primal functional is strongly convex and Lipschitz
continuous on some ball. We extend the result from Dvinskikh & Gasnikov (2019) for this method to
the case when only biased stochastic oracle is available. R-RRMA-AC-SA$^2$ is an accelerated stochastic
method based on the restarts of RRMA-AC-SA$^2$ from Foster et al. (2019) and SSTM_sc is just stochastic
STM for strongly convex problems. Both methods are applied to the dual problem when the primal functional
is strongly convex, smooth and Lipschitz continuous on some ball and use stochastic dual first-order
oracle. We develop convergence analysis for these methods for the unbiased and biased oracles respectively.
Finally, we apply all aforementioned results and approaches to solve the decentralized distributed
optimization problem and discuss the optimality of the obtained results in terms of communication
rounds and the number of oracle calls per node. 