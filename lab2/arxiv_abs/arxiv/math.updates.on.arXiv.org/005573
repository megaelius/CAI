We consider two-player zero-sum differential games (ZSDGs), where the state process (dynamical
system) depends on the random initial condition and the state process's distribution, and the objective
functional includes the state process's distribution and a random target variable. Examples of
such ZSDGs would be mean-field games, mean-field type control, empirical risk optimization in
statistical learning, and pursuit-evasion games. Unlike ZSDGs studied in the existing literature,
where the value function is a function on a finite-dimensional state space, the class of ZSDGs of
this paper introduces a new technical challenge, since the corresponding (lower and upper) value
functions are defined on $\mathcal{P}_2$ (the set of probability measures with finite second moments)
or $\mathcal{L}_2$ (the set of random variables with finite second moments), both of which are infinite-dimensional
spaces. We show that the (lower and upper) value functions on $\mathcal{P}_2$ and $\mathcal{L}_2$
are equivalent to each other and continuous, where the former implies that they are law invariant.
Then we prove that these value functions satisfy dynamic programming principles. We use the notion
of derivative of a function of probability measures in $\mathcal{P}_2$ and its lifted version in
$\mathcal{L}_2$ to show that the (lower and upper) value functions are viscosity solutions to the
associated (lower and upper) Hamilton-Jacobi-Isaacs (HJI) equations that are (infinite-dimensional)
first-order partial differential equations on $\mathcal{P}_2$ and $\mathcal{L}_2$. When the
dynamics and running cost are independent of time, the value function is a unique viscosity solution
of the corresponding HJI equation and under the Isaacs condition, the ZSDG has a value. Numerical
examples are provided to illustrate the theoretical results of the paper. 