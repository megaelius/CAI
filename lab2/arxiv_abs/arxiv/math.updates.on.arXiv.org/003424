This paper connects causal discovery under the influence of hidden variables with switching regression
models. Given a response $Y$ and a vector $X = (X^1,\dots,X^d)$ of $d$ predictors, we investigate
the problem of inferring direct causes of $Y$ among the vector $X$. Models for $Y$ that use all of $Y$'s
causal covariates as predictors enjoy the property of being invariant across different environments.
Given data from such environments, this property has been exploited for causal discovery: one collects
the models that show predictive stability across all environments and outputs the set of predictors
that are necessary to obtain stability. If some of the direct causes are latent, however, the above
reasoning breaks down. In this paper, we propose a relaxed version of the invariance assumption,
which can be used for causal discovery in the presence of latent variables with a low-range discrete
influence on the target $Y$. This assumption gives rise to switching regression models, where each
value of the (unknown) hidden variable corresponds to a different regression coefficient. We provide
sufficient conditions for the existence, consistency and asymptotic normality of the maximum
likelihood estimator in linear switching regression models with Gaussian noise, and construct
a test for the equality of such models. Our results on switching regression models allow us to prove
that asymptotic false discovery control for the causal discovery method is obtained under mild
conditions. The method can further be used to infer the states of the hidden variables, yielding
a process-based classification of the data points. We provide an algorithm, make available code,
and illustrate the performance of our method on simulated data. We further apply the method to a real
data set on estimation of photosynthetic activity and evaluate how well the inferred classification
reconstructs the vegetation type. 