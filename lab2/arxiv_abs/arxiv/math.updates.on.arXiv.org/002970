In recent years deep artificial neural networks (DNNs) have very successfully been employed in
numerical simulations for a multitude of computational problems including, for example, object
and face recognition, natural language processing, fraud detection, computational advertisement,
and numerical approximations of partial differential equations (PDEs). Such numerical simulations
indicate that DNNs seem to admit the fundamental flexibility to overcome the curse of dimensionality
in the sense that the number of real parameters used to describe the DNN grows at most polynomially
in both the reciprocal of the prescribed approximation accuracy $ \varepsilon > 0 $ and the dimension
$ d \in \mathbb{N} $ of the function which the DNN aims to approximate in such computational problems.
There is also a large number of rigorous mathematical approximation results for artificial neural
networks in the scientific literature but there are only a few special situations where results
in the literature can rigorously explain the success of DNNs when approximating high-dimensional
functions. The key contribution of this article is to reveal that DNNs do overcome the curse of dimensionality
in the numerical approximation of Kolmogorov PDEs with constant diffusion and nonlinear drift
coefficients. We prove that the number of parameters used to describe the employed DNN grows at most
polynomially in both the reciprocal of the prescribed approximation accuracy $ \varepsilon > 0
$ and the PDE dimension $ d \in \mathbb{N} $. A crucial ingredient in our proof is the fact that the artificial
neural network used to approximate the solution of the PDE is indeed a deep artificial neural network
with a large number of hidden layers. 