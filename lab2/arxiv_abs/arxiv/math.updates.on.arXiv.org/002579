We consider an $\ell_0$-minimization problem where $f(x) + \gamma \|x\|_0$ is minimized over a
polyhedral set and the $\ell_0$-norm penalty (or regularizer) implicitly emphasizes sparsity
of the solution. Such a setting captures a range of problems in image processing and statistical
learning. However, convex regularizers are often employed as substitutes of this norm, and far
less is known about directly solving the $\ell_0$-minimization problem. Inspired by [20], we consider
the resolution of an equivalent formulation of the $\ell_0$-minimization problem as a mathematical
program with complementarity constraints (MPCC) and make the following contributions towards
the characterization and computation of KKT points from such a formulation: (i) First, we show that
feasible points of this formulation satisfy the relatively weak Guignard CQ while a nondegenerate
feasible point satisfies the Abadie CQ. Furthermore, under suitable convexity assumptions on
$f(x)$, an equivalence is derived between first, second-order KKT points, and local minimizers
of the MPCC formulation. (ii) Next, we consider the resolution of the MPCC formulation by ADMM where
each subproblem is tractable, in contrast with many available ADMM schemes for nonconvex problems
[37,28]. Specifically, in spite of nonconvexity, we show that one of the ADMM updates can be effectively
reduced to a closed-form expression by recognizing a hidden convexity property while the other
necessitates solving a convex program. In this framework, we prove subsequential convergence
under certain assumptions and overall convergence under the K-L property. A perturbed variant
of this scheme is also proposed for which convergence can be claimed under milder assumptions. Preliminary
numerics suggest that proposed ADMM schemes may significantly outperform their standard nonconvex
ADMM counterparts. 