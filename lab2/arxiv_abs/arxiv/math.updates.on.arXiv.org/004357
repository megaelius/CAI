Building on a recent framework for distributionally robust optimization in machine learning,
we develop a similar framework for estimation of the inverse covariance matrix for multivariate
data. We provide a novel notion of a Wasserstein ambiguity set specifically tailored to this estimation
problem, from which we obtain a representation for a tractable class of regularized estimators.
Special cases include penalized likelihood estimators for Gaussian data, specifically the graphical
lasso estimator. As a consequence of this formulation, a natural relationship arises between the
radius of the Wasserstein ambiguity set and the regularization parameter in the estimation problem.
Using this relationship, one can directly control the level of robustness of the estimation procedure
by specifying a desired level of confidence with which the ambiguity set contains a distribution
with the true population covariance. Furthermore, a unique feature of our formulation is that the
radius can be expressed in closed-form as a function of the ordinary sample covariance matrix. Taking
advantage of this finding, we develop a simple algorithm to determine a regularization parameter
for graphical lasso, using only the bootstrapped sample covariance matrices, meaning that computationally
expensive repeated evaluation of the graphical lasso algorithm is not necessary. Alternatively,
the distributionally robust formulation can also quantify the robustness of the corresponding
estimator if one uses an off-the-shelf method such as cross-validation. Finally, we numerically
study the obtained regularization criterion and analyze the robustness of other automated tuning
procedures used in practice. 