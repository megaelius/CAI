Nonparametric methods play a central role in modern empirical work. While they provide inference
procedures that are more robust to parametric misspecification bias, they may be quite sensitive
to tuning parameter choices. We study the effects of bias correction on confidence interval coverage
in the context of kernel density and local polynomial regression estimation, and prove that bias
correction can be preferred to undersmoothing for minimizing coverage error and increasing robustness
to tuning parameter choice. This is achieved using a novel, yet simple, Studentization, which leads
to a new way of constructing kernel-based bias-corrected confidence intervals. In addition, for
practical cases, we derive coverage error optimal bandwidths and discuss easy-to-implement bandwidth
selectors. For interior points, we show that the MSE-optimal bandwidth for the original point estimator
(before bias correction) delivers the fastest coverage error decay rate after bias correction
when second-order (equivalent) kernels are employed, but is otherwise suboptimal because it is
too "large". Finally, for odd-degree local polynomial regression, we show that, as with point estimation,
coverage error adapts to boundary points automatically when appropriate Studentization is used;
however, the MSE-optimal bandwidth for the original point estimator is suboptimal. All the results
are established using valid Edgeworth expansions and illustrated with simulated data. Our findings
have important consequences for empirical work as they indicate that bias-corrected confidence
intervals, coupled with appropriate standard errors, have smaller coverage error and are less
sensitive to tuning parameter choices in practically relevant cases where additional smoothness
is available. 