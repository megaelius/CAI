This thesis is concerned with the design of distributed algorithms for solving optimization problems.
We consider networks where each node has exclusive access to a cost function, and design algorithms
that make all nodes cooperate to find the minimum of the sum of all the cost functions. Several problems
in signal processing, control, and machine learning can be posed as such optimization problems.
Given that communication is often the most energy-consuming operation in networks, it is important
to design communication-efficient algorithms. The main contributions of this thesis are a classification
scheme for distributed optimization and a set of corresponding communication-efficient algorithms.
The class of optimization problems we consider is quite general, since each function may depend
on arbitrary components of the optimization variable, and not necessarily on all of them. In doing
so, we go beyond the common assumption in distributed optimization and create additional structure
that can be used to reduce the number of communications. This structure is captured by our classification
scheme, which identifies easier instances of the problem, for example the standard distributed
optimization problem, where all functions depend on all the components of the variable. In our algorithms,
no central node coordinates the network, all the communications occur between neighboring nodes,
and the data associated with each node is processed locally. We show several applications including
average consensus, support vector machines, network flows, and several distributed scenarios
for compressed sensing. We also propose a new framework for distributed model predictive control.
Through extensive numerical experiments, we show that our algorithms outperform prior distributed
algorithms in terms of communication-efficiency, even some that were specifically designed for
a particular application. 