An optimal experimental set-up maximizes the value of data for statistical inference and prediction,
which is particularly important for experiments that are time consuming or expensive to perform.
In the context of partial differential equations (PDEs), multilevel methods have been proven in
many cases to dramatically reduce the computational complexity of their single-level counterparts.
Here, two multilevel methods are proposed to efficiently compute the expected information gain
using a Kullback-Leibler divergence measure in simulation-based Bayesian optimal experimental
design. The first method is a multilevel double loop Monte Carlo (MLDLMC) with importance sampling,
which greatly reduces the computational work of the inner loop. The second proposed method is a multilevel
double loop stochastic collocation (MLDLSC) with importance sampling, which is high-dimensional
integration by deterministic quadrature on sparse grids. In both methods, the Laplace approximation
is used as an effective means of importance sampling, and the optimal values for method parameters
are determined by minimizing the average computational work subject to a desired error tolerance.
The computational efficiencies of the methods are demonstrated for computing the expected information
gain for Bayesian inversion to infer the fiber orientation in composite laminate materials by an
electrical impedance tomography experiment, given a particular set-up of the electrode configuration.
MLDLSC shows a better performance than MLDLMC by exploiting the regularity of the underlying computational
model with respect to the additive noise and the unknown parameters to be statistically inferred.
