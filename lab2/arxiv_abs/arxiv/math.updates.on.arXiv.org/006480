In this paper, we investigate the scheduling design of a mobile edge computing (MEC) system, where
active mobile devices with computation tasks randomly appear in a cell. Every task can be computed
at either the mobile device or the MEC server. We jointly optimize the task offloading decision,
uplink transmission device selection and power allocation by formulating the problem as an infinite-horizon
Markov decision process (MDP). Compared with most of the existing literature, this is the first
attempt to address the transmission and computation optimization with the random device arrivals
in an infinite time horizon to our best knowledge. Due to the uncertainty in the device number and
location, the conventional approximate MDP approaches addressing the curse of dimensionality
cannot be applied. An alternative and suitable low-complexity solution framework is proposed
in this work. We first introduce a baseline scheduling policy, whose value function can be derived
analytically with the statistics of random mobile device arrivals. Then, one-step policy iteration
is adopted to obtain a sub-optimal scheduling policy whose performance can be bounded analytically.
The complexity of deriving the sub-optimal policy is reduced dramatically compared with conventional
solutions of MDP by eliminating the complicated value iteration. To address a more general scenario
where the statistics of random mobile device arrivals are unknown, a novel and efficient algorithm
integrating reinforcement learning and stochastic gradient descent (SGD) is proposed to improve
the system performance in an online manner. Simulation results show that the gain of the sub-optimal
policy over various benchmarks is significant. 