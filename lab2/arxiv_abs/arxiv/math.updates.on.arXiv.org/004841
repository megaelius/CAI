Motivated by the increasing computational capacity of wireless user equipments (UEs), e.g., smart
phones, tablets, or vehicles, as well as the increasing concerns about sharing private data, a new
machine learning model has emerged, namely federated learning (FL), that allows a decoupling of
data acquisition and computation at the central unit. Unlike centralized learning taking place
in a data center, FL usually operates in a wireless edge network where the communication medium is
resource-constrained and unreliable. Due to limited bandwidth, only a portion of UEs can be scheduled
for updates at each iteration. Due to the shared nature of the wireless medium, transmissions are
subjected to interference and are not guaranteed. The performance of FL system in such a setting
is not well understood. In this paper, an analytical model is developed to characterize the performance
of FL in wireless networks. Particularly, tractable expressions are derived for the convergence
rate of FL in a wireless setting, accounting for effects from both scheduling schemes and inter-cell
interference. Using the developed analysis, the effectiveness of three different scheduling
policies, i.e., random scheduling (RS), round robin (RR), and proportional fair (PF), are compared
in terms of FL convergence rate. It is shown that running FL with PF outperforms RS and RR if the network
is operating under a high signal-to-interference-plus-noise ratio (SINR) threshold, while RR
is more preferable when the SINR threshold is low. Moreover, the FL convergence rate decreases rapidly
as the SINR threshold increases, thus confirming the importance of compression and quantization
of the update parameters. The analysis also reveals a trade-off between the number of scheduled
UEs and subchannel bandwidth under a fixed amount of available spectrum. 