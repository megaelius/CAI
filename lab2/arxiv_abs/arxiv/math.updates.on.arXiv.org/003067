In practical conjugate gradient (CG) computations it is important to monitor the quality of the
approximate solution to $Ax=b$ so that the CG algorithm can be stopped when the required accuracy
is reached. The relevant convergence characteristics, like the $A$-norm of the error or the normwise
backward error, cannot be easily computed. However, they can be estimated. Such estimates often
depend on approximations of the smallest or largest eigenvalue of~$A$. In the paper we introduce
a new upper bound for the $A$-norm of the error, which is closely related to the Gauss-Radau upper
bound, and discuss the problem of choosing the parameter $\mu$ which should represent a lower bound
for the smallest eigenvalue of $A$.The new bound has several practical advantages, the most important
one is that it can be used as an approximation to the $A$-norm of the error even if $\mu$ is not exactly
a lower bound for the smallest eigenvalue of $A$. In this case, $\mu$ can be chosen, e.g., as the smallest
Ritz value or its approximation. We also describe a very cheap algorithm, based on the incremental
norm estimation technique, which allows to estimate the smallest and largest Ritz values during
the CG computations. An improvement of the accuracy of these estimates of extreme Ritz values is
possible, at the cost of storing the CG coefficients and solving a linear system with a tridiagonal
matrix at each CG iteration. Finally, we discuss how to cheaply approximate the normwise backward
error. The numerical experiments demonstrate the efficiency of the estimates of the extreme Ritz
values, and show their practical use in error estimation in CG. 