Assuming to be in a compactly supported domain $G \subset \mathbb{R}^{N},$ in the presence of some
noisy measurement $f^{\delta}$ with noise level $\delta,$ true information $\varphi^{\dagger}$
must be extracted from the linear ill-posed problem below $$ f^{\delta} = \mathcal{H}\varphi^{\dagger}+\delta
\nonumber\\ \mbox{subject to }\varphi^{\dagger}(x) \geq 0, \mbox{ for all }x\in G, $$ where the
finite dimensional forward operator $\mathcal{H}$ is defined between Hilbert spaces, $\mathcal{H}:
\mathcal{X} \rightarrow \mathcal{Y}.$ To find the target function $\varphi$ we formulate the
following cost functional $$ E_{\alpha,\beta}^{\sigma}(\varphi) := \frac{\sigma}{2} \Vert\mathcal{H}\varphi
- f^{\delta}\Vert_{\mathcal{L}^2(G)}^2 + \alpha J_{\beta}(\varphi), $$ with the positive valued
data fidelity term $\sigma$ to reconstruct the nonnegative real valued function, and we define
the positive real valued TV-penalization as $$ J_{\beta}(\varphi) := \int_{G} \sqrt{\vert\nabla\varphi(x)\vert^2
+ \beta } d x,$$ with $0 <\beta \ll 1.$ Thus the solution to the linear operator equation above is
the regularized minimizer of the cost functional $E_{\alpha,\beta}^{\sigma}(\varphi).$ Minimizing
total variation type cost functionals can be achieved with different algorithms. As a computationally
economic method, lagged diffusivity fixed point iteration is studied to find the minimizer of the
total cost functional above. Despite open question on the convergence of the regularized solution,
robustness analysis of the particular algorithm is in the focus of this work. Possible options for
Lagrangian multiplier for the approximately regularized solution to meet the problem constraint
and the choice of the regularization parameter are proposed. Traditional and contemporary questions
on the stopping rule are answered. A complete scheme of the algorithm containing the analysis is
also available. 