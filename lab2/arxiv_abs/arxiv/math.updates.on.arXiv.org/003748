Random survival forest and survival trees are popular models in statistics and machine learning.
However, there is a lack of general understanding regarding consistency, splitting rules and influence
of the censoring mechanism. In this paper, we investigate the statistical properties of existing
methods from several interesting perspectives. First, we show that traditional splitting rules
with censored outcomes rely on a biased estimation of the within-node failure distribution. To
exactly quantify this bias, we develop a concentration bound of the within-node estimation based
on non i.i.d. samples and apply it to the entire forest. Second, we analyze the entanglement between
the failure and censoring distributions caused by univariate splits, and show that without correcting
the bias at an internal node, survival tree and forest models can still enjoy consistency under suitable
conditions. In particular, we demonstrate this property under two cases: a finite-dimensional
case where the splitting variables and cutting points are chosen randomly, and a high-dimensional
case where the covariates are weakly correlated. Our results can also degenerate into an independent
covariate setting, which is commonly used in the random forest literature for high-dimensional
sparse models. However, it may not be avoidable that the convergence rate depends on the total number
of variables in the failure and censoring distributions. Third, we propose a new splitting rule
that compares bias-corrected cumulative hazard functions at each internal node. We show that the
rate of consistency of this new model depends only on the number of failure variables, which improves
from non-bias-corrected versions. We perform simulation studies to confirm that this can substantially
benefit the prediction error. 