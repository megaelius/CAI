High dimensional error covariance matrices are used to weight the contribution of observation
and background terms in data assimilation procedures. As error covariance matrices are often obtained
by sampling methods, the resulting matrices are often degenerate or ill-conditioned, making them
too expensive to use in practice. In order to combat these problems, reconditioning methods are
used. In this paper we present new theory for two existing methods that can be used to reduce the condition
number of (or 'recondition') any covariance matrix: ridge regression, and the minimum eigenvalue
method. These methods are used in practice at numerical weather prediction centres, but their theoretical
impact on the covariance matrix itself is not well understood. Here we address this by investigating
the impact of reconditioning on variances and covariances of a general covariance matrix in both
a theoretical and practical setting. Improved theoretical understanding provides guidance to
users with respect to both method selection, and choice of target condition number. The new theory
shows that, for the same target condition number, both methods increase variances compared to the
original matrix, and that the ridge regression method results in a larger increase to the variances
compared to the original matrix than the minimum eigenvalue method for any covariance matrix. We
also prove that the ridge regression method strictly decreases the absolute value of off-diagonal
correlations. We apply the reconditioning methods to two examples: a simple general correlation
function, and an error covariance matrix arising from interchannel correlations. The minimum
eigenvalue method results in smaller overall changes to the correlation matrix than the ridge regression
method, but in contrast can increase off-diagonal correlations. 