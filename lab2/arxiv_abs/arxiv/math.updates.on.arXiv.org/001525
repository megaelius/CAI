We give statistical guarantees for the sample average approximation (SAA) of stochastic optimization
problems. Precisely, we derive exponential non-asymptotic finite-sample deviation inequalities
for the approximate optimal solutions and optimal value of the SAA estimator. In that respect, we
give three main contributions. First, our bounds do not require sub-Gaussian assumptions on the
data as in previous literature of stochastic optimization (SO). Instead, we just assume H\"older
continuous and heavy-tailed data (i.e. finite 2nd moments), a framework suited for risk-averse
portfolio optimization. Second, we derive new deviation inequalities for SO problems with expected-valued
stochastic constraints which guarantee joint approximate feasibility and optimality without
metric regularity of the solution set nor the use of reformulations. Thus, unlike previous works,
we do not require strong growth conditions on the objective function, the use of penalization nor
necessary first order conditions. Instead, we use metric regularity of the feasible set as a sufficient
condition, making our analysis general for many classes of problems. Our bounds imply exact feasibility
and approximate optimality for convex feasible sets with strict feasibility and approximate feasibility
and optimality for metric regular sets which are non-convex or which are convex but not strictly
feasible. In our bounds the feasible set's metric regular constant is an additional condition number.
For convex sets, we use localization arguments for concentration of measure, obtaining feasibility
estimates in terms of smaller metric entropies. Third, we obtain a general uniform concentration
inequality for heavy-tailed H\"older continuous random functions using empirical process theory.
This is the main tool in our analysis but it is also a result of independent interest. 