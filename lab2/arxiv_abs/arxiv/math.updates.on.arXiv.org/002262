A coding theorem and converse are proved for a large class of abstract stationary channels with time
structure including the result by Kadota and Wyner (1972) on continuous-time real-valued channels
as special cases. As main contribution the coding theorem is proved for a significantly weaker condition
on the channel output memory - called total ergodicity w.r.t. finite alphabet block-memoryless
input sources - and under a crucial relaxation of the measurability requirement for the channel.
These improvements are achieved by introducing a suitable characterization of information rate
capacity. It is shown that the $\psi$-mixing output memory condition used by Kadota and Wyner is
quite restrictive and excludes important channel models, in particular for the class of Gaussian
channels. In fact, it is proved that for Gaussian (e.g., fading or additive noise) channels the $\psi$-mixing
condition is equivalent to finite output memory. Further, it is demonstrated that the measurability
requirement of Kadota and Wyner is not satisfied for relevant continuous-time channel models such
as linear filters, whereas the condition used in this paper is satisfied for these models. Moreover,
a weak converse is derived for all stationary channels with time structure. Intersymbol interference
as well as input constraints are taken into account in a general and flexible way, including amplitude
and average power constraints as special case. Formulated in rigorous mathematical terms complete,
explicit, and transparent proofs are presented. As a side product a gap in the proof of Kadota and
Wyner - illustrated by a counterexample - is closed by providing a corrected proof of a lemma on the
monotonicity of some sequence of normalized mutual information quantities. An abstract framework
is established to treat discrete- and continuous-time channels with memory and arbitrary alphabets
in a unified way. 