We consider a setting in which it is desired to find an optimal complex vector $\mathbf{x}\in\mathbb{C}^N$
that satisfies $\mathcal{A}(\mathbf{x}) \approx \mathbf{b}$ in a least-squares sense, where
$\mathbf{b} \in \mathbb{C}^M$ is a data vector (possibly noise-corrupted), and $\mathcal{A}(\cdot):
\mathbb{C}^N \rightarrow \mathbb{C}^M$ is a measurement operator. If $\mathcal{A}(\cdot)$
were linear, this reduces to the classical linear least-squares problem, which has a well-known
analytic solution as well as powerful iterative solution algorithms. However, instead of linear
least-squares, this work considers the more complicated scenario where $\mathcal{A}(\cdot)$
is nonlinear, but can be represented as the summation and/or composition of some operators that
are linear and some operators that are antilinear. Some common nonlinear operations that have this
structure include complex conjugation or taking the real-part or imaginary-part of a complex vector.
Previous literature has shown that this kind of mixed linear/antilinear least-squares problem
can be mapped into a linear least-squares problem by considering $\mathbf{x}$ as a vector in $\mathbb{R}^{2N}$
instead of $\mathbb{C}^N$. While this approach is valid, the replacement of the original complex-valued
optimization problem with a real-valued optimization problem can be complicated to implement,
and can also be associated with increased computational complexity. In this work, we describe theory
and computational methods that enable mixed linear/antilinear least-squares problems to be solved
iteratively using standard linear least-squares tools, while retaining all of the complex-valued
structure of the original inverse problem. An illustration is provided to demonstrate that this
approach can simplify the implementation and reduce the computational complexity of iterative
solution algorithms. 