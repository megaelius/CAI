This paper considers the problem of estimating a high-dimensional vector of parameters $\boldsymbol{\theta}
\in \mathbb{R}^n$ from a noisy observation. The noise vector is i.i.d. Gaussian with known variance.
For a squared-error loss function, the James-Stein (JS) estimator is known to dominate the simple
maximum-likelihood (ML) estimator when the dimension $n$ exceeds two. The JS-estimator shrinks
the observed vector towards the origin, and the risk reduction over the ML-estimator is greatest
for $\boldsymbol{\theta}$ that lie close to the origin. JS-estimators can be generalized to shrink
the data towards any target subspace. Such estimators also dominate the ML-estimator, but the risk
reduction is significant only when $\boldsymbol{\theta}$ lies close to the subspace. This leads
to the question: in the absence of prior information about $\boldsymbol{\theta}$, how do we design
estimators that give significant risk reduction over the ML-estimator for a wide range of $\boldsymbol{\theta}$?
In this paper, we propose shrinkage estimators that attempt to infer the structure of $\boldsymbol{\theta}$
from the observed data in order to construct a good attracting subspace. In particular, the components
of the observed vector are separated into clusters, and the elements in each cluster shrunk towards
a common attractor. The number of clusters and the attractor for each cluster are determined from
the observed vector. We provide concentration results for the squared-error loss and convergence
results for the risk of the proposed estimators. The results show that the estimators give significant
risk reduction over the ML-estimator for a wide range of $\boldsymbol{\theta}$, particularly
for large $n$. Simulation results are provided to support the theoretical claims. 