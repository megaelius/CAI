We develop and analyze a projected particle Langevin optimization method to learn the distribution
in the Sch\"{o}nberg integral representation of the radial basis functions from training samples.
More specifically, we characterize a distributionally robust optimization method with respect
to the Wasserstein distance to optimize the distribution in the Sch\"{o}nberg integral representation.
To provide theoretical performance guarantees, we analyze the scaling limits of a projected particle
online (stochastic) optimization method in the mean-field regime. In particular, we prove that
in the scaling limits, the empirical measure of the Langevin particles converges to the law of a reflected
It\^{o} diffusion-drift process. Moreover, the drift is also a function of the law of the underlying
process. Using It\^{o} lemma for semi-martingales and Grisanov's change of measure for the Wiener
processes, we then derive a Mckean-Vlasov type partial differential equation (PDE) with Robin
boundary conditions that describes the evolution of the empirical measure of the projected Langevin
particles in the mean-field regime. In addition, we establish the existence and uniqueness of the
steady-state solutions of the derived PDE in the weak sense. We apply our learning approach to train
radial kernels in the kernel locally sensitive hash (LSH) functions, where the training data-set
is generated via a $k$-mean clustering method on a small subset of data-base. We subsequently apply
our kernel LSH with a trained kernel for image retrieval task on MNIST data-set, and demonstrate
the efficacy of our kernel learning approach. We also apply our kernel learning approach in conjunction
with the kernel support vector machines (SVMs) for classification of benchmark data-sets. 