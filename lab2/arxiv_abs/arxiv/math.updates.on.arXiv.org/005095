Functional constrained optimization is becoming more and more important in machine learning and
operations research. Such problems have potential applications in risk-averse machine learning,
semisupervised learning and robust optimization among others. In this paper, we first present
a novel Constraint Extrapolation (ConEx) method for solving convex functional constrained problems,
which utilizes linear approximations of the constraint functions to define the extrapolation
(or acceleration) step. We show that this method is a unified algorithm that achieves the best-known
rate of convergence for solving different functional constrained convex composite problems,
including convex or strongly convex, and smooth or nonsmooth problems with stochastic objective
and/or stochastic constraints. Many of these rates of convergence were in fact obtained for the
first time in the literature. In addition, ConEx is a single-loop algorithm that does not involve
any penalty subproblems. Contrary to existing dual methods, it does not require the projection
of Lagrangian multipliers into a (possibly unknown) bounded set. Second, for nonconvex functional
constrained problem, we introduce a new proximal point method which transforms the initial nonconvex
problem into a sequence of convex functional constrained subproblems. We establish the convergence
and rate of convergence of this algorithm to KKT points under different constraint qualifications.
For practical use, we present inexact variants of this algorithm, in which approximate solutions
of the subproblems are computed using the aforementioned ConEx method and establish their associated
rate of convergence. To the best of our knowledge, most of these convergence and complexity results
of the proximal point method for nonconvex problems also seem to be new in the literature. 