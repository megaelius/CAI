We study distributed optimization problems over a network when the communication between the nodes
is constrained, and so information that is exchanged between the nodes must be quantized. Recent
advances using the distributed gradient algorithm with a quantization scheme at a fixed resolution
have established convergence, but at rates significantly slower than when the communications
are unquantized. In this paper, we introduce a novel quantization method, which we refer to as adaptive
quantization, that allows us to match the convergence rates under perfect communications. Our
approach adjusts the quantization scheme used by each node as the algorithm progresses: as we approach
the solution, we become more certain about where the state variables are localized, and adapt the
quantizer codebook accordingly. We bound the convergence rates of the proposed method as a function
of the communication bandwidth, the underlying network topology, and structural properties of
the constituent objective functions. In particular, we show that if the objective functions are
convex or strongly convex, then using adaptive quantization does not affect the rate of convergence
of the distributed subgradient methods when the communications are quantized, except for a constant
that depends on the resolution of the quantizer. To the best of our knowledge, the rates achieved
in this paper are better than any existing work in the literature for distributed gradient methods
under finite communication bandwidths. We also provide numerical simulations that compare convergence
properties of the distributed gradient methods with and without quantization for solving distributed
regression problems for both quadratic and absolute loss functions. 