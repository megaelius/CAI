Concentration inequalities form an essential toolkit in the study of high dimensional (HD) statistical
methods. Most of the relevant statistics literature in this regard is based on sub-Gaussian or sub-exponential
tail assumptions. In this paper, we first bring together various probabilistic inequalities for
sums of independent random variables under much weaker exponential type (namely sub-Weibull)
tail assumptions. These results extract a part sub-Gaussian tail behavior in finite samples, matching
the asymptotics governed by the central limit theorem, and are compactly represented in terms of
a new Orlicz quasi-norm - the Generalized Bernstein-Orlicz norm - that typifies such tail behaviors.
We illustrate the usefulness of these inequalities through the analysis of four fundamental problems
in HD statistics. In the first two problems, we study the rate of convergence of the sample covariance
matrix in terms of the maximum elementwise norm and the maximum k-sub-matrix operator norm which
are key quantities of interest in bootstrap, HD covariance matrix estimation and HD inference.
The third example concerns the restricted eigenvalue condition, required in HD linear regression,
which we verify for all sub-Weibull random vectors through a unified analysis, and also prove a more
general result related to restricted strong convexity in the process. In the final example, we consider
the Lasso estimator for linear regression and establish its rate of convergence under much weaker
than usual tail assumptions (on the errors as well as the covariates), while also allowing for misspecified
models and both fixed and random design. To our knowledge, these are the first such results for Lasso
obtained in this generality. The common feature in all our results over all the examples is that the
convergence rates under most exponential tails match the usual ones under sub-Gaussian assumptions.
