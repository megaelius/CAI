Random linear mappings are widely used in modern signal processing, compressed sensing and machine
learning. These mappings may be used to embed the data into a significantly lower dimension while
at the same time preserving useful information. This is done by approximately preserving the distances
between data points, which are assumed to belong to $\mathbb{R}^n$. Thus, the performance of these
mappings is usually captured by how close they are to an isometry on the data. Random Gaussian linear
mappings have been the object of much study, while the sub-Gaussian settings is not yet fully understood.
In the latter case, the performance depends on the sub-Gaussian norm of the rows. In many applications,
e.g., compressed sensing, this norm may be large, or even growing with dimension, and thus it is important
to characterize this dependence. We study when a sub-Gaussian matrix can become a near isometry
on a set, show that previous best known dependence on the sub-Gaussian norm was sub-optimal, and
present the optimal dependence. Our result not only answers a remaining question posed by Liaw,
Mehrabian, Plan and Vershynin in 2017, but also generalizes their work. We also develop a new Bernstein
type inequality for sub-exponential random variables, and a new Hanson-Wright inequality for
quadratic forms of sub-Gaussian random variables, in both cases improving the bounds in the sub-Gaussian
regime under moment constraints. Finally, we illustrate popular applications such as Johnson-Lindenstrauss
embeddings, randomized sketches and blind demodulation, whose theoretical guarantees can be
improved by our results in the sub-Gaussian case. 