Gibbs sampling is a widely popular Markov chain Monte Carlo algorithm that can be used to analyze
intractable posterior distributions associated with Bayesian hierarchical models. There are
two standard versions of the Gibbs sampler: The systematic scan (SS) version, where all variables
are updated at each iteration, and the random scan (RS) version, where a single, randomly selected
variable is updated at each iteration. The literature comparing the theoretical properties of
SS and RS Gibbs samplers is reviewed, and an alternative hybrid scan Gibbs sampler is introduced,
which is particularly well suited to Bayesian models with latent variables. The word "hybrid" reflects
the fact that the scan used within this algorithm has both systematic and random elements. Indeed,
at each iteration, one updates the entire set of latent variables, along with a randomly chosen block
of the remaining variables. The hybrid scan (HS) Gibbs sampler has important advantages over the
two standard scan Gibbs samplers. Firstly, the HS algorithm is often easier to analyze from a theoretical
standpoint. In particular, it can be much easier to establish the geometric ergodicity of a HS Gibbs
Markov chain than to do the same for the corresponding SS and RS versions. Secondly, the sandwich
methodology developed in Hobert and Marchev (2008), which is also reviewed, can be applied to the
HS Gibbs algorithm (but not to the standard scan Gibbs samplers). It is shown that, under weak regularity
conditions, adding sandwich steps to the HS Gibbs sampler always results in a theoretically superior
algorithm. Three specific Bayesian hierarchical models of varying complexity are used to illustrate
the results. One is a simple location-scale model for data from the Student's $t$ distribution,
which is used as a pedagogical tool. The other two are sophisticated, yet practical Bayesian regression
models. 