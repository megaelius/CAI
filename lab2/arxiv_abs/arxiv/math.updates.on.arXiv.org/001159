"Bounds on information combining" are entropic inequalities that determine how the information
(entropy) of a set of random variables can change when these are combined in certain prescribed ways.
Such bounds play an important role in classical information theory, particularly in coding and
Shannon theory; entropy power inequalities are special instances of them. The arguably most elementary
kind of information combining is the addition of two binary random variables (a CNOT gate), and the
resulting quantities play an important role in Belief propagation and Polar coding. We investigate
this problem in the setting where quantum side information is available, which has been recognized
as a hard setting for entropy power inequalities. Our main technical result is a non-trivial, and
close to optimal, lower bound on the combined entropy, which can be seen as an almost optimal "quantum
Mrs. Gerber's Lemma". Our proof uses three main ingredients: (1) a new bound on the concavity of von
Neumann entropy, which is tight in the regime of low pairwise state fidelities; (2) the quantitative
improvement of strong subadditivity due to Fawzi-Renner, in which we manage to handle the minimization
over recovery maps; (3) recent duality results on classical-quantum-channels due to Renes et al.
We furthermore present conjectures on the optimal lower and upper bounds under quantum side information,
supported by interesting analytical observations and strong numerical evidence. We finally apply
our bounds to Polar coding for binary-input classical-quantum channels, and show the following
three results: (A) Even non-stationary channels polarize under the polar transform. (B) The blocklength
required to approach the symmetric capacity scales at most sub-exponentially in the gap to capacity.
(C) Under the aforementioned lower bound conjecture, a blocklength polynomial in the gap suffices.
