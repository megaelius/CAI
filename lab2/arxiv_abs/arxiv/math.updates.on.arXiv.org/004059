Gaussian graphical models are a popular tool to learn the dependence structure in the form of a graph
among variables of interest. Bayesian methods have gained in popularity in the last two decades
due to their ability to simultaneously learn the covariance and the graph and characterize uncertainty
in the selection. For scalability of the Markov chain Monte Carlo algorithms, decomposability
is commonly imposed on the graph space. A wide variety of graphical conjugate priors are proposed
jointly on the covariance matrix and the graph with improved algorithms to search along the space
of decomposable graphs, rendering the methods extremely popular in the context of multivariate
dependence modeling. {\it An open problem} in Bayesian decomposable structure learning is whether
the posterior distribution is able to select a meaningful decomposable graph that it is ``close''
in an appropriate sense to the true non-decomposable graph, when the dimension of the variables
increases with the sample size. In this article, we explore specific conditions on the true precision
matrix and the graph which results in an affirmative answer to this question using a commonly used
hyper-inverse Wishart prior on the covariance matrix and a suitable complexity prior on the graph
space, both in the well-specified and misspecified settings. In absence of structural sparsity
assumptions, our strong selection consistency holds in a high dimensional setting where $p = O(n^{\alpha})$
for $\alpha < 1/3$. We show when the true graph is non-decomposable, the posterior distribution
on the graph concentrates on a set of graphs that are {\it minimal triangulations} of the true graph.
