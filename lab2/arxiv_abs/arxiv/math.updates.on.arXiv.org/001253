We study the necessary and sufficient complexity of ReLU neural networks---in terms of depth and
number of weights---which is required for approximating classifier functions in an $L^2$-sense.
As a model class, we consider the set $\mathcal{E}^\beta (\mathbb R^d)$ of possibly discontinuous
piecewise $C^\beta$ functions $f : [-1/2, 1/2]^d \to \mathbb R$, where the different "smooth regions"
of $f$ are separated by $C^\beta$ hypersurfaces. For given dimension $d \geq 2$, regularity $\beta
> 0$, and accuracy $\varepsilon > 0$, we construct artificial neural networks with ReLU activation
function that approximate functions from $\mathcal{E}^\beta(\mathbb R^d)$ up to an $L^2$ error
of $\varepsilon$. The constructed networks have a fixed number of layers, depending only on $d$
and $\beta$ and they have $\mathcal{O}(\varepsilon^{-2(d-1)/\beta})$ many non-zero weights,
which we prove to be optimal. For the proof of optimality, we establish a lower bound on the description
complexity of the class $\mathcal{E}^\beta (\mathbb R^d)$. By showing that a family of approximating
neural networks gives rise to an encoder for $\mathcal{E}^\beta (\mathbb R^d)$, we then prove that
one cannot approximate a general function $f \in \mathcal{E}^\beta (\mathbb R^d)$ using neural
networks that are less complex than those produced by our construction. In addition to the optimality
in terms of the number of weights, we show that in order to achieve this optimal approximation rate,
one needs ReLU networks of a certain minimal depth. Precisely, for piecewise $C^\beta(\mathbb
R^d)$ functions, this minimal depth is given---up to a multiplicative constant---by $\beta/d$.
Up to a log factor, our constructed networks match this bound. This partly explains the benefits
of depth for ReLU networks by showing that deep networks are necessary to achieve efficient approximation
of (piecewise) smooth functions. 