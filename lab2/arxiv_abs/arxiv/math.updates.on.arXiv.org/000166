The incorporation of priors in the Optimal Uncertainty Quantification (OUQ) framework reveals
extreme brittleness in Bayesian inference; very mild assumptions are sufficient to demonstrate
that, given a set of priors, conditioning on observations can produce results as bad as possible,
regardless of the sample size. Furthermore, this brittleness result shows that, contrary to popular
belief, there is no such thing as a "close enough" model in Bayesian inference; a model may share an
arbitrarily large number of finite-dimensional marginals with, or be arbitrarily close (in Prokhorov
or total variation metrics) to, the data-generating distribution and still make the largest possible
prediction error after conditioning on an arbitrarily large number of samples. The initial purpose
of this paper is to unwrap this brittleness mechanism by providing (i) a quantitative version of
the Brittleness Theorem of ( arXiv:1304.6772 ) (ii) a detailed and comprehensive analysis of its
application to the revealing example of estimating the mean of a random variable on the unit interval
$[0,1]$ using priors that exactly capture the distribution of an arbitrarily large numbers of Hausdorff
moments. However, in doing so, we also found that the free parameter associated with Markov and Krein's
canonical representations of truncated Hausdorff moments generates reproducing kernel identities
corresponding to reproducing kernel Hilbert spaces of polynomials. Furthermore, these reproducing
identities lead to biorthogonal systems of Selberg integral formulas. Therefore, whereas a Selberg
integral formula was initially used to compute the volume of the Hausdorff moment space, it appears
that a refined analysis of the integral geometry of the Hausdorff moment space, in the service of
proving the Brittleness of Bayesian Inference, also reveals a new family of Selberg integral formulas.
