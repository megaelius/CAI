In classification applications such as severe disease diagnosis and fraud detection, people have
clear priorities over the two types of classification errors. For instance, diagnosing a patient
with cancer to be healthy may lead to loss of life, which incurs a much higher cost than the other way
around. The classical binary classification paradigm does not take into account such priorities,
as it aims to minimize the overall classification error. In contrast, the Neyman-Pearson (NP) paradigm
seeks classifiers with a minimal type II error while having the prioritized type I error constrained
under a user-specified level, addressing asymmetric type I/II error priorities in the previously
mentioned scenarios. Despite recent advances in the NP classification literature, two essential
issues pose challenges: i) current theoretical framework assumes bounded feature support, which
does not admit parametric settings; ii) in practice, existing NP classifiers involve splitting
class 0 samples into two parts using a pre-fixed split proportion. To address the first challenge,
we present NP-sLDA that adapts the popular sparse linear discriminant analysis (sLDA, Mai et al.
(2012)) to the NP paradigm. On the theoretical front, this is the first theoretically justified
NP classifier that takes parametric assumptions and unbounded feature support. We formulate a
new conditional margin assumption and a new conditional detection condition to accommodate unbounded
feature support and show that NP-sLDA satisfies the NP oracle inequalities. Numerical results
show that NP-sLDA is a valuable addition to the existing NP classifiers. To address the second challenge,
we construct a general data-adaptive sample splitting scheme that improves the classification
performance upon the default half-half class 0 split used in Tong et al. (2018). 