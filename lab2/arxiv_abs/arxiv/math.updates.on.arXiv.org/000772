While variable selection and oracle inequalities for the estimation and prediction error have
received considerable attention in the literature on high-dimensional models, very little work
has been done in the area of testing and construction of confidence bands in high-dimensional models.
However, in a recent paper van de Geer et al. (2014) showed how the Lasso can be desparsified in order
to create asymptotically honest (uniform) confidence band. In this paper we consider the conservative
Lasso which penalizes more correctly than the Lasso and hence has a lower estimation error. In particular,
we develop an oracle inequality for the conservative Lasso only assuming the existence of a certain
number of moments. This is done by means of the Marcinkiewicz-Zygmund inequality which in our context
provides sharper bounds than Nemirovski's inequality. As opposed to van de Geer et al. (2014) we
allow for heteroskedastic non-subgaussian error terms and covariates. Next, we desparsify the
conservative Lasso estimator and derive the asymptotic distribution of tests involving an increasing
number of parameters. As a stepping stone towards this, we also provide a feasible uniformly consistent
estimator of the asymptotic covariance matrix of an increasing number of parameters which is robust
against conditional heteroskedasticity. To our knowledge we are the first to do so. Next, we show
that our confidence bands are honest over sparse high-dimensional sub vectors of the parameter
space and that they contract at the optimal rate. All our results are valid in high-dimensional models.
Our simulations reveal that the desparsified conservative Lasso estimates the parameters much
more precisely than the desparsified Lasso, has much better size properties and produces confidence
bands with markedly superior coverage rates. 