If the nonparametric regression function of a response variable $Y$ on covariates $X$ and $Z$ is
an affine function of $X$ such that the slope $\alpha$ and the intercept $\beta$ are real valued measurable
functions on the range of the completely arbitrary random element $Z$, then, under the assumptions
that $X$ has a finite moment of order greater than or equal to 2, $Y$ has a finite moment of conjugate
order, and $\alpha(Z)$ and $\alpha(Z)X$ have finite first moments, we show that the nonparametric
regression function equals the least squares linear regression function of $Y$ on $X$ with all the
moments that appear in the expression of the linear regression function calculated conditional
on $Z$. Consequently, conditional mean independence implies zero conditional covariance and
a degenerate version of the aforesaid affine form for the nonparametric regression function, whereas
the aforesaid affine form and zero conditional covariance imply conditional mean independence.
That the least squares linear regression formula for the nonparametric regression function holds
if $(X, Y, Z)$ is multivariate Normal is not difficult to establish without appealing to the aforesaid
affine form for the nonparametric regression function; however, to show that the least squares
linear regression formula holds if $X$ is Bernoulli, when $Y$ has only a finite first moment and $Z$
is completely arbitrary, it seems one must verify that the nonparametric regression function has
the aforesaid affine form using that 1 is the conjugate exponent of $\infty$, since a direct, tedious
verification of the formula is possible if $Y$ is bounded, or if $Y$ has a finite second moment and
the range of $Z$ is a Polish space, but seems impossible otherwise. 