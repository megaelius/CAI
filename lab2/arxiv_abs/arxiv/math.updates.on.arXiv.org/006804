These notes are about ridge functions. Recent years have witnessed a flurry of interest in these
functions. Ridge functions appear in various fields and under various guises. They appear in fields
as diverse as partial differential equations (where they are called plane waves), computerized
tomography and statistics. These functions are also the underpinnings of many central models in
neural networks. We are interested in ridge functions from the point of view of approximation theory.
The basic goal in approximation theory is to approximate complicated objects by simpler objects.
Among many classes of multivariate functions, linear combinations of ridge functions are a class
of simpler functions. These notes study some problems of approximation of multivariate functions
by linear combinations of ridge functions. We present here various properties of these functions.
The questions we ask are as follows. When can a multivariate function be expressed as a linear combination
of ridge functions from a certain class? When do such linear combinations represent each multivariate
function? If a precise representation is not possible, can one approximate arbitrarily well? If
well approximation fails, how can one compute/estimate the error of approximation, know that a
best approximation exists? How can one characterize and construct best approximations? We also
study properties of generalized ridge functions, which are very much related to linear superpositions
and Kolmogorov's famous superposition theorem. These notes end with a few applications of ridge
functions to the problem of approximation by single and two hidden layer neural networks with a restricted
set of weights. We hope that these notes will be useful and interesting to both researchers and students.
