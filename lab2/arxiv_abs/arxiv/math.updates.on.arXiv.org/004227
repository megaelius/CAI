In computer communications, discrete data are channel coded to add robustness to noise and then
modulated into continuous signals for transmission and reception. In a hard detection setting,
received signals are demodulated into discrete data that are provided to a decoder for inference
of the transmitted code-word. If additional soft detection information on the quality of the received
signal is provided to the decoder, it is known that its use can improve decoding accuracy. Incorporating
that information, however, typically comes at the expense of increased algorithmic complexity
of the decoder. Here we introduce and analyse a mechanism by which soft detection information can
be used within Guessing Random Additive Noise Decoding (GRAND) framework in such a way that decoding
accuracy is increased, but computational complexity is decreased. The principle envisages an
enhanced code-book-independent quantization of soft detection information where demodulated
symbols are indicated to be confidently reliable or are labeled as being unreliable. We introduce
two algorithms that incorporate this symbol reliability information, one of which identifies
a Maximum Likelihood (ML) decoding and the other either reports an ML decoding or an error. We prove
that both are capacity-achieving when used with random code-books, and determine both error exponents
and asymptotic complexity. These decoding algorithms are suitable for use with any block-code
and have complexities that can only reduce as code-book rates increase. With respect to their hard
detection counterparts, they are capable of achieving higher rates with lower error probabilities,
and have reduced algorithmic complexity. 