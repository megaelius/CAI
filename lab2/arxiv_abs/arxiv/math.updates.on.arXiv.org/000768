We study exact recovery conditions for convex relaxations of point cloud clustering problems,
focusing on two of the most common optimization problems for unsupervised clustering: $k$-means
and $k$-medians clustering. Motivations for focusing on convex relaxations are: (a) they come
with a certificate of optimality, and (b) they are generic tools not tailored to the specific recovery-guarantee
conditions. More precisely, consider the distributional setting where there are $k$ clusters
and data from each cluster consists of $n$ points sampled from a symmetric distribution within a
ball of unit radius of dimension $m$. We ask: what is the minimal separation distance between cluster
centers needed for various convex relaxations to exactly recover these $k$ clusters as its optimal
integral solution? For the $k$-median linear programming relaxation we show a tight bound: exact
recovery is obtained given arbitrarily small cluster separation $\Delta > 2+\epsilon$, for
any $\epsilon>0$. Under the same distributional model, the \emph{$k$-means} LP relaxation
fails to recover such clusters at separation as large as $2 +\sqrt{2}$. Yet, if we enforce PSD constraints
on the $k$-means LP, we get exact cluster recovery at separation as low as $\Delta > 2 + \sqrt{2k/m}+
\epsilon.$ In contrast, common heuristics such as Lloyd's algorithm (a.k.a. the $k$-means algorithm)
can \emph{fail} to recover clusters in this setting, even just three clusters and arbitrarily large
cluster separation distance. To complement the theoretical analysis, we provide an experimental
study of the recovery guarantees for these various methods. Our work provides new insights into
the power of these relaxations and we believe that this line of research will lead to a deeper understanding
of such methods in the future. 