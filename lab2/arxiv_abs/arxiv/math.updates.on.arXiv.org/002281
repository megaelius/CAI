In this paper, we study the efficiency of a {\bf R}estarted {\bf S}ub{\bf G}radient (RSG) method
that periodically restarts the standard subgradient method (SG). We show that, when applied to
a broad class of convex optimization problems, RSG method can find an $\epsilon$-optimal solution
with a low complexity than SG method. In particular, we first show that RSG can reduce the dependence
of SG's iteration complexity on the distance between the initial solution and the optimal set to
that between the $\epsilon$-level set and the optimal set. In addition, we show the advantages of
RSG over SG in solving three different families of convex optimization problems. (a) For the problems
whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local
quadratic growth property, RSG has an $O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration
complexity. (c) For the problems that admit a local Kurdyka-\L ojasiewicz property with a power
constant of $\beta\in[0,1)$, RSG has an $O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$
iteration complexity. On the contrary, with only the standard analysis, the iteration complexity
of SG is known to be $O(\frac{1}{\epsilon^2})$ for these three classes of problems. The novelty
of our analysis lies at exploiting the lower bound of the first-order optimality residual at the
$\epsilon$-level set. It is this novelty that allows us to explore the local properties of functions
(e.g., local quadratic growth property, local Kurdyka-\L ojasiewicz property, more generally
local error bounds) to develop the improved convergence of RSG. We demonstrate the effectiveness
of the proposed algorithms on several machine learning tasks including regression and classification.
