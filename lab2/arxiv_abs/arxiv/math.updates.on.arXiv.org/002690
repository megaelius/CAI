In this work, we examine the optimality of Gaussian signalling for covert communications with an
upper bound on $\mathcal{D}(p_{_1}(\mathbf{y})||p_{_0}(\mathbf{y}))$ or $\mathcal{D}(p_{_0}(\mathbf{y})||p_{_1}(\mathbf{y}))$
as the covert communication constraint, where $\mathcal{D}(p_{_1}(\mathbf{y})||p_{_0}(\mathbf{y}))$
and $\mathcal{D}(p_{_0}(\mathbf{y})||p_{_1}(\mathbf{y}))$ are different due to the asymmetry
of Kullback-Leibler divergence, $p_{_0}(\mathbf{y})$ and $p_{_1}(\mathbf{y})$ are the likelihood
functions of the observation $\mathbf{y}$ at the warden under the null hypothesis (no covert transmission)
and alternative hypothesis (a covert transmission occurs), respectively. Considering additive
white Gaussian noise at both the receiver and the warden, we prove that Gaussian signalling is optimal
in terms of maximizing the mutual information of transmitted and received signals for covert communications
with $\mathcal{D}(p_{_1}(\mathbf{y})||p_{_0}(\mathbf{y})) \leq 2\epsilon^2$ as the constraint.
More interestingly, we also prove that Gaussian signalling is not optimal for covert communications
with an upper bound on $\mathcal{D}(p_{_0}(\mathbf{y})||p_{_1}(\mathbf{y}))$ as the constraint,
for which as we explicitly show a skew-normal signalling can outperform Gaussian signalling in
terms of achieving higher mutual information. Finally, we prove that, for Gaussian signalling,
an upper bound on $\mathcal{D}(p_{_1}(\mathbf{y})||p_{_0}(\mathbf{y}))$ is a tighter constraint
in terms of leading to lower mutual information than the same upper bound on $\mathcal{D}(p_{_0}(\mathbf{y})||p_{_1}(\mathbf{y}))$,
by proving $\mathcal{D}(p_{_0}(\mathbf{y})||p_{_1}(\mathbf{y})) \leq \mathcal{D}(p_{_1}(\mathbf{y})||p_{_0}(\mathbf{y}))$.
