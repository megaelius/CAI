Information geometry uses the formal tools of differential geometry to describe the space of probability
distributions as a Riemannian manifold with an additional dual structure. The formal equivalence
of compositional data with discrete probability distributions makes it possible to apply the same
description to the sample space of Compositional Data Analysis (CoDA). The latter has been formally
described as a Euclidean space with an orthonormal basis featuring components that are suitable
combinations of the original parts. In contrast to the Euclidean metric, the information-geometric
description singles out the Fisher information metric as the only one keeping the manifold's geometric
structure invariant under equivalent representations of the underlying random variables. Well-known
concepts that are valid in Euclidean coordinates, e.g., the Pythogorean theorem, are generalized
by information geometry to corresponding notions that hold for more general coordinates. In briefly
reviewing Euclidean CoDA and, in more detail, the information-geometric approach, we show how
the latter justifies the use of distance measures and divergences that so far have received little
attention in CoDA as they do not fit the Euclidean geometry favored by current thinking. We also show
how entropy and relative entropy can describe amalgamations in a simple way, while Aitchison distance
requires the use of geometric means to obtain more succinct relationships. We proceed to prove the
information monotonicity property for Aitchison distance. We close with some thoughts about new
directions in CoDA where the rich structure that is provided by information geometry could be exploited.
