We introduce new global and local inexact oracle concepts for a wide class of convex functions in
composite convex minimization. Such inexact oracles naturally come from primal-dual framework,
barrier smoothing, inexact computations of gradients and Hessian, and many other situations.
We also provide examples showing that the class of convex functions equipped with the newly inexact
second-order oracles is larger than standard self-concordant as well as Lipschitz gradient function
classes. Further, we investigate several properties of convex and/or self-concordant functions
under the inexact second-order oracles which are useful for algorithm development. Next, we apply
our theory to develop inexact proximal Newton-type schemes for minimizing general composite convex
minimization problems equipped with such inexact oracles. Our theoretical results consist of
new optimization algorithms, accompanied with global convergence guarantees to solve a wide class
of composite convex optimization problems. When the first objective term is additionally self-concordant,
we establish different local convergence results for our method. In particular, we prove that depending
on the choice of accuracy levels of the inexact second-order oracles, we obtain different local
convergence rates ranging from $R$-linear and $R$-superlinear to $R$-quadratic. In special cases,
where convergence bounds are known, our theory recovers the best known rates. We also apply our settings
to derive a new primal-dual method for composite convex minimization problems. Finally, we present
some representative numerical examples to illustrate the benefit of our new algorithms. 