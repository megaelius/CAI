Manski's celebrated maximum score estimator for the binary choice model has been the focus of much
investigation in both the econometrics and statistics literatures, but its behavior under growing
dimension scenarios still largely remains unknown. This paper seeks to address that gap. Two different
cases are considered: $p$ grows with $n$ but at a slow rate, i.e. $p/n \rightarrow 0$; and $p \gg n$
(fast growth). By relating Manski's score estimation to empirical risk minimization in a classification
problem, we show that under a \emph{soft margin condition} involving a smoothness parameter $\alpha
> 0$, the rate of the score estimator in the slow regime is essentially $\left((p/n)\log n\right)^{\frac{\alpha}{\alpha
+ 2}}$, while, in the fast regime, the $l_0$ penalized score estimator essentially attains the rate
$((s_0 \log{p} \log{n})/n)^{\frac{\alpha}{\alpha + 2}}$, where $s_0$ is the sparsity of the true
regression parameter. For the most interesting regime, $\alpha = 1$, the rates of Manski's estimator
are therefore $\left((p/n)\log n\right)^{1/3}$ and $((s_0 \log{p} \log{n})/n)^{1/3}$ in the
slow and fast growth scenarios respectively, which can be viewed as high-dimensional analogues
of cube-root asymptotics: indeed, this work is possibly the first study of a non-regular statistical
problem in a high-dimensional framework. We also establish upper and lower bounds for the minimax
$L_2$ error in the Manski's model that differ by a logarithmic factor, and construct a minimax-optimal
estimator in the setting $\alpha=1$. Finally, we provide computational recipes for the maximum
score estimator in growing dimensions that show promising results. 