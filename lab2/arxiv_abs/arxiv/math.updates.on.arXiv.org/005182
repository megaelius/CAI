We seek causes through science, religion, and in everyday life. We get excited when a big rock causes
a big splash, and we get scared when it tumbles without a cause. But our causal cognition is usually
biased. The 'why' is influenced by the 'who'. It is influenced by the 'self', and by 'others'. We share
rituals, we watch action movies, and we influence each other to believe in the same causes. Human
mind is packed with subjectivity because shared cognitive biases bring us together. But they also
make us vulnerable. An artificial mind is deemed to be more objective than the human mind. After many
years of science-fiction fantasies about even-minded androids, they are now sold as personal or
expert assistants, as brand advocates, as policy or candidate supporters, as network influencers.
Artificial agents have been stunningly successful in disseminating artificial causal beliefs
among humans. As malicious artificial agents continue to manipulate human cognitive biases, and
deceive human communities into ostensive but expansive causal illusions, the hope for defending
us has been vested into developing benevolent artificial agents, tasked with preventing and mitigating
cognitive distortions inflicted upon us by their malicious cousins. Can the distortions of human
causal cognition be corrected on a more solid foundation of artificial causal cognition? In the
present paper, we study a simple model of causal cognition, viewed as a quest for causal models. We
show that, under very mild and hard to avoid assumptions, there are always self-confirming causal
models, which perpetrate self-deception, and seem to preclude a royal road to objectivity. 