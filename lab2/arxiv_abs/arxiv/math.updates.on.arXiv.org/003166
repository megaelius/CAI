Let $K_1$, $\dots$, $K_n$ be bounded, complete, separable metric spaces. Let $f:\prod_i K_i \to
\mathbb{R}$ be a bounded and continuous potential function, and let $\mu\ \propto\ \mathrm{e}^f$
be the associated Gibbs distribution. At each point $\mathbf{x} \in \prod_i K_i$ one can define
a `discrete gradient' $\nabla_{\mathbf{x}}f$ by comparing the values of $f$ at all points which
differ from $\bf{x}$ in at most one coordinate. In case $\prod_i K_i = \{-1,1\}^n \subset \mathbb{R}^n$,
the discrete gradient $\nabla_{\mathbf{x}}f$ is naturally identified with a vector in $\mathbb{R}^n$.
This paper shows that a `low-complexity' assumption on $\nabla f$ implies that $\mu$ can be approximated
by a mixture of other measures, relatively few in number, and most of them close in a natural transportation
distance to product measures. This implies also an approximation to the partition function of $f$
in terms of product measures, along the lines of Chatterjee and Dembo's theory of `nonlinear large
deviations'. An important precedent for this work is a result of Eldan in the case $\prod_i K_i = \{-1,1\}^n$.
Eldan's assumption is that the discrete gradients $\nabla_{\mathbf{x}} f$ all lie in a subset of
$\mathbb{R}^n$ that has small Gaussian width. His proof is based on the careful construction of
a diffusion in $\mathbb{R}^n$ which starts at the origin and ends with the desired distribution
on the subset $\{-1,1\}^n$. Here our assumption is a more naive covering-number bound on the set
of gradients $\{\nabla_{\mathbf{x}}f:\ \mathbf{x} \in \prod_i K_i\}$, and our proof relies only
on basic inequalities of information theory. As a result, it is shorter, and applies to Gibbs measures
on abitrary product spaces. 