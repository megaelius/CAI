Algebraic topology methods have recently played an important role for statistical analysis with
complicated geometric structured data. Among them, persistent homology is a well-known tool to
extract robust topological features, and outputs as persistence diagrams. Unfortunately, persistence
diagrams are point multi-sets which can not be used in machine learning algorithms for vector data.
To deal with it, an emerged approach is to use kernel methods. Besides that, geometry for persistence
diagrams is also an important factor. A popular geometry for persistence diagrams is the Wasserstein
metric. However, Wasserstein distance is not negative definite. Thus, it is limited to build positive
definite kernels upon the Wasserstein distance without approximation. In this work, we explore
an alternative Riemannian manifold geometry, namely the Fisher information metric. By building
upon the geodesic distance on the Riemannian manifold, we propose a positive definite kernel, namely
Riemannian manifold kernel. Then, we analyze eigensystem of the integral operator induced by the
proposed kernel for kernel machines. Based on that, we conduct generalization error bounds via
covering numbers and Rademacher averages for kernel machines using the Riemannian manifold kernel.
Additionally, we also show some nice properties for the proposed kernel such as stability, infinite
divisibility and comparative time complexity with other kernels for persistence diagrams in term
of computation. Throughout experiments with many different tasks on various benchmark datasets,
we illustrate that the Riemannian manifold kernel improves performances of other baseline kernels.
