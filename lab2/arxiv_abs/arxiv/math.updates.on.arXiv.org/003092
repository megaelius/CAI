We present exponential finite-sample nonasymptotic deviation inequalities for the SAA estimator's
near-optimal solution set over the class of stochastic optimization problems with heavy-tailed
random \emph{convex} functions in the objective and constraints. Such setting is better suited
for problems where a sub-Gaussian data generating distribution is less expected, e.g., in stochastic
portfolio optimization. One of our contributions is to exploit \emph{convexity} of the perturbed
objective and the perturbed constraints as a property which entails \emph{localized} deviation
inequalities for joint feasibility and optimality guarantees. This means that our bounds are significantly
tighter in terms of diameter and metric entropy since they depend only on the near-optimal solution
set but not on the whole feasible set. As a result, we obtain a much sharper sample complexity estimate
when compared to a general nonconvex problem. In our analysis, we derive some localized deterministic
perturbation error bounds for convex optimization problems which are of independent interest.
To obtain our results, we only assume a metric regular convex feasible set, possibly not satisfying
the Slater condition and not having a metric regular solution set. In this general setting, joint
near feasibility and near optimality are guaranteed. If in addition the set satisfies the Slater
condition, we obtain finite-sample simultaneous \emph{exact} feasibility and near optimality
guarantees (for a sufficiently small tolerance). Another contribution of our work is to present,
as a proof of concept of our localized techniques, a persistent result for a variant of the LASSO estimator
under very weak assumptions on the data generating distribution. 