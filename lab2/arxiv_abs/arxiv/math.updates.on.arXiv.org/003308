We develop a methodology to prove geometric convergence of the parameter sequence $\{\theta_n\}_{n\geq
0}$ of a stochastic algorithm. The convergence is measured via a function $\Psi$ that is similar
to a Lyapunov function. Important algorithms that motivate the introduction of this methodology
are stochastic algorithms deriving from optimization methods solving deterministic optimization
problems. Among them, we are especially interested in analyzing comparison-based algorithms
that typically derive from stochastic approximation algorithms with a constant step-size. We
employ the so-called ODE method that relates a stochastic algorithm to its mean ODE, along with the
Lyapunov-like function $\Psi$ such that the geometric convergence of $\Psi(\theta_n)$ implies---in
the case of a stochastic optimization algorithm---the geometric convergence of the expected distance
between the optimum of the optimization problem and the search point generated by the algorithm.
We provide two sufficient conditions such that $\Psi(\theta_n)$ decreases at a geometric rate.
First, $\Psi$ should decrease "exponentially" along the solution to the mean ODE. Second, the deviation
between the stochastic algorithm and the ODE solution (measured with the function $\Psi$) should
be bounded by $\Psi(\theta_n)$ times a constant. We provide in addition practical conditions that
allow to verify easily the two sufficient conditions without knowing in particular the solution
of the mean ODE. Our results are any-time bounds on $\Psi(\theta_n)$, so we can deduce not only asymptotic
upper bound on the convergence rate, but also the first hitting time of the algorithm. The main results
are applied to two comparison-based stochastic algorithms with a constant step-size for optimization
on discrete and continuous domains. 