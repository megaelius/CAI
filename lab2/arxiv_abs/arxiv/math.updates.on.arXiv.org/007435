In this paper, we give explicit descriptions of versions of (Local-) Backtracking Gradient Descent
and New Q-Newton's method to the Riemannian setting. Then, we combine Nash's embedding theorem
and some good local retractions to extend our previous convergence results for the above methods
on Euclidean and Banach/Hilbert spaces to Riemannian manifolds. Here are some easy to state consequences
of results in this paper for Morse's functions, here X is a general Riemannian manifold of finite
dimension and $f:X\rightarrow \mathbb{R}$ a $C^2$ function which is Morse (that is, all its critical
points are non-degenerate). {\bf Theorem.} For random choices of the hyperparameters in the Riemanian
Local Backtracking Gradient Descent algorithm and for random choices of the initial point $x_0$,
the sequence $\{x_n\}$ constructed by the algorithm either (i) converges to a local minimum of $f$
or (ii) eventually leaves every compact subsets of $X$ (in other words, diverges to infinity on $X$).
If $f$ has compact sublevels, then only the former alternative happens. The convergence rate is
the same as in the classical paper by Armijo. {\bf Theorem.} Assume that $f$ is $C^3$. For random choices
of the hyperparametes in the Riemannian New Q-Newton's method, if the sequence constructed by the
algorithm converges, then the limit is a critical point. We have a Stable-Center manifold theorem,
near saddle points of $f$, for the dynamical system associated to the algorithm. If the limit point
is a non-degenerate minimum point, then the rate of convergence is quadratic. If moreover $X$ is
an open subset of a Lie group and the initial point $x_0$ is chosen randomly, then we can globally avoid
saddle points. 