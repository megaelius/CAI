In the realm of big data and machine learning, data-parallel, distributed stochastic algorithms
have drawn significant attention in the present days.~While the synchronous versions of these
algorithms are well understood in terms of their convergence, the convergence analyses of their
asynchronous counterparts are not widely studied. In this paper, we propose and analyze a {\it distributed,
asynchronous parallel} SGD in light of solving an arbitrary consistent linear system by reformulating
the system into a stochastic optimization problem as studied by Richt\'{a}rik and Tak\'{a}\~{c}
in [35]. We compare the convergence rates of our asynchronous SGD algorithm with the synchronous
parallel algorithm proposed by Richt\'{a}rik and Tak\'{a}\v{c} in [35] under different choices
of the hyperparameters---the stepsize, the damping factor, the number of processors, and the delay
factor. We show that our asynchronous parallel SGD algorithm also enjoys a global linear convergence
rate, similar to the {\em basic} method and the synchronous parallel method in [35] for solving any
arbitrary consistent linear system via stochastic reformulation. We also show that our asynchronous
parallel SGD improves upon the {\em basic} method with a better convergence rate when the number
of processors is larger than four. We further show that this asynchronous approach performs asymptotically
better than its synchronous counterpart for certain linear systems. Moreover, for certain linear
systems, we compute the minimum number of processors required for which our asynchronous parallel
SGD is better, and find that this number can be as low as two for some ill-conditioned problems. 