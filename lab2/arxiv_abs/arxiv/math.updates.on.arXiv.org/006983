This paper focuses on stochastic methods for solving smooth non-convex strongly-concave min-max
problems, which have received increasing attention due to their potential applications in deep
learning (e.g., deep AUC maximization). However, most of the existing algorithms are slow in practice,
and their analysis revolves around the convergence to a nearly stationary point. We consider leveraging
the Polyak-\L ojasiewicz (PL) condition to design faster stochastic algorithms with stronger
convergence guarantee. Although PL condition has been utilized for designing many stochastic
minimization algorithms, their applications for non-convex min-max optimization remains rare.
In this paper, we propose and analyze proximal epoch-based methods, and establish fast convergence
in terms of both {\bf the primal objective gap and the duality gap}. Our analysis is interesting in
threefold: (i) it is based on a novel Lyapunov function that consists of the primal objective gap
and the duality gap of a regularized function; (ii) it only requires a weaker PL condition for establishing
the primal objective convergence than that required for the duality gap convergence; (iii) it yields
the optimal dependence on the accuracy level $\epsilon$, i.e., $O(1/\epsilon)$. We also make explicit
the dependence on the problem parameters and explore regions of weak convexity parameter that lead
to improved dependence on condition numbers. Experiments on deep AUC maximization demonstrate
the effectiveness of our methods. Our method also beats the 1st place on {\bf Stanford CheXpert competition}
in terms of AUC on the public validation set. 