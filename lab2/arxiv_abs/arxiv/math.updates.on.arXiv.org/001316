Sufficient dimension reduction (SDR) provides a framework for reducing the predictor space dimension
in regression problems. We consider SDR in the context of deterministic functions of several variables
such as those arising in computer experiments. In this context, SDR serves as a methodology for uncovering
ridge structure in functions, and two primary algorithms for SDR---sliced inverse regression
(SIR) and sliced average variance estimation (SAVE)---approximate matrices of integrals using
a sliced mapping of the response. We interpret this sliced approach as a Riemann sum approximation
of the particular integrals arising in each algorithm. We employ well-known tools from numerical
analysis---namely, multivariate tensor product Gauss-Christoffel quadrature and orthogonal
polynomials---to produce new algorithms that improve upon the Riemann sum-based numerical integration
in SIR and SAVE. We call the new algorithms Lanczos-Stieltjes inverse regression (LSIR) and Lanczos-Stieltjes
average variance estimation (LSAVE) due to their connection with Stieltjes' method---and Lanczos'
related discretization---for generating a sequence of polynomials that are orthogonal to a given
measure. We show that the quadrature-based approach approximates the desired integrals, and we
study the behavior of LSIR and LSAVE with three numerical examples. As expected in high order numerical
integration, the quadrature-based LSIR and LSAVE exhibit exponential convergence in the integral
approximations compared to the first order convergence of the classical SIR and SAVE. The disadvantage
of LSIR and LSAVE is that the underlying tensor product quadrature suffers from the curse of dimensionality---that
is, the number of quadrature nodes grows exponentially with the input space dimension. Therefore,
the proposed approach is most appropriate for deterministic functions with fewer than ten independent
inputs. 