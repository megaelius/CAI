Modern machine learning models are often so complex that they achieve vanishing classification
error on the training set. Max-margin linear classifiers are among the simplest classification
methods that have zero training error (with linearly separable data). Despite this simplicity,
their high-dimensional behavior is not yet completely understood. We assume to be given i.i.d.
data $(y_i,{\boldsymbol x}_i)$, $i\le n$ with ${\boldsymbol x}_i\sim {\sf N}({\boldsymbol 0},{\boldsymbol
\Sigma})$ a $p$-dimensional Gaussian feature vector, and $y_i \in\{+1,-1\}$ a label whose distribution
depends on a linear combination of the covariates $\langle {\boldsymbol \theta}_*,{\boldsymbol
x}_i\rangle$. We consider the proportional asymptotics $n,p\to\infty$ with $p/n\to \psi$, and
derive exact expressions for the limiting prediction error. Our asymptotic results match simulations
already when $n,p$ are of the order of a few hundreds. We explore several choices for the the pair $({\boldsymbol
\theta}_*,{\boldsymbol \Sigma})$, and show that the resulting generalization curve (test error
error as a function of the overparametrization ratio $\psi=p/n$) is qualitatively different,
depending on this choice. In particular we consider a specific structure of $({\boldsymbol \theta}_*,{\boldsymbol
\Sigma})$ that captures the behavior of nonlinear random feature models or, equivalently, two-layers
neural networks with random first layer weights. In this case, we observe that the test error is monotone
decreasing in the number of parameters. This finding agrees with the recently developed `double
descent' phenomenology for overparametrized models. 