The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers that achieve a minimal
type II error while enforcing the prioritized type I error controlled under some user-specified
level $\alpha$. This paradigm serves naturally in applications such as severe disease diagnosis
and spam detection, where people have clear priorities among the two error types. Recently, Tong,
Feng and Li (2018) proposed a nonparametric umbrella algorithm that adapts all scoring-type classification
methods (e.g., logistic regression, support vector machines, random forest) to respect the given
type I error upper bound $\alpha$ with high probability, without specific distributional assumptions
on the features and the responses. Universal the umbrella algorithm is, it demands an explicit minimum
sample size requirement on class $0$, which is often the more scarce class, such as in rare disease
diagnosis applications. In this work, we employ the parametric linear discriminant analysis (LDA)
model and propose a new parametric thresholding algorithm, which does not need the minimum sample
size requirements on class $0$ observations and thus is suitable for small sample applications
such as rare disease diagnosis. Leveraging both the existing nonparametric and the newly proposed
parametric thresholding rules, we propose four LDA-based NP classifiers, for both low- and high-dimensional
settings. On the theoretical front, we prove NP oracle inequalities for one proposed classifier,
where the rate for excess type II error benefits from the explicit parametric model assumption.
Furthermore, as NP classifiers involve a sample splitting step of class $0$ observations, we construct
a new adaptive sample splitting scheme that can be applied universally to NP classifiers, and this
adaptive strategy reduces the type II error of these classifiers. 