An optimal experimental set-up maximizes the value of data for statistical inferences and predictions.
An optimal set-up is particularly important for experiments that are time consuming or expensive
to perform. In the context of partial differential equations (PDEs), multilevel methods have been
proven to dramatically reduce the computational complexity of their single-level counterparts.
Here, two multilevel methods, which efficiently compute the expected information gain using a
Kullback-Leibler divergence measure in simulation-based Bayesian optimal experimental design,
are proposed. The first method is a multilevel double loop Monte Carlo (MLDLMC) with importance
sampling that greatly reduces the computational work of the inner loop. The second proposed method
is a multilevel double loop stochastic collocation (MLDLSC) with importance sampling, which performs
a high-dimensional integration by deterministic quadrature on sparse grids. In both methods,
the Laplace approximation is used as an effective means of importance sampling, and the optimal
values of the method parameters are determined by minimizing the average computational work, subject
to a desired error tolerance. The computational efficiencies of the methods are demonstrated by
computing the expected information gain from an electrical impedance tomography experiment where
the fiber orientation in composite laminate materials are inferred through Bayesian inversion.
MLDLSC performs better than MLDLMC when the regularity of the underlying computational model,
with respect to the additive noise and the unknown parameters, can be exploited. 