It is a common saying that testing for conditional independence, i.e., testing whether X is independent
of Y, given Z, is a hard statistical problem if Z is a continuous random variable. In this paper, we
prove that conditional independence is indeed a particularly difficult hypothesis to test for.
Statistical tests are required to have a size that is smaller than a predefined significance level,
and different tests usually have power against a different class of alternatives. We prove that
a valid test for conditional independence does not have power against any alternative. Given the
non-existence of a uniformly valid conditional independence test, we argue that tests must be designed
so their suitability for a particular problem setting may be judged easily. To address this need,
we propose in the case where X and Y are univariate to nonlinearly regress X on Z, and Y on Z and then compute
a test statistic based on the sample covariance between the residuals, which we call the generalised
covariance measure (GCM). We prove that validity of this form of test relies almost entirely on the
weak requirement that the regression procedures are able to estimate the conditional means X given
Z, and Y given Z, at a slow rate. We extend the methodology to handle settings where X and Y may be multivariate
or even high-dimensional. While our general procedure can be tailored to the setting at hand by combining
it with any regression technique, we develop the theoretical guarantees for kernel ridge regression.
A simulation study shows that the test based on GCM is competitive with state of the art conditional
independence tests. Code will be available as an R package. 