In analysis of multi-component complex systems, such as neural systems, identifying groups of
units that share similar functionality will aid understanding of the underlying structures of
the system. To find such a grouping, it is useful to evaluate to what extent the units of the system
are separable. Separability or inseparability can be evaluated by quantifying how much information
would be lost if the system were partitioned into subsystems, and the interactions between the subsystems
were hypothetically removed. A system of two independent subsystems are completely separable
without any loss of information while a system of strongly interacted subsystems cannot be separated
without a large loss of information. Among all the possible partitions of a system, the partition
that minimizes the loss of information, called the Minimum Information Partition (MIP), can be
considered as the optimal partition for characterizing the underlying structures of the system.
Although the MIP would reveal novel characteristics of the neural system, an exhaustive search
for the MIP is numerically intractable due to the combinatorial explosion of possible partitions.
Here, we propose a computationally efficient search to precisely identify the MIP among all possible
partitions by exploiting the submodularity of the measure of information loss. Mutual information
is one such submodular information loss functions, and is a natural choice for measuring the degree
of statistical dependence between paired sets of random variables. By using mutual information
as a loss function, we show that the search for MIP can be performed in a practical order of computational
time for a reasonably large system. We also demonstrate that MIP search allows for the detection
of underlying global structures in a network of nonlinear oscillators. 