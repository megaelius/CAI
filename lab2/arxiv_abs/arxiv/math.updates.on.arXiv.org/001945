In this contribution we are concerned with tight a posteriori error estimation for projection based
model order reduction of $\inf$-$\sup$ stable parameterized variational problems. In particular,
we consider the Reduced Basis Method in a Petrov-Galerkin framework, where the reduced approximation
spaces are constructed by the (weak) Greedy algorithm. We propose and analyze a hierarchical a posteriori
error estimator which evaluates the difference of two reduced approximations of different accuracy.
Based on the a priori error analysis of the (weak) Greedy algorithm, it is expected that the hierarchical
error estimator is sharp with efficiency index close to one, if the Kolmogorov N-with decays fast
for the underlying problem and if a suitable saturation assumption for the reduced approximation
is satisfied. We investigate the tightness of the hierarchical a posteriori estimator both from
a theoretical and numerical perspective. For the respective approximation with higher accuracy
we study and compare basis enrichment of Lagrange- and Taylor-type reduced bases. Numerical experiments
indicate the efficiency for both, the construction of a reduced basis using the hierarchical error
estimator in a weak Greedy algorithm, and for tight online certification of reduced approximations.
This is particularly relevant in cases where the $\inf$-$\sup$ constant may become small depending
on the parameter. In such cases a standard residual-based error estimator -- complemented by the
successive constrained method to compute a lower bound of the parameter dependent $\inf$-$\sup$
constant -- may become infeasible. 