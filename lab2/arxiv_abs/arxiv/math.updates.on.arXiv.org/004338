The multi-population replicator dynamics (RD) can be considered a dynamic approach to the study
of multi-player games, where it was shown to be related to Cross' learning, as well as of systems of
coevolving populations. However, not all of its equilibria are Nash equilibria (NE) of the underlying
game, and neither convergence to an NE nor convergence in general are guaranteed. Although interior
equilibria are guaranteed to be NE, no interior equilibrium can be asymptotically stable in the
multi-population RD, resulting, e.g., in cyclic orbits around a single interior NE. We introduce
a new notion of equilibria of RD, called mutation limits, which is based on the inclusion of a naturally
arising, simple form of mutation, but is invariant under the specific choice of mutation parameters.
We prove the existence of such mutation limits for a large range of games, and consider a subclass
of particular interest, that of attracting mutation limits. Attracting mutation limits are approximated
by asymptotically stable equilibria of the (mutation-)perturbed RD, and hence, offer an approximate
dynamic solution of the underlying game, especially if the original dynamic has no asymptotically
stable equilibria. In this sense, mutation stabilises the system in certain cases and makes attracting
mutation limits near-attainable. Furthermore, the relevance of attracting mutation limits as
a game theoretic equilibrium concept is emphasised by a similarity of (mutation-)perturbed RD
to the Q-learning algorithm in the context of multi-agent reinforcement learning. In contrast
to the guaranteed existence of mutation limits, attracting mutation limits do not exist in all games,
raising the question of their characterization. 