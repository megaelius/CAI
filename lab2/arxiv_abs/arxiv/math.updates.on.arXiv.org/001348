Several statistical approaches based on reproducing kernels have been proposed to detect abrupt
changes arising in the full distribution of the observations and not only in the mean or variance.
Some of these approaches enjoy good statistical properties (oracle inequality, \ldots). Nonetheless,
they have a high computational cost both in terms of time and memory. This makes their application
difficult even for small and medium sample sizes ($n< 10^4$). This computational issue is addressed
by first describing a new efficient and exact algorithm for kernel multiple change-point detection
with an improved worst-case complexity that is quadratic in time and linear in space. It allows dealing
with medium size signals (up to $n \approx 10^5$). Second, a faster but approximation algorithm
is described. It is based on a low-rank approximation to the Gram matrix. It is linear in time and space.
This approximation algorithm can be applied to large-scale signals ($n \geq 10^6$). These exact
and approximation algorithms have been implemented in \texttt{R} and \texttt{C} for various kernels.
The computational and statistical performances of these new algorithms have been assessed through
empirical experiments. The runtime of the new algorithms is observed to be faster than that of other
considered procedures. Finally, simulations confirmed the higher statistical accuracy of kernel-based
approaches to detect changes that are not only in the mean. These simulations also illustrate the
flexibility of kernel-based approaches to analyze complex biological profiles made of DNA copy
number and allele B frequencies. An R package implementing the approach will be made available on
github. 