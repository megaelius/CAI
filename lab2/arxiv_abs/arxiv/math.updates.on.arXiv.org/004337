We present a probabilistic analysis of the long-time behaviour of the nonlocal, diffusive equations
with a gradient flow structure in 2-Wasserstein metric, namely, the Mean-Field Langevin Dynamics
(MFLD). Our work is motivated by a desire to provide a theoretical underpinning for the convergence
of stochastic gradient type algorithms widely used for non-convex learning tasks such as training
of deep neural networks. The key insight is that the certain class of the finite dimensional non-convex
problems becomes convex when lifted to infinite dimensional space of measures. We leverage this
observation and show that the corresponding energy functional defined on the space of probability
measures has a unique minimiser which can be characterised by a first order condition using the notion
of linear functional derivative. Next, we show that the flow of marginal laws induced by the MFLD
converges to the stationary distribution which is exactly the minimiser of the energy functional.
We show that this convergence is exponential under conditions that are satisfied for highly regularised
learning tasks. At the heart of our analysis is a pathwise perspective on Otto calculus used in gradient
flow literature which is of independent interest. Our proof of convergence to stationary probability
measure is novel and it relies on a generalisation of LaSalle's invariance principle. Importantly
we do not assume that interaction potential of MFLD is of convolution type nor that has any particular
symmetric structure. This is critical for applications. Finally, we show that the error between
finite dimensional optimisation problem and its infinite dimensional limit is of order one over
the number of parameters. 