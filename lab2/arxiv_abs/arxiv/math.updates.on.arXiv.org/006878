Traditional deep learning models are trained at a centralized server using labeled data samples
collected from end devices or users. Such data samples often include private information, which
the users may not be willing to share. Federated learning (FL) is an emerging approach to train such
learning models without requiring the users to share their possibly private labeled data. In FL,
each user trains its copy of the learning model locally. The server then collects the individual
updates and aggregates them into a global model. A major challenge that arises in this method is the
need of each user to efficiently transmit its learned model over the throughput limited uplink channel.
In this work, we tackle this challenge using tools from quantization theory. In particular, we identify
the unique characteristics associated with conveying trained models over rate-constrained channels,
and propose a suitable quantization scheme for such settings, referred to as universal vector quantization
for FL (UVeQFed). We show that combining universal vector quantization methods with FL yields a
decentralized training system in which the compression of the trained models induces only a minimum
distortion. We then theoretically analyze the distortion, showing that it vanishes as the number
of users grows. We also characterize the convergence of models trained with the traditional federated
averaging method combined with UVeQFed to the model which minimizes the loss function. Our numerical
results demonstrate the gains of UVeQFed over previously proposed methods in terms of both distortion
induced in quantization and accuracy of the resulting aggregated model. 