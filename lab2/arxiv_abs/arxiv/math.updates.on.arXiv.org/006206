This book deals with functions allowing to express the dissimilarity (discrepancy) between two
data fields or ''divergence functions'' with the aim of applications to linear inverse problems.
Most of the divergences found in the litterature are used in the field of information theory to quantify
the difference between two probability density functions, that is between positive data whose
sum is equal to one. In such context, they take a simplified form that is not adapted to the problems
considered here, in which the data fields are non-negative but with a sum not necessarily equal to
one. In a systematic way, we reconsider the classical divergences and we give their forms adapted
to inverse problems. To this end, we will recall the methods allowing to build such divergences,
and propose some generalizations. The resolution of inverse problems implies systematically
the minimisation of a divergence between physical measurements and a model depending of the unknown
parameters. In the context image reconstruction, the model is generally linear and the constraints
that must be taken into account are the non-negativity as well as (if necessary) the sum constraint
of the unknown parameters. To take into account in a simple way the sum constraint, we introduce the
class of scale invariant or affine invariant divergences. Such divergences remains unchanged
when the model parameters are multiplied by a constant positive factor. We show the general properties
of the invariance factors, and we give some interesting characteristics of such divergences.An
extension of such divergences allows to obtain the property of invariance with respect to both the
arguments of the divergences; this characteristic can be used to introduce the smoothness regularization
of inverse problems, that is a regularisation in the sense of Tikhonov.We then develop in a last step,
minimisation methods of the divergences subject to non-negativity and sum constraints on the solution
components. These methods are founded on the Karush-Kuhn-Tucker conditions that must be fulfilled
at the optimum. The Tikhonov regularization is considered in these methods.Chapter 11 associated
with Appendix 9 deal with the application to the NMF, while Chapter 12 is dedicated to the Blind Deconvolution
problem.In these two chapters, the interest of the scale invariant divergences is highlighted.
