This work centers around results related to Proposition 21 of Pitman and Yor's (1997) paper on the
two parameter Poisson Dirichlet distribution indexed by (\alpha,\theta) for 0<\alpha<1,
also \alpha=0, and \theta>-\alpha, denoted PD(\alpha,\theta). We develop explicit stick-breaking
representations for a class that contains the PD(\alpha,\theta) for the range \theta=0, and \theta>0,
we call PG(\alpha,\zeta). We also construct a larger class, EPG(\alpha,\zeta), containing the
entire range. These classes are indexed by \alpha, and an arbitrary non-negative random variable
\zeta. The bulk of this work focuses on investigating various properties of this larger class, EPG(\alpha,\zeta),
which lead to connections to other work in the literature. In particular, we develop completely
explicit stick-breaking representations for this entire class via size biased sampling as described
in Perman, Pitman and Yor (1992). This represents the first case outside of the PD(\alpha,\theta)
where one obtains explicit results for the entire range of \alpha, the EPG are within the larger class
of mass partitions generated by conditioning on the total mass of an \alpha-stable subordinator.
Furthermore Markov chains are derived which establish links between Markov chains derived from
stick-breaking(insertion/deletion), as described in Perman, Pitman and Yor and Markov chains
derived from successive usage of dual coagulation fragmentation operators described in Bertoin
and Goldschmidt and Dong, Goldschmidt and Martin. Which have connections to certain types of fragmentation
trees and coalescents appearing in the recent literature. Our results are also suggestive of new
models and tools, for applications in Bayesian Nonparametrics/Machine Learning, where PD(\alpha,\theta)
bridges are often referred to as Pitman-Yor processes. 