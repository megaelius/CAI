Markov Decision Processes (MDP) are a widely used model for dynamic decision-making problems.
However, MDPs require precise specification of model parameters, and often the cost of a policy
can be highly sensitive to the estimated parameters. Robust MDPs ameliorate this issue by allowing
one to specify uncertainty sets around the parameters, which leads to a non-convex optimization
problem. This non-convex problem can be solved via the Value Iteration (VI) algorithm, but VI requires
repeatedly solving convex programs that become prohibitively expensive as MDPs grow larger. We
propose a first-order methods (FOM) algorithmic framework, where we interleave approximate value
iteration update with a first-order-based computation of the robust Bellman update. Our algorithm
relies on having a proximal setup for the uncertainty sets. We go on to instantiate this proximal
setup for $s$-rectangular ellipsoidal uncertainty sets and Kullback-Leibler (KL) uncertainty
sets. By carefully controlling the warm-starts of our FOM and the increasing approximation rate
at each VI iteration, our algorithm achieves a convergence rate of $O \left( A^{2} S^{3}\log(S)\log(\epsilon^{-1})
\epsilon^{-1} \right)$ for the best choice of parameters, where $S,A$ are the numbers of states
and actions. Our dependence on the number of states and actions is significantly better than that
of VI algorithms. In numerical experiments on ellipsoidal uncertainty sets we show that our algorithm
is significantly more scalable than state-of-the-art approaches. In the class of $s$-rectangular
robust MDPs, to the best of our knowledge, our algorithm is the only one addressing KL uncertainty
sets. It is also the only one to solve ellipsoidal uncertainty sets to optimality when the state and
actions spaces become on the order of several hundreds. 