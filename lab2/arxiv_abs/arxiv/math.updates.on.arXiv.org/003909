Many large scale problems in computational fluid dynamics such as uncertainty quantification,
Bayesian inversion, data assimilation and PDE constrained optimization are considered very challenging
computationally as they require a large number of expensive (forward) numerical solutions of the
corresponding PDEs. We propose a machine learning algorithm, based on deep artificial neural networks,
that learns the underlying input parameters to observable map from a few training samples (computed
realizations of this map). By a judicious combination of theoretical arguments and empirical observations,
we find suitable network architectures and training hyperparameters that result in robust and
efficient neural network approximations of the parameters to observable map. Numerical experiments
for realistic high dimensional test problems, demonstrate that even with approximately 100 training
samples, the resulting neural networks have a prediction error of less than one to two percent, at
a computational cost which is several orders of magnitude lower than the cost of the underlying PDE
solver. Moreover, we combine the proposed deep learning algorithm with Monte Carlo (MC) and Quasi-Monte
Carlo (QMC) methods to efficiently compute uncertainty propagation for nonlinear PDEs. Under
the assumption that the underlying neural networks generalize well, we prove that the deep learning
MC and QMC algorithms are guaranteed to be faster than the baseline (quasi-) Monte Carlo methods.
Numerical experiments demonstrating one to two orders of magnitude speed up over baseline QMC and
MC algorithms, for the intricate problem of computing probability distributions of the observable,
are also presented. 