The Lanczos method is one of the most powerful and fundamental techniques for solving an extremal
symmetric eigenvalue problem. Convergence-based error estimates are well studied, with the estimate
depending heavily on the eigenvalue gap. However, in practice, this gap is often relatively small,
resulting in significant overestimates of error. One way to avoid this issue is through the use of
uniform error estimates, namely, bounds that depend only on the dimension of the matrix and the number
of iterations. In this work, we prove a number of upper and lower uniform error estimates for the Lanczos
method. These results include the first known lower bounds for error in the Lanczos method and significantly
improved upper bounds for error measured in the $p$-norm, $p>1$. These lower bounds imply that the
maximum error of $m$ iterations of the Lanczos method over all $n \times n$ symmetric matrices does
indeed depend on the dimension $n$. The improved upper and lower bounds for extremal eigenvalues
translates immediately to error estimates for the condition number of a symmetric positive definite
matrix. In addition, we prove more specific results for matrices that possess some level of eigenvalue
regularity or whose eigenvalues converge to some limiting empirical spectral distribution. In
particular, if this limiting distribution can be bounded above and below by a Jacobi weight function,
then the relative error after $m$ iterations is of order $m^{-2}$ with high probability. Through
numerical experiments, we show that the theoretical estimates of this paper do apply to practical
computations for reasonably sized matrices. 