Asynchronous distributed algorithms are a popular way to reduce synchronization costs in large-scale
optimization, and in particular for neural network training. However, for nonsmooth and nonconvex
objectives, few convergence guarantees exist beyond cases where closed-form proximal operator
solutions are available. As most popular contemporary deep neural networks lead to nonsmooth and
nonconvex objectives, there is now a pressing need for such convergence guarantees. In this paper,
we analyze for the first time the convergence of stochastic asynchronous optimization for this
general class of objectives. In particular, we focus on stochastic subgradient methods allowing
for block variable partitioning, where the shared-memory-based model is asynchronously updated
by concurrent processes. To this end, we first introduce a probabilistic model which captures key
features of real asynchronous scheduling between concurrent processes; under this model, we establish
convergence with probability one to an invariant set for stochastic subgradient methods with momentum.
From the practical perspective, one issue with the family of methods we consider is that it is not
efficiently supported by machine learning frameworks, as they mostly focus on distributed data-parallel
strategies. To address this, we propose a new implementation strategy for shared-memory based
training of deep neural networks, whereby concurrent parameter servers are utilized to train a
partitioned but shared model in single- and multi-GPU settings. Based on this implementation,
we achieve on average 1.2x speed-up in comparison to state-of-the-art training methods for popular
image classification tasks without compromising accuracy. 