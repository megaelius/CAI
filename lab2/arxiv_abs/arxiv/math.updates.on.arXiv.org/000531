One of the central problems in the classification of individual test sequences (e.g. genetic analysis),
is that of checking for the similarity of sample test sequences as compared with a set of much longer
training sequences. This is done by a set of classifiers for test sequences of length N, where each
of the classifiers is trained by the training sequences so as to minimize the classification error
rate when fed with each of the training sequences. It should be noted that the storage of long training
sequences is considered to be a serious bottleneck in the next generation sequencing for Genome
analysis Some popular classification algorithms adopt a probabilistic approach, by assuming
that the sequences are realizations of some variable-length Markov process or a hidden Markov process
(HMM), thus enabling the imbeding of the training data onto a variable-length Suffix-tree, the
size of which is usually linear in $N$, the length of the test sequence. Despite of the fact that it
is not assumed here that the sequences are realizations of probabilistic processes (an assumption
that does not seem to be fully justified when dealing with biological data), it is demonstrated that
"feature-based" classifiers, where particular substrings (called "features" or markers) are
sought in a set of "big data" training sequences may be based on a universal compaction of the training
data that is contained in a set of $t$ (long) individual training sequences, onto a suffix-tree with
no more than O(N) leaves, regardless of how long the training sequence is, at only a vanishing increase
in the classification error rate. 