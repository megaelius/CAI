We introduce a new class of extremum seeking controllers able to achieve fixed time convergence
to the solution of optimization problems defined by static and dynamical systems. Unlike existing
approaches in the literature, the convergence time of the proposed algorithms does not depend on
the initial conditions and it can be prescribed a priori by tuning the parameters of the controller.
Specifically, our first contribution is a novel gradient-based extremum seeking algorithm for
cost functions that satisfy the Polyak-Lojasiewicz (PL) inequality with some coefficient \kappa
> 0, and for which the extremum seeking controller guarantees a fixed upper bound on the convergence
time that is independent of the initial conditions but dependent on the coefficient \kappa. Second,
in order to remove the dependence on \kappa, we introduce a novel Newton-based extremum seeking
algorithm that guarantees a fully assignable fixed upper bound on the convergence time, thus paralleling
existing asymptotic results in Newton-based extremum seeking where the rate of convergence is
fully assignable. Finally, we study the problem of optimizing dynamical systems, where the cost
function corresponds to the steady-state input-to-output map of a stable but unknown dynamical
system. In this case, after a time scale transformation is performed, the proposed extremum seeking
controllers achieve the same fixed upper bound on the convergence time as in the static case. Our
results exploit recent gradient flow structures proposed by Garg and Panagou in [3], and are established
by using averaging theory and singular perturbation theory for dynamical systems that are not necessarily
Lipschitz continuous. We confirm the validity of our results via numerical simulations that illustrate
the key advantages of the extremum seeking controllers presented in this paper. 