Mixture models are a natural choice in many applications, but it can be difficult to place an apriori
upper bound on the number of components. To circumvent this, investigators are turning increasingly
to Dirichlet process mixture models (DPMMs) and, more generally, Pitman-Yor mixtures. These models
are well suited to Bayesian density estimation. An interesting question is whether they can be turned
to the problem of {\em classification} or {\em clustering}, which involves allocating observations
to clusters. This is becoming increasingly widely used among investigators. This article considers
the MAP (maximal posterior partition) clustering for the Gauss-Gauss DPM (where the cluster means
have Gaussian distribution and, for each cluster, the observations within the cluster have Gaussian
distribution; the number and sizes of the clusters generated according to a Chinese Restaurant
Process). It is proved that the convex hulls of the clusters created by the MAP are pairwise `almost
disjoint' (they may have at most one point in common). Let $n$ denote the number of data points. It
is proved that, for each $R < \infty$ the size of the smallest cluster that intersects the ball of radius
$R$ in the MAP is comparable with $n$ as $n \rightarrow \infty $. Consequently, the number of clusters
that intersect the ball radius $R$ in the MAP for $R < \infty$ remains bounded as $n \rightarrow \infty$.
Furthermore, if the data arises from independent identically distributed sampling from a given
distribution $P$ with bounded support then the asymptotic MAP partition of the observation space
maximises a function which has a straightforward expression; it is the difference between the variance
of a linear transformation of the within-component means and the entropy of the partition. 