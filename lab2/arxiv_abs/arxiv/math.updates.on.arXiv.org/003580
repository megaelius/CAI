Benford's Law predicts that the first significant digit on the leftmost side of numbers in real-life
data is proportioned between all possible 1 to 9 digits approximately as in LOG(1 + 1/digit), so that
low digits occur much more frequently than high digits in the first place. The two essential prerequisites
for data configuration with regards to compliance with Benford's Law are high order of magnitude
and positive skewness with a tail falling to the right of the histogram, so that quantitative configuration
is such that the small is numerous and the big is rare. A related topic in the study of Benford's Law
is the stark contrast between multiplications and additions of random variables and their distinct
resultant quantitative and digital configurations. Random multiplication processes induce
substantial increase in order of magnitude and they tend to the skewed Lognormal Distribution,
favoring the small over the big. Random addition processes on the other hand do not induce any increase
in order of magnitude and they tend to the symmetrical Normal Distribution as predicated by the Central
Limit Theorem, favoring the medium over the small and the big. Thus, while multiplication processes
are highly conducive to Benford behavior, addition processes are highly detrimental to Benford
behavior. In this article it is shown that often in real-life data, multiplication and addition
processes mix together within one measurement or expression, and consequently they fiercely compete
for dominance, each attempting to exert the greatest influence upon sizes and digits. Such tugs
of war between additions and multiplications are won or lost depending on the orders of magnitude
of the generating random variables, as well as on the relative strength of the two warring sides,
measured in terms of the comparative arithmetical involvement in the algebraic expression of the
process. 