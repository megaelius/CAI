The field of compressed sensing has become a major tool in high-dimensional analysis, with the realization
that vectors can be recovered from relatively very few linear measurements as long as the vectors
lie in a low-dimensional structure, typically the vectors that are zero in most coordinates with
respect to a basis. However, there are many applications where we instead want to recover vectors
that are sparse not with respect to a basis, but rather to a general dictionary. That is, the vector
can be written as the linear combination of very few columns of a matrix $\mathbf{D}$, where the columns
of $\mathbf{D}$ form a (typically overcomplete) spanning set. In this direction, we show that as
an matrix $\mathbf{D}$ stays bounded away from zero in norm on a set $S$ and a provided map ${\boldsymbol
\Phi}$ comprised of i.i.d. subgaussian rows has number of measurements at least proportional to
the square of $w(\mathbf{D}S)$, the Gaussian width of the related set $\mathbf{D}S$, then with
high probability the composition ${\boldsymbol \Phi} \mathbf{D}$ also stays bounded away from
zero in norm on $S$ with bound proportional to $w(\mathbf{D}S)$. This result has potential as a powerful
tool in dimension reduction analysis. As a specific application, we obtain that the null space property
is preserved under such subgaussian maps with high probability. This result is nearly optimal in
the sense that we require only a minimal condition on $\mathbf{D}$. Consequently we obtain stable
recovery guarantees for dictionary-sparse signals via the $\ell_1$-synthesis method, which
is typically challenging, even with random measurements. 