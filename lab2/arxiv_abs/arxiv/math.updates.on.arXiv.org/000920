The aim of this paper is to describe an (empirical) Bayes estimator for parametric and nonparametric
regression functions with good frequentist properties. Our estimator is the posterior distribution
based on a proper objective prior for the regression function, which we call I-prior. We show that
the posterior mean under the I-prior has some potential advantages over the Tikhonov regularizer
because the latter tends to undersmooth while the I-prior estimator does not. Our asymptotic results,
small sample simulations, and real data analyses all show a significant advantage for the I-prior
methodology. We assume the regression function lies in a reproducing kernel Hilbert space (RKHS)
over some covariate space. The I-prior is defined as the distribution maximizing entropy subject
to a suitable constraint based on the Fisher information for the regression function, and turns
out to be Gaussian with mean chosen a priori, and covariance kernel proportional to the Fisher information.
This has the intuitively appealing property that the more information is available about a linear
functional of the regression function, the larger its prior variance, and, broadly speaking, the
less influential the prior is on the posterior. The I-prior methodology has some particularly nice
properties if the regression function is assumed to lie in a centered fractional Brownian motion
(FBM) RKHS over Euclidean space. Firstly, one parameter less needs to be estimated than using various
standard kernels (e.g., exponential or Matern); secondly, I-prior realizations are H\"older
continuous of order between 0 and 2, which is a suitable range for many applications and may be more
convenient than the range 0 to 1 for FBM process realizations. The I-prior methodology is implemented
in the R-package iprior (Jamil, 2017). 