This paper analyzes block-coordinate proximal gradient methods for minimizing the sum of a separable
smooth function and a (nonseparable) nonsmooth function, both of which are allowed to be nonconvex.
The main tool in our analysis is the forward-backward envelope (FBE), which serves as a particularly
suitable continuous and real-valued Lyapunov function. Global and linear convergence results
are established when the cost function satisfies the Kurdyka-\L ojasiewicz property without imposing
convexity requirements on the smooth function. Two prominent special cases of the investigated
setting are regularized finite sum minimization and the sharing problem; in particular, an immediate
byproduct of our analysis leads to novel convergence results and rates for the popular Finito/MISO
algorithm in the nonsmooth and nonconvex setting with very general sampling strategies. This paper
analyzes block-coordinate proximal gradient methods for minimizing the sum of a separable smooth
function and a (nonseparable) nonsmooth function, both of which are allowed to be nonconvex. The
main tool in our analysis is the forward-backward envelope (FBE), which serves as a particularly
suitable continuous and real-valued Lyapunov function. Global and linear convergence results
are established when the cost function satisfies the Kurdyka-\L ojasiewicz property without imposing
convexity requirements on the smooth function. Two prominent special cases of the investigated
setting are regularized finite sum minimization and the sharing problem; in particular, an immediate
byproduct of our analysis leads to novel convergence results and rates for the popular Finito/MISO
algorithm in the nonsmooth and nonconvex setting with very general sampling strategies. 