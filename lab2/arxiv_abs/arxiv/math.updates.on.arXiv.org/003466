This thesis addresses the interplay between asymptotic hypothesis testing and entropy inequalities
in quantum information theory. In the first part of the thesis we focus on hypothesis testing. We
consider two main settings; one can either fix quantum states while optimizing over measurements
or fix a measurement and evaluate its capability to discriminate by optimizing over states. In the
former setting, we prove a composite quantum Stein's Lemma. We also discuss how this gives an operational
interpretation to several quantities of interest. For the latter, we give the optimal asymptotic
error rates in several symmetric and asymmetric settings, and discuss properties and examples
of these rates. In the second part, the focus is shifted to entropy inequalities. We start with recoverability
inequalities. Using tools developed to prove the composite Stein's Lemma, we give a strengthened
lower bound on the conditional quantum mutual information (CQMI). Next, we give an operational
interpretation to the relative entropy of recovery via hypothesis testing. Then, we discuss some
recent counterexamples, which show that the relative entropy of recovery is not a lower bound on
the CQMI; we provide more counterexamples where some systems are classical. We then turn to a seemingly
different type of entropy inequalities called bounds on information combining. Using a particular
recoverability inequality, we give a lower bound and additionally conjecture optimal lower and
upper bounds. Furthermore, we discuss implications of our bounds to the finite blocklength behavior
of Polar codes. Finally, we discuss Renyi-$2$ entropy inequalities for Gaussian states, by exploiting
their formulation as log-det inequalities to find recoverability related bounds on several quantities.
We apply this to Gaussian steerability and entanglement measures, proving their monogamy and several
other features. 