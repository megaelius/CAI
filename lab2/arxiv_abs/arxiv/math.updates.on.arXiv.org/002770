A coupling of two distributions $P_{X}$ and $P_{Y}$ is a joint distribution $P_{XY}$ with marginal
distributions equal to $P_{X}$ and $P_{Y}$. Given marginals $P_{X}$ and $P_{Y}$ and a real-valued
function $f$ of the joint distribution $P_{XY}$, what is its minimum over all couplings $P_{XY}$
of $P_{X}$ and $P_{Y}$? We study the asymptotics of such coupling problems with different $f$'s
and with $X$ and $Y$ replaced by $X^{n}=(X_{1},\ldots,X_{n})$ and $Y^{n}=(Y_{1},\ldots,Y_{n})$
where $X_{i}$ and $Y_{i}$ are i.i.d.\ copies of random variables $X$ and $Y$ with distributions
$P_{X}$ and $P_{Y}$ respectively. These include the maximal coupling, minimum distance coupling,
maximal guessing coupling, and minimum entropy coupling problems. We characterize the limiting
values of these coupling problems as $n$ tends to infinity. We show that they typically converge
at least exponentially fast to their limits. Moreover, for the problems of maximal coupling and
minimum excess-distance probability coupling, we also characterize (or bound) the optimal convergence
rates (exponents). Furthermore, for the maximal guessing coupling problem we show that it is equivalent
to the distribution approximation problem. Therefore, some existing results for the latter problem
can be used to derive the asymptotics of the maximal guessing coupling problem. We also study the
asymptotics of the maximal guessing coupling problem for two \emph{general} sources and a generalization
of this problem, named the \emph{maximal guessing coupling through a channel problem}. We apply
the preceding results to several new information-theoretic problems, including exact intrinsic
randomness, exact resolvability, channel capacity with input distribution constraint, and perfect
stealth and secrecy communication. 