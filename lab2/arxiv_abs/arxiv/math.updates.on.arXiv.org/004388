High dimensional error covariance matrices and their inverses are used to weight the contribution
of observation and background information in data assimilation procedures. As observation error
covariance matrices are often obtained by sampling methods, estimates are often degenerate or
ill-conditioned, making it impossible to invert an observation error covariance matrix without
the use of techniques to reduce its condition number. In this paper we present new theory for two existing
methods that can be used to recondition any covariance matrix: ridge regression, and the minimum
eigenvalue method. We compare these methods with multiplicative variance inflation, which cannot
alter the condition number of a matrix, but is often used to account for neglected correlation information.
We investigate the impact of reconditioning on variances and correlations of a general covariance
matrix in both a theoretical and practical setting. The new theory shows that, for the same target
condition number, both methods increase variances compared to the original matrix, with larger
increases for ridge regression than the minimum eigenvalue method. We prove that the ridge regression
method strictly decreases the absolute value of off-diagonal correlations. Theoretical comparison
of the impact of reconditioning and multiplicative variance inflation on the data assimilation
objective function shows that variance inflation alters information across all scales uniformly,
whereas reconditioning has a larger effect on scales corresponding to smaller eigenvalues. We
then consider two examples: a general correlation function, and an observation error covariance
matrix arising from interchannel correlations. The minimum eigenvalue method results in smaller
overall changes to the correlation matrix than ridge regression, but can increase off-diagonal
correlations. 