How does a 110-layer ResNet learn a high-complexity classifier using relatively few training examples
and short training time? We present a theory towards explaining this in terms of hierarchical learning.
We refer hierarchical learning as the learner learns to represent a complicated target function
by decomposing it into a sequence of simpler functions to reduce sample and time complexity. This
paper formally analyzes how multi-layer neural networks can perform such hierarchical learning
efficiently and automatically by applying SGD. On the conceptual side, we present, to the best of
our knowledge, the FIRST theory result indicating how deep neural networks can be sample and time
efficient on certain hierarchical learning tasks, when NO KNOWN non-hierarchical algorithms
(such as kernel method, linear regression over feature mappings, tensor decomposition, sparse
coding, and their simple combinations) are efficient. We establish a principle called "backward
feature correction", where training higher layers in the network can improve the features of lower
level ones. We believe this is the key to understand the deep learning process in multi-layer neural
networks. On the technical side, we show for every input dimension $d > 0$, there is a concept class
consisting of degree $\omega(1)$ multi-variate polynomials so that, using $\omega(1)$-layer
neural networks as learners, SGD can learn any target function from this class in $\mathsf{poly}(d)$
time using $\mathsf{poly}(d)$ samples to any $\frac{1}{\mathsf{poly}(d)}$ error, through learning
to represent it as a composition of $\omega(1)$ layers of quadratic functions. In contrast, we present
lower bounds stating that several non-hierarchical learners, including any kernel methods, neural
tangent kernels, must suffer from $d^{\omega(1)}$ sample or time complexity to learn this concept
class even to $d^{-0.01}$ error. 