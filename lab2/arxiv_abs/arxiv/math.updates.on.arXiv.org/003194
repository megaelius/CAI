In this paper, we develop a new computational approach which is based on minimizing the difference
of two convex functionals (DC) to solve a broader class of phase retrieval problems. The approach
splits a standard nonlinear least squares minimizing function associated with the phase retrieval
problem into the difference of two convex functions and then solves a sequence of convex minimization
sub-problems. For each subproblem, the Nesterov's accelerated gradient descent algorithm or
the Barzilai-Borwein (BB) algorithm is used. In the setting of sparse phase retrieval, a standard
$\ell_1$ norm term is added into the minimization mentioned above. The subproblem is approximated
by a proximal gradient method which is solved by the shrinkage-threshold technique directly without
iterations. In addition, a modified Attouch-Peypouquet technique is used to accelerate the iterative
computation. These lead to more effective algorithms than the Wirtinger flow (WF) algorithm and
the Gauss-Newton (GN) algorithm and etc.. A convergence analysis of both DC based algorithms shows
that the iterative solutions is convergent linearly to a critical point and will be closer to a global
minimizer than the given initial starting point. Our study is a deterministic analysis while the
study for the Wirtinger flow (WF) algorithm and its variants, the Gauss-Newton (GN) algorithm,
the trust region algorithm is based on the probability analysis. In particular, the DC based algorithms
are able to retrieve solutions using a number $m$ of measurements which is about twice of the number
$n$ of entries in the solution with high frequency of successes. When $m\approx n$, the $\ell_1$
DC based algorithm is able to retrieve sparse signals. 