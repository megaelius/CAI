In this paper, by using tools of second-order variational analysis, we study the popular forward-backward
splitting method with Beck-Teboulle's line-search for solving convex optimization problem where
the objective function can be split into the sum of a differentiable function and a possible nonsmooth
function. We first establish that this method exhibits global convergence to an optimal solution
of the problem (if it exists) without the usual assumption that the gradient of the differentiable
function involved is globally Lipschitz continuous. We also obtain the $o(k^{-1})$ complexity
for the functional value sequence when this usual assumption is weaken from global Lipschitz continuity
to local Lipschitz continuity, improving the existing $\mathcal{O}(k^{-1})$ complexity result.
We then derive the local and global Q-linear convergence of the method in terms of both the function
value sequence and the iterative sequence, under a general metric subregularity assumption which
is automatically satisfied for convex piecewise-linear-quadratic optimization problems. In
particular, we provide verifiable sufficient conditions for metric subregularity assumptions,
and so, local and global Q-linear convergence of the proposed method for broad structured optimization
problems arise in machine learning and signal processing including Poisson linear inverse problem,
the partly smooth optimization problems, as well as the $\ell_1$-regularized optimization problems.
Our results complement the current literature by providing $Q$-linear convergence result to the
forward-backward splitting method under weaker assumptions. Moreover, via this approach, we
obtain several full characterizations for the uniqueness of optimal solution to Lasso problem,
which covers some recent results in this direction. 