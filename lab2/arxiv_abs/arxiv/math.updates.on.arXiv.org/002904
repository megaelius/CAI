The development of new classification and regression algorithms based on empirical risk minimization
(ERM) over deep neural network hypothesis classes, coined Deep Learning, revolutionized the area
of artificial intelligence, machine learning, and data analysis. More recently, these methods
have been applied to the numerical solution of high dimensional PDEs with great success. In particular,
recent simulations indicate that deep learning based algorithms are capable of overcoming the
curse of dimensionality for the numerical solution of linear Kolmogorov PDEs. Kolmogorov PDEs
have been widely used in models from engineering, finance, and the natural sciences. Nearly all
approximation methods for Kolmogorov PDEs in the literature suffer under the curse of dimensionality.
By contrast, in recent work by some of the authors it was shown that deep ReLU neural networks are capable
of approximating solutions of Kolmogorov PDEs without incurring the curse of dimensionality.
The present paper considerably strengthens these results by providing an analysis of the generalization
error. In particular we show that for Kolmogorov PDEs with affine drift and diffusion coefficients
and a given accuracy $\varepsilon>0$, ERM over deep neural network hypothesis classes of size scaling
polynomially in the dimension $d$ and $\varepsilon^{-1}$ and with a number of training samples
scaling polynomially in the dimension $d$ and $\varepsilon^{-1}$ approximates the solution of
the Kolmogorov PDE to within accuracy $\varepsilon$ with high probability. We conclude that ERM
over deep neural network hypothesis classes breaks the curse of dimensionality for the numerical
solution of linear Kolmogorov PDEs with affine drift and diffusion coefficients. To the best of
our knowledge this is the first rigorous mathematical result that proves the efficiency of deep
learning methods for high dimensional problems. 