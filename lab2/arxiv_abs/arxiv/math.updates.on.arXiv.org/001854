Recently, deep learning approaches with various network architectures have achieved significant
performance improvement over existing iterative reconstruction methods in various imaging problems.
However, it is still unclear why these deep learning architectures work for specific inverse problems.
To address these issues, here we show that the long-searched-for missing link is the convolution
framelets for representing a signal by convolving local and non-local bases. The convolution framelets
was originally developed to generalize the theory of low-rank Hankel matrix approaches for inverse
problems, and this paper further extends the idea so that we can obtain a deep neural network using
multilayer convolution framelets with perfect reconstruction (PR) under rectilinear linear
unit nonlinearity (ReLU). Our analysis also shows that the popular deep network components such
as residual block, redundant filter channels, and concatenated ReLU (CReLU) do indeed help to achieve
the PR, while the pooling and unpooling layers should be augmented with high-pass branches to meet
the PR condition. Moreover, by changing the number of filter channels and bias, we can control the
shrinkage behaviors of the neural network. This discovery leads us to propose a novel theory for
deep convolutional framelets neural network. Using numerical experiments with various inverse
problems, we demonstrated that our deep convolution framelets network shows consistent improvement
over existing deep architectures.This discovery suggests that the success of deep learning is
not from a magical power of a black-box, but rather comes from the power of a novel signal representation
using non-local basis combined with data-driven local basis, which is indeed a natural extension
of classical signal processing theory. 