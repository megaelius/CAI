Over the last few years there have been dramatic advances in our understanding of mathematical and
computational models of complex systems in the presence of uncertainty. This has led to a growth
in the area of uncertainty quantification as well as the need to develop efficient, scalable, stable
and convergent computational methods for solving differential equations with random inputs.
Stochastic Galerkin methods based on polynomial chaos expansions have shown superiority to other
non-sampling and many sampling techniques. However, for complicated governing equations numerical
implementations of stochastic Galerkin methods can become non-trivial. On the other hand, Monte
Carlo and other traditional sampling methods, are straightforward to implement. However, they
do not offer as fast convergence rates as stochastic Galerkin. Other numerical approaches are the
stochastic collocation (SC) methods, which inherit both, the ease of implementation of Monte Carlo
and the robustness of stochastic Galerkin to a great deal. However, stochastic collocation and
its powerful extensions, e.g. sparse grid stochastic collocation, can simply fail to handle more
levels of complication. The seemingly innocent Burgers equation driven by Brownian motion is such
an example. In this work we propose a novel enhancement to stochastic collocation methods using
locally improved deterministic model reduction techniques that can handle this pathological
example and hopefully other more complicated equations like Stochastic Navier-Stokes. Local
improvements to reduced-order models are achieved using sensitivity analysis of the proper orthogonal
decomposition. Our numerical results show that the proposed technique is not only reliable and
robust but also very efficient. 