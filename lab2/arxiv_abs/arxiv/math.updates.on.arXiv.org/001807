The conventional channel resolvability problem refers to the determination of the minimum rate
needed for an input process to approximate the output distribution of a channel in either the total
variation distance or the relative entropy. In contrast to previous works, in this paper, we use
the (normalized or unnormalized) R\'enyi divergence (with the R\'enyi parameter in [0,2]) to measure
the level of approximation. We also provide asymptotic expressions for normalized R\'enyi divergence
when the R\'enyi parameter is larger than or equal to $1$ as well as (lower and upper) bounds for the
case when the same parameter is smaller than $1$. We characterize the minimum rate needed to ensure
that the R\'enyi resolvability vanishes asymptotically. The optimal rates are the same for both
the normalized and unnormalized cases. In addition, the minimum rate when the R\'enyi parameter
no larger than $1$ equals the minimum mutual information over all input distributions that induce
the target output distribution similarly to the traditional case. When the R\'enyi parameter is
larger than $1$ the minimum rate is, in general, larger than the mutual information. The optimal
R\'enyi resolvability is proven to vanish at least exponentially fast for both of these two cases,
as long as the code rate is larger than the minimum admissible one. The optimal exponential rate of
decay for i.i.d. random codes is also characterized exactly. We apply these results to the wiretap
channel, and completely characterize the optimal tradeoff between the rates of the secret and non-secret
messages when the leakage measure is given by the (unnormalized) R\'enyi divergence. This tradeoff
differs from the conventional setting when the leakage is measured by the traditional mutual information.
