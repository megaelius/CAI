We introduce a unified algorithmic framework, called proximal-like incremental aggregated gradient
(PLIAG) method, for minimizing the sum of a convex function that consists of additive relatively
smooth convex components and a proper lower semi-continuous convex regularization function,
over an abstract feasible set whose geometry can be captured by using the domain of a Legendre function.
The PLIAG method includes many existing algorithms in the literature as special cases such as the
proximal gradient method, the Bregman proximal gradient method (also called NoLips algorithm),
the incremental aggregated gradient method, the incremental aggregated proximal method, and
the proximal incremental aggregated gradient method. It also includes some novel interesting
iteration schemes. First we show the PLIAG method is globally sublinearly convergent without requiring
a growth condition, which extends the sublinear convergence result for the proximal gradient algorithm
to incremental aggregated type first order methods. Then by embedding a so-called Bregman distance
growth condition into a descent-type lemma to construct a special Lyapunov function, we show that
the PLIAG method is globally linearly convergent in terms of both function values and Bregman distances
to the optimal solution set, provided that the step size is not greater than some positive constant.
These convergence results derived in this paper are all established beyond the standard assumptions
in the literature (i.e., without requiring the strong convexity and the Lipschitz gradient continuity
of the smooth part of the objective). When specialized to many existing algorithms, our results
recover or supplement their convergence results under strictly weaker conditions. 