It is one of the most challenging issues in applied mathematics to approximately solve high-dimensional
partial differential equations (PDEs) and most of the numerical approximation methods for PDEs
in the scientific literature suffer from the so-called curse of dimensionality in the sense that
the number of computational operations employed in the corresponding approximation scheme to
obtain an approximation precision $\varepsilon>0$ grows exponentially in the PDE dimension and/or
the reciprocal of $\varepsilon$. Recently, certain deep learning based approximation methods
for PDEs have been proposed and various numerical simulations for such methods suggest that deep
neural network (DNN) approximations might have the capacity to indeed overcome the curse of dimensionality
in the sense that the number of real parameters used to describe the approximating DNNs grows at most
polynomially in both the PDE dimension $d\in\mathbb{N}$ and the reciprocal of the prescribed accuracy
$\varepsilon>0$. There are now also a few rigorous results in the scientific literature which substantiate
this conjecture by proving that DNNs overcome the curse of dimensionality in approximating solutions
of PDEs. Each of these results establishes that DNNs overcome the curse of dimensionality in approximating
suitable PDE solutions at a fixed time point $T>0$ and on a compact cube $[a,b]^d$ in space but none
of these results provides an answer to the question whether the entire PDE solution on $[0,T]\times
[a,b]^d$ can be approximated by DNNs without the curse of dimensionality. It is precisely the subject
of this article to overcome this issue. More specifically, the main result of this work in particular
proves for every $a\in\mathbb{R}$, $ b\in (a,\infty)$ that solutions of certain Kolmogorov PDEs
can be approximated by DNNs on the space-time region $[0,T]\times [a,b]^d$ without the curse of
dimensionality. 