We study the problem of recovering a hidden binary $k$-sparse $p$-dimensional vector $\beta$ from
$n$ noisy linear observations $Y=X\beta+W$ where $X_{ij}$ are i.i.d. $\mathcal{N}(0,1)$ and
$W_i$ are i.i.d. $\mathcal{N}(0,\sigma^2)$. A closely related hypothesis testing problem is
to distinguish the pair $(X,Y)$ generated from this structured model from a corresponding null
model where $(X,Y)$ consist of purely independent Gaussian entries. In the low sparsity $k=o(p)$
and high signal to noise ratio $k/\sigma^2=\Omega\left(1\right)$ regime, we establish an `All-or-Nothing'
information-theoretic phase transition at a critical sample size $n^*=2 k\log \left(p/k\right)
/\log \left(1+k/\sigma^2\right)$, resolving a conjecture of \cite{gamarnikzadik}. Specifically,
we show that if $\liminf_{p\to \infty} n/n^*>1$, then the maximum likelihood estimator almost
perfectly recovers the hidden vector with high probability and moreover the true hypothesis can
be detected with a vanishing error probability. Conversely, if $\limsup_{p\to \infty} n/n^*<1$,
then it becomes information-theoretically impossible even to recover an arbitrarily small but
fixed fraction of the hidden vector support, or to test hypotheses strictly better than random guess.
Our proof of the impossibility result builds upon two key techniques, which could be of independent
interest. First, we use a conditional second moment method to upper bound the Kullback-Leibler
(KL) divergence between the structured and the null model. Second, inspired by the celebrated area
theorem, we establish a lower bound to the minimum mean squared estimation error of the hidden vector
in terms of the KL divergence between the two models. 