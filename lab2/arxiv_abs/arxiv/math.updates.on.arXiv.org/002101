In this work, we describe advanced numerical tools for working with multivariate functions and
for the analysis of large data sets. These tools will drastically reduce the required computing
time and the storage cost, and, therefore, will allow us to consider much larger data sets or finer
meshes. Covariance matrices are crucial in spatio-temporal statistical tasks, but are often very
expensive to compute and store, especially in 3D. Therefore, we approximate covariance functions
by cheap surrogates in a low-rank tensor format. We apply the Tucker and canonical tensor decompositions
to a family of Matern- and Slater-type functions with varying parameters and demonstrate numerically
that their approximations exhibit exponentially fast convergence. We prove the exponential convergence
of the Tucker and canonical approximations in tensor rank parameters. Several statistical operations
are performed in this low-rank tensor format, including evaluating the conditional covariance
matrix, spatially averaged estimation variance, computing a quadratic form, determinant, trace,
loglikelihood, inverse, and Cholesky decomposition of a large covariance matrix. Low-rank tensor
approximations reduce the computing and storage costs essentially. For example, the storage cost
is reduced from an exponential $\mathcal{O}(n^d)$ to a linear scaling $\mathcal{O}(drn)$, where
$d$ is the spatial dimension, $n$ is the number of mesh points in one direction, and $r$ is the tensor
rank. Prerequisites for applicability of the proposed techniques are the assumptions that the
data, locations, and measurements lie on a tensor (axes-parallel) grid and that the covariance
function depends on a distance, $\Vert x-y \Vert$. 