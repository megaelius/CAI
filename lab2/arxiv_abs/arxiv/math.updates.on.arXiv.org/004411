The future of high-performance computing, specifically the next generation of Exascale computers,
will presumably see memory capacity and bandwidth fail to keep pace with data generation. Current
strategies proposed to address this bottleneck entail the omission of large fractions of data,
as well as the incorporation of $\textit{in situ}$ compression algorithms to avoid overuse of memory.
To ensure that post-processing operations are successful, this must be done in a way that ensures
that a sufficiently accurate representation of the solution is stored. Moreover, in situations
in which the input/output system becomes a bottleneck in analysis, visualization, etc., the number
of passes made over the input data must be minimized. In the interest of addressing this problem,
this work focuses on the application of pass-efficient compressive matrix decompositions to high-dimensional
simulation data from turbulent particle-laden flows. It also includes the presentation of a novel
single-pass matrix decomposition algorithm for computing interpolative decompositions. The
methods are described extensively and numerical experiments at $Re_{\tau} = 180$ and $St^{+}=
0, 1, 10$ are performed. In the unladen channel flow case, compression factors exceeding 400 are
achieved while maintaining accuracy with respect to first- and second-order flow statistics.
In the particle-laden case, compression factors between 10 and 150 with relative reconstruction
errors of ${\cal O}(10^{-3})$ are achieved. These results show that these methods can enable efficient
computation of various quantities of interest in both the carrier and disperse phases. These algorithms
are easily parallelized and can be incorporated directly into solvers, which will allow for effective
$\textit{in situ}$ compression of large-scale simulation data. 