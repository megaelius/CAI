Benford's Law predicts that the first significant digit on the leftmost side of numbers in real-life
data is proportioned between all possible 1 to 9 digits approximately as in LOG(1 + 1/digit), so that
low digits occur much more frequently than high digits in the first place. For example, digit 1 occurs
approximately 30.1% in the first place in random numbers, while digit 9 occurs only approximately
4.6%. In this article it is shown that a process where a large enough set of identical quantities constantly
alternates between minuscule random consolidations (summing two randomly chosen values into
a singular value) and tiny random fragmentations (division of one randomly chosen value into two
new values) converges digit-wise to the Benford proportions after sufficiently many such cycles.
The statistical tendency of the system after numerous cycles is to have approximately 2/3 multiplicative
expressions which are conducive to Benford behavior as they tend to the Lognormal Distribution,
and 1/3 additive expressions which are detrimental to Benford behavior as they tend to the Normal
Distribution, hence the process represents in essence a tug of war between addition and multiplication.
Since the process encounters the so-called Achilles' heel of the Central Limit Theorem, namely
additions of skewed distributions with high order of magnitude, additions are not very effective,
and the war is decisively won by multiplication, leading to Benford behavior. Randomness in selecting
the particular quantity to be fragmented, as well as randomness in selecting the two particular
quantities to be consolidated, is essential for convergence. Not surprisingly then, fragmentation
itself could be performed either randomly say via a realization from the continuous Uniform on (0,
1), or deterministically via any fixed split ratio such as say 25% - 75%, and Benford's Law emerges
in either case. 