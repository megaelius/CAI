Motivated from invariance principle and recent developments in artificial intelligence, we introduce
a risk-sensitive linear quadratic optimal control whose solution resembles a deep neural network,
called deep teams. Two notions of equivariant and partially equivariant systems are defined for
linear quadratic models. It is shown that such systems can be partitioned into a few sub-populations
of decision makers wherein the decision makers in each sub-population are coupled in both dynamics
and cost function through a number of linear regressions of the states and actions of the decision
makers. Two non-classical information structures are considered: deep-state sharing and partial
deep-state sharing, where deep state refers to the linear regression of the states of the decision
makers in each sub-population. For deep-state sharing structure, a closed-form representation
of the globally optimal strategy is obtained in terms of a deep Riccati equation, whose dimension
is independent of the number of decision makers in each sub-population, i.e., the solution is scalable.
In addition, two sub-optimal sequential strategies under partial deep-state sharing information
structure are proposed by introducing two Kalman-like filters, one based on the finite-population
model and the other one based on the infinite-population model. It is shown that the prices of information
associated with the above sub-optimal solutions converge to zero as the number of decision makers
goes to infinity. Furthermore, an explicit connection between optimal control theory and deep
learning is established wherein it is proved that a simple deep neural network can incorporate an
arbitrary number of orthonormal features, substantiating the fact that deep neural networks are
structurally rich enough to model complex dynamical systems. 