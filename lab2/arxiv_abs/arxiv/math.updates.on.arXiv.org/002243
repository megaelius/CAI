In this paper, we analyze the convergence of the alternating direction method of multipliers (ADMM)
for minimizing a nonconvex and possibly nonsmooth objective function, $\phi(x_0,\ldots,x_p,y)$,
subject to coupled linear equality constraints. Our ADMM updates each of the primal variables $x_0,\ldots,x_p,y$,
followed by updating the dual variable. We separate the variable $y$ from $x_i$'s as it has a special
role in our analysis. The developed convergence guarantee covers a variety of nonconvex functions
such as piecewise linear functions, $\ell_q$ quasi-norm, Schatten-$q$ quasi-norm ($0<q<1$),
minimax concave penalty (MCP), and smoothly clipped absolute deviation (SCAD) penalty. It also
allows nonconvex constraints such as compact manifolds (e.g., spherical, Stiefel, and Grassman
manifolds) and linear complementarity constraints. Also, the $x_0$-block can be almost any lower
semi-continuous function. By applying our analysis, we show, for the first time, that several ADMM
algorithms applied to solve nonconvex models in statistical learning, optimization on manifold,
and matrix decomposition are guaranteed to converge. Our results provide sufficient conditions
for ADMM to converge on (convex or nonconvex) monotropic programs with three or more blocks, as they
are special cases of our model. ADMM has been regarded as a variant to the augmented Lagrangian method
(ALM). We present a simple example to illustrate how ADMM converges but ALM diverges with bounded
penalty parameter $\beta$. Indicated by this example and other analysis in this paper, ADMM might
be a better choice than ALM for some nonconvex \emph{nonsmooth} problems, because ADMM is not only
easier to implement, it is also more likely to converge for the concerned scenarios. 