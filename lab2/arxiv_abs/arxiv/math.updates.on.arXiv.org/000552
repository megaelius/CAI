We address the numerical solution of infinite-dimensional inverse problems in the framework of
Bayesian inference. In the Part I companion to this paper (arXiv.org: 1308.1313 ), we considered
the linearized infinite-dimensional inverse problem. Here in Part II, we relax the linearization
assumption and consider the fully nonlinear infinite-dimensional inverse problem using a Markov
chain Monte Carlo (MCMC) sampling method. To address the challenges of sampling high-dimensional
pdfs arising from Bayesian inverse problems governed by PDEs, we build on the stochastic Newton
MCMC method. This method exploits problem structure by taking as a proposal density a local Gaussian
approximation of the posterior pdf, whose construction is made tractable by invoking a low-rank
approximation of its data misfit component of the Hessian. Here we introduce an approximation of
the stochastic Newton proposal in which we compute the low-rank-based Hessian at just the MAP point,
and then reuse this Hessian at each MCMC step. We compare the performance of the proposed method to
the original stochastic Newton MCMC method and to an independence sampler. The comparison of the
three methods is conducted on a synthetic ice sheet inverse problem. For this problem, the stochastic
Newton MCMC method with a MAP-based Hessian converges at least as rapidly as the original stochastic
Newton MCMC method, but is far cheaper since it avoids recomputing the Hessian at each step. On the
other hand, it is more expensive per sample than the independence sampler; however, its convergence
is significantly more rapid, and thus overall it is much cheaper. Finally, we present extensive
analysis and interpretation of the posterior distribution, and classify directions in parameter
space based on the extent to which they are informed by the prior or the observations. 