We consider that at every instant each member of a population, which we refer to as an agent, selects
one strategy out of a finite set. The agents are nondescript, and their strategy choices are described
by the so-called population state vector, whose entries are the portions of the population selecting
each strategy. Likewise, each entry constituting the so-called payoff vector is the reward attributed
to a strategy. We consider that a general finite-dimensional nonlinear dynamical system, denoted
as payoff dynamical model (PDM), describes a mechanism that determines the payoff as a causal map
of the population state. A bounded-rationality protocol, inspired primarily on evolutionary
biology principles, governs how each agent revises its strategy repeatedly based on complete or
partial knowledge of the population state and payoff. The population is protocol-homogeneous
but is otherwise strategy-heterogeneous considering that the agents are allowed to select distinct
strategies concurrently. A stochastic mechanism determines the instants when agents revise their
strategies, but we consider that the population is large enough that, with high probability, the
population state can be approximated with arbitrary accuracy uniformly over any finite horizon
by a so-called (deterministic) mean population state. We propose an approach that takes advantage
of passivity principles to obtain sufficient conditions determining, for a given protocol and
PDM, when the mean population state is guaranteed to converge to a meaningful set of equilibria,
which could be either an appropriately defined extension of Nash's for the PDM or a perturbed version
of it. By generalizing and unifying previous work, our framework also provides a foundation for
future work. 