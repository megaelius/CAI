We investigate the properties of a Block Decomposition Method (BDM), which extends the power of
a Coding Theorem Method (CTM) that approximates local estimations of algorithmic complexity and
of logical depth based upon Solomonoff-Levin's theory of algorithmic probability providing a
closer connection to algorithmic complexity than previous attempts based on statistical regularities
e.g. as spotted by some popular lossless compression schemes. The strategy behind BDM is to find
small computer programs that produce the components of a larger, decomposed object. The set of short
computer programs can then be artfully arranged in sequence so as to produce the original object
and to estimate an upper bound on the greatest length of the shortest computer program that produces
said original object. We show that the method provides efficient estimations of algorithmic complexity
but that it performs like Shannon entropy when it loses accuracy. We estimate errors and study the
behaviour of BDM for different boundary conditions, all of which are compared and assessed in detail.
The measure may be adapted for use with more multi-dimensional objects than strings, objects such
as arrays and tensors. To test the measure we provide proofs of the algorithmic complexity relations
among dual, isomorphic and cospectral graphs. Finally, we introduce a measure based upon the seminal
concept of logical depth whose numerical comparisons to CTM and BDM agree with the theoretical expectations.
We provide implementations in most major programming languages --Mathematica, Matlab, R, Java,
Perl, Python, Pascal, C++, and Haskell-- and a free online algorithmic complexity calculator.
