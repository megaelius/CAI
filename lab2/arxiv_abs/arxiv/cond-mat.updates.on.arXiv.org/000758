A major obstacle to understanding neural coding and computation is the fact that experimental recordings
typically sample only a small fraction of the neurons in a circuit. Measured neural properties are
skewed by interactions between recorded neurons and the "hidden" portion of the network. To properly
interpret neural data and determine how biological structure gives rise to neural circuit function,
we thus need a better understanding of the relationships between measured effective neural properties
and the true underlying physiological properties. Here, we focus on how the effective spatiotemporal
dynamics of the synaptic interactions between neurons are reshaped by coupling to unobserved neurons.
We find that the effective interactions from a pre-synaptic neuron $r'$ to a post-synaptic neuron
$r$ can be decomposed into a sum of the true interaction from $r'$ to $r$ plus corrections from every
directed path from $r'$ to $r$ through unobserved neurons. Importantly, the resulting formula
reveals when the hidden units have---or do not have---major effects on reshaping the interactions
among observed neurons. As a particular example of interest, we derive a formula for the impact of
hidden units in random networks with "strong" coupling---connection weights that scale with $1/\sqrt{N}$,
where $N$ is the network size, precisely the scaling observed in recent experiments. With this quantitative
relationship between measured and true interactions, we can study how network properties shape
effective interactions, which properties are relevant for neural computations, and how to manipulate
effective interactions. 