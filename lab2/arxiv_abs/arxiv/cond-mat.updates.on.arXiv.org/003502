The length and time scales of atomistic simulations are limited by the computational cost of the
methods used to predict material properties. In recent years there has been great progress in the
use of machine learning algorithms to develop fast and accurate interatomic potential models,
but it remains a challenge to develop models that generalize well and are fast enough to be used at
extreme time and length scales. To address this challenge, we have developed a machine learning
algorithm based on symbolic regression in the form of genetic programming that is capable of discovering
accurate, computationally efficient manybody potential models. The key to our approach is to explore
a hypothesis space of models based on fundamental physical principles and select models within
this hypothesis space based on their accuracy, speed, and simplicity. The focus on simplicity reduces
the risk of overfitting the training data and increases the chances of discovering a model that generalizes
well. Our algorithm was validated by rediscovering an exact Lennard-Jones potential and a Sutton
Chen embedded atom method potential from training data generated using these models. By using training
data generated from density functional theory calculations, we found potential models for elemental
copper that are simple, as fast as embedded atom models, and capable of accurately predicting properties
outside of their training set. Our approach requires relatively small sets of training data, making
it possible to generate training data using highly accurate methods at a reasonable computational
cost. We present our approach, the forms of the discovered models, and assessments of their transferability,
accuracy and speed. 