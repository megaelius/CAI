The standard Hopfield model for associative neural networks accounts for biological Hebbian learning
and acts as the harmonic oscillator for pattern recognition, however its maximal storage capacity
is $\alpha \sim 0.14$, far from the theoretical bound for symmetric networks, i.e. $\alpha =1$.
Inspired by sleeping and dreaming mechanisms in mammal brains, we propose an extension of this model
displaying the standard on-line (awake) learning mechanism (that allows the storage of external
information in terms of patterns) and an off-line (sleep) unlearning$\&$consolidating mechanism
(that allows spurious-pattern removal and pure-pattern reinforcement): this obtained daily
prescription is able to saturate the theoretical bound $\alpha=1$, remaining also extremely robust
against thermal noise. Both neural and synaptic features are analyzed both analytically and numerically.
In particular, beyond obtaining a phase diagram for neural dynamics, we focus on synaptic plasticity
and we give explicit prescriptions on the temporal evolution of the synaptic matrix. We analytically
prove that our algorithm makes the Hebbian kernel converge with high probability to the projection
matrix built over the pure stored patterns. Furthermore, we obtain a sharp and explicit estimate
for the "sleep rate" in order to ensure such a convergence. Finally, we run extensive numerical simulations
(mainly Monte Carlo sampling) to check the approximations underlying the analytical investigations
(e.g., we developed the whole theory at the so called replica-symmetric level, as standard in the
Amit-Gutfreund-Sompolinsky reference framework) and possible finite-size effects, finding
overall full agreement with the theory. 