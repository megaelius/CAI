Perceptual manifolds arise when a neural population responds to an ensemble of sensory signals
associated with different physical features (e.g., orientation, pose, scale, location, and intensity)
of the same perceptual object. Object recognition and discrimination requires classifying the
manifolds in a manner that is insensitive to variability within a manifold. How neuronal systems
give rise to invariant object classification and recognition is a fundamental problem in brain
theory as well as in machine learning. Here we study the ability of a readout network to classify objects
from their perceptual manifold representations. We develop a statistical mechanical theory for
the linear classification of manifolds with arbitrary geometry revealing a remarkable relation
to the mathematics of conic decomposition. Novel geometrical measures of manifold radius and manifold
dimension are introduced which can explain the classification capacity for manifolds of various
geometries. The general theory is demonstrated on a number of representative manifolds, including
L2 ellipsoids prototypical of strictly convex manifolds, L1 balls representing polytopes consisting
of finite sample points, and orientation manifolds which arise from neurons tuned to respond to
a continuous angle variable, such as object orientation. The effects of label sparsity on the classification
capacity of manifolds are elucidated, revealing a scaling relation between label sparsity and
manifold radius. Theoretical predictions are corroborated by numerical simulations using recently
developed algorithms to compute maximum margin solutions for manifold dichotomies. Our theory
and its extensions provide a powerful and rich framework for applying statistical mechanics of
linear classification to data arising from neuronal responses to object stimuli, as well as to artificial
deep networks trained for object recognition tasks. 