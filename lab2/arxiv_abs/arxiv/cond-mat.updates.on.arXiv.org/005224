RNNs are popular dynamical models, used for processing sequential data. Prior theoretical work
in understanding the properties of RNNs has focused on models with additive interactions, where
the input to a unit is a weighted sum of the output of the remaining units in network. However, there
is ample evidence that neurons can have gating - i.e. multiplicative - interactions. Such gating
interactions have significant effects on the collective dynamics of the network. Furthermore,
the best performing RNNs in machine learning have gating interactions. Thus, gating interactions
are beneficial for information processing and learning tasks. We develop a dynamical mean-field
theory (DMFT) of gating to understand the dynamical regimes produced by gating. Our gated RNN reduces
to the classical RNNs in certain limits and is closely related to popular gated models in machine
learning. We use random matrix theory (RMT) to analytically characterize the spectrum of the Jacobian
and show how gating produces slow modes and marginal stability. Thus, gating is a potential mechanism
to implement computations involving line attractor dynamics. The long-time behavior of the gated
network is studied using its Lyapunov spectrum, and the DMFT is used to provide an analytical prediction
for the maximum Lyapunov exponent. We also show that gating gives rise to a novel, discontinuous
transition to chaos, where the proliferation of critical points is decoupled with the appearance
of chaotic dynamics; the nature of this chaotic state is characterized in detail. Using the DMFT
and RMT, we produce phase diagrams for gated RNN. Finally, we address the gradients by leveraging
the adjoint sensitivity framework to develop a DMFT for the gradients. The theory developed here
sheds light on the rich dynamical behaviour produced by gating interactions and has implications
for architectural choices and learning dynamics. 