$H$-theorem states that the entropy production is nonnegative and, therefore, the entropy of a
closed system should monotonically change in time. In information processing, the entropy production
is positive for random transformation of signals (the information processing lemma). Originally,
the $H$-theorem and the information processing lemma were proved for the classical Boltzmann-Gibbs-Shannon
entropy and for the correspondent divergence (the relative entropy). Many new entropies and divergences
have been proposed during last decades and for all of them the $H$-theorem is needed. This note proposes
a simple and general criterion to check whether the $H$-theorem is valid for a convex divergence
$H$ and demonstrates that some of the popular divergences obey no $H$-theorem. We consider systems
with $n$ states $A_i$ that obey first order kinetics (master equation). A convex function $H$ is
a Lyapunov function for all master equations with given equilibrium if and only if its conditional
minima properly describe the equilibria of pair transitions $A_i \rightleftharpoons A_j$. This
theorem does not depend on the principle of detailed balance and is valid for general Markov kinetics.
Elementary analysis of pair equilibria demonstrates that the popular Bregman divergences like
Euclidean distance or Itakura-Saito distance in the space of distribution cannot be the universal
Lyapunov functions for the first-order kinetics and can increase in Markov processes. Therefore,
they violate the second law and the information processing lemma. In particular, for these measures
of information (divergences) random manipulation with data may add information to data. The main
results are extended to nonlinear generalized mass action law kinetic equations. In Appendix,
a new family of the universal Lyapunov functions for the generalized mass action law kinetics is
described. 