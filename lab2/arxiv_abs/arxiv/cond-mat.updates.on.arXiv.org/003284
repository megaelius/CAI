We address the fundamental question of how to optimally probe a scene with electromagnetic (EM)
radiation to yield a maximum amount of information relevant to a particular task. Machine learning
(ML) techniques have emerged as powerful tools to extract task-relevant information from a wide
variety of EM measurements, ranging from optics to the microwave domain. However, given the ability
to actively illuminate a particular scene with a programmable EM wavefront, it is often not clear
what wavefronts optimally encode information for the task at hand (e.g., object detection, classification).
Here, we show that by integrating a physical model of scene illumination and detection into a ML pipeline,
we can jointly learn optimal sampling and measurement processing strategies for a given task. We
consider in simulation the example of classifying objects using microwave radiation produced
by dynamic metasurfaces. By integrating an analytical forward model describing the metamaterial
elements as coupled dipoles into the ML pipeline, we jointly train analog model weights with digital
neural network weights. The learned non-intuitive illumination settings yield a higher classification
accuracy using fewer measurements. On the practical level, these results are highly relevant to
emerging context-aware systems such as autonomous vehicles, touchless human-interactive devices
or within smart health care, where strict time constraints place severe limits on measurement strategies.
On the conceptual level, our work serves as a bridge between wavefront shaping and tunable metasurface
design on the physical layer and ML techniques on the processing layer. 