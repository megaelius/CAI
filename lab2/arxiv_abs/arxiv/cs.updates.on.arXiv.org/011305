With the wide application of remote sensing technology in various fields, the accuracy and security
requirements for remote sensing images (RSIs) recognition are also increasing. In recent years,
due to the rapid development of deep learning in the field of image recognition, RSI recognition
models based on deep convolution neural networks (CNNs) outperform traditional hand-craft feature
techniques. However, CNNs also pose security issues when they show their capability of accurate
classification. By adding a very small variation of the adversarial perturbation to the input image,
the CNN model can be caused to produce erroneous results with extremely high confidence, and the
modification of the image is not perceived by the human eye. This added adversarial perturbation
image is called an adversarial example, which poses a serious security problem for systems based
on CNN model recognition results. This paper, for the first time, analyzes adversarial example
problem of RSI recognition under CNN models. In the experiments, we used different attack algorithms
to fool multiple high-accuracy RSI recognition models trained on multiple RSI datasets. The results
show that RSI recognition models are also vulnerable to adversarial examples, and the models with
different structures trained on the same RSI dataset also have different vulnerabilities. For
each RSI dataset, the number of features also affects the vulnerability of the model. Many features
are good for defensive adversarial examples. Further, we find that the attacked class of RSI has
an attack selectivity property. The misclassification of adversarial examples of the RSIs are
related to the similarity of the original classes in the CNN feature space. In addition, adversarial
examples in RSI recognition are of great significance for the security of remote sensing applications,
showing a huge potential for future research. 