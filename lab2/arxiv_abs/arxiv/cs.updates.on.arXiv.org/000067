Context dependence is central to the description of complexity. Building on our previous pairwise
definition of "set complexity" we use an information theory approach to formulate general measures
of systems complexity. We examine the properties of multi-variable dependency by beginning with
a formulation that uses the concept of interaction information. We then present a new measure of
multi-variable dependency, the "differential interaction information." This quantity has a
number of useful properties, and its two variable form reduces to the pairwise "set complexity"
of a set of variables as defined in our previous work. This pairwise quantity was proposed as a context-dependent
measure of information in biological systems, which we here generalize to an arbitrary number of
variables. Critical limiting properties of the "differential interaction information" are key
to the generalization. This general complexity measure, or measures, can extend previous ideas
about biological information and provides a more sophisticated conceptual basis for study of complexity.
The properties of "differential interaction information" also suggest new and powerful approaches
to data analysis. The representation of complex systems as hypergraphs can embody most of the important
internal dependencies that generate complexity, and differential interaction information can
provide weights of the hyperedges given a data set of system measurements. We describe a general
scheme for this kind of analysis and inference and illustrate it with simulated data sets. The conceptual
and practical conjoining of a generalized set complexity measure, multi-variable dependency,
and hypergraphs is our central result. While our focus is on biological systems, our constructs
represent an information-based description of arbitrary, multiple dependencies and are applicable
to any complex system. 