Vector quantization aims to form new vectors/matrices with shared values close to the original.
It could compress data with acceptable information loss, and could be of great usefulness in areas
like Image Processing, Pattern Recognition and Machine Learning. In recent years, the importance
of quantization has been soaring as it has been discovered huge potentials in deploying practical
neural networks, which is among one of the most popular research topics. Conventional vector quantization
methods usually suffer from their own flaws: hand-coding domain rules quantization could produce
poor results when encountering complex data, and clustering-based algorithms have the problem
of inexact solution and high time consumption. In this paper, we explored vector quantization problem
from a new perspective of sparse least square optimization and designed multiple algorithms with
their program implementations. Specifically, deriving from a sparse form of coefficient matrix,
three types of sparse least squares, with $l_0$, $l_1$, and generalized $l_1 + l_2$ penalizations,
are designed and implemented respectively. In addition, to produce quantization results with
given amount of quantized values(instead of penalization coefficient $\lambda$), this paper
proposed a cluster-based least square quantization method, which could also be regarded as an improvement
of information preservation of conventional clustering algorithm. The algorithms were tested
on various data and tasks and their computational properties were analyzed. The paper offers a new
perspective to probe the area of vector quantization, while the algorithms proposed could provide
more appropriate options for quantization tasks under different circumstances. 