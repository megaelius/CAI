A self-learning optimal control algorithm for sequential manufacturing processes with time-discrete
control actions is proposed and evaluated with simulated deep drawing processes. The necessary
control model is built during consecutive process executions under optimal control via Reinforcement
Learning, using the measured product quality as reward after each process execution. Prior model
formation, which is required by state-of-the-art algorithms like Model Predictive Control and
Approximate Dynamic Programming, is therefore obsolete. This avoids the difficulties in system
identification and accurate modelling, which arise with processes subject to non-linear dynamics
and stochastic influences. Also runtime complexity problems of these approaches are avoided,
which arise when more complex models and larger control prediction horizons are employed. Instead
of using pre-created process- and observation-models, Reinforcement Learning algorithms build
functions of expected future reward during processing, which are then used for optimal process
control decisions. The learning of such expectation functions is realized online by interacting
with the process. The proposed algorithm also takes stochastic variations of the process conditions
into consideration and is able to cope with partial observability. A method for the adaptive optimal
control of partially observable fixed-horizon manufacturing processes, based on Q-learning
is developed and studied. The resulting algorithm is instantiated and then evaluated by application
to a time-stochastic optimal control problem in metal sheet deep drawing, where the experiments
use FEM-simulated processes. The Reinforcement Learning based control shows superior results
over the model-based Model Predictive Control and Approximate Dynamic Programming approaches.
