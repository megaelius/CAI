Automatic Speech Recognition (ASR) is the interdisciplinary subfield of computational linguistics
that develops methodologies and technologies that enables the recognition and translation of
spoken language into text by computers. It incorporates knowledge and research in linguistics,
computer science, and electrical engineering fields. Sentiment analysis is contextual mining
of text which identifies and extracts subjective information in the source material and helping
a business to understand the social sentiment of their brand, product or service while monitoring
online conversations. According to the speech structure, three models are used in speech recognition
to do the match: Acoustic Model, Phonetic Dictionary and Language Model. Any speech recognition
program is evaluated using two factors: Accuracy (percentage error in converting spoken words
to digital data) and Speed (the extent to which the program can keep up with a human speaker). For the
purpose of converting speech to text (STT), we will be studying the following open source toolkits:
CMU Sphinx and Kaldi. The toolkits use Mel-Frequency Cepstral Coefficients (MFCC) and I-vector
for feature extraction. CMU Sphinx has been used with pre-trained Hidden Markov Models (HMM) and
Gaussian Mixture Models (GMM), while Kaldi is used with pre-trained Neural Networks (NNET) as acoustic
models. The n-gram language models contain the phonemes or pdf-ids for generating the most probable
hypothesis (transcription) in the form of a lattice. The speech dataset is stored in the form of .raw
or .wav file and is transcribed in .txt file. The system then tries to identify opinions within the
text, and extract the following attributes: Polarity (if the speaker expresses a positive or negative
opinion) and Keywords (the thing that is being talked about). 