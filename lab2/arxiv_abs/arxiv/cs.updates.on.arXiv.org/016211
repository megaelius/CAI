Machine learning (ML) is redefining what is possible in data-intensive fields of science and engineering.
However, applying ML to problems in the physical sciences comes with a unique set of challenges:
scientists want physically interpretable models that can (i) generalize to predict previously
unobserved behaviors, (ii) provide effective forecasting predictions (extrapolation), and
(iii) be certifiable. Autonomous systems will necessarily interact with changing and uncertain
environments, motivating the need for models that can accurately extrapolate based on physical
principles (e.g. Newton's universal second law for classical mechanics, $F=ma$). Standard ML
approaches have shown impressive performance for predicting dynamics in an interpolatory regime,
but the resulting models often lack interpretability and fail to generalize. We introduce a unified
sparse optimization framework that learns governing dynamical systems models from data, selecting
relevant terms in the dynamics from a library of possible functions. The resulting models are parsimonious,
have physical interpretations, and can generalize to new parameter regimes. Our framework allows
the use of non-convex sparsity promoting regularization functions and can be adapted to address
key challenges in scientific problems and data sets, including outliers, parametric dependencies,
and physical constraints. We show that the approach discovers parsimonious dynamical models on
several example systems. This flexible approach can be tailored to the unique challenges associated
with a wide range of applications and data sets, providing a powerful ML-based framework for learning
governing models for physical systems from data. 