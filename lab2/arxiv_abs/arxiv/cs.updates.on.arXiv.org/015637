With advances in reinforcement learning (RL), agents are now being developed in high-stakes application
domains such as healthcare and transportation. Explaining the behavior of these agents is challenging,
as the environments in which they act have large state spaces, and their decision-making can be affected
by delayed rewards, making it difficult to analyze their behavior. To address this problem, several
approaches have been developed. Some approaches attempt to convey the $\textit{global}$ behavior
of the agent, describing the actions it takes in different states. Other approaches devised $\textit{local}$
explanations which provide information regarding the agent's decision-making in a particular
state. In this paper, we combine global and local explanation methods, and evaluate their joint
and separate contributions, providing (to the best of our knowledge) the first user study of combined
local and global explanations for RL agents. Specifically, we augment strategy summaries that
extract important trajectories of states from simulations of the agent with saliency maps which
show what information the agent attends to. Our results show that the choice of what states to include
in the summary (global information) strongly affects people's understanding of agents: participants
shown summaries that included important states significantly outperformed participants who
were presented with agent behavior in a randomly set of chosen world-states. We find mixed results
with respect to augmenting demonstrations with saliency maps (local information), as the addition
of saliency maps did not significantly improve performance in most cases. However, we do find some
evidence that saliency maps can help users better understand what information the agent relies
on in its decision making, suggesting avenues for future work that can further improve explanations
of RL agents. 