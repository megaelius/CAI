It is well-known that overparametrized neural networks trained using gradient-based methods
quickly achieve small training error with appropriate hyperparameter settings. Recent papers
have proved this statement theoretically for highly overparametrized networks under reasonable
assumptions. The limiting case when the network size approaches infinity has also been considered.
These results either assume that the activation function is ReLU or they crucially depend on the
minimum eigenvalue of a certain Gram matrix depending on the data, random initialization and the
activation function. In the latter case, existing works only prove that this minimum eigenvalue
is non-zero and do not provide quantitative bounds. On the empirical side, a contemporary line of
investigations has proposed a number of alternative activation functions which tend to perform
better than ReLU at least in some settings but no clear understanding has emerged. This state of affairs
underscores the importance of theoretically understanding the impact of activation functions
on training. In the present paper, we provide theoretical results about the effect of activation
function on the training of highly overparametrized 2-layer neural networks. We show that for smooth
activations, such as tanh and swish, the minimum eigenvalue can be exponentially small depending
on the span of the dataset implying that the training can be very slow. In contrast, for activations
with a "kink," such as ReLU, SELU, ELU, all eigenvalues are large under minimal assumptions on the
data. Several new ideas are involved. Finally, we corroborate our results empirically. 