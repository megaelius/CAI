Generative neural network models, including Generative Adversarial Network (GAN) and Auto-Encoders
(AE), are among the most popular neural network models to generate adversarial data. The GAN model
is composed of a generator that produces synthetic data and of a discriminator that discriminates
between the generator's output and the true data. AE consist of an encoder which maps the model distribution
to a latent manifold and of a decoder which maps the latent manifold to a reconstructed distribution.
However, generative models are known to provoke chaotically scattered reconstructed distribution
during their training, and consequently, incomplete generated adversarial distributions. Current
distance measures fail to address this problem because they are not able to acknowledge the shape
of the data manifold, i.e. its topological features, and the scale at which the manifold should be
analyzed. We propose Persistent Homology for Generative Models, PHom-GeM, a new methodology to
assess and measure the distribution of a generative model. PHom-GeM minimizes an objective function
between the true and the reconstructed distributions and uses persistent homology, the study of
the topological features of a space at different spatial resolutions, to compare the nature of the
true and the generated distributions. Our experiments underline the potential of persistent homology
for Wasserstein GAN in comparison to Wasserstein AE and Variational AE. The experiments are conducted
on a real-world data set particularly challenging for traditional distance measures and generative
neural network models. PHom-GeM is the first methodology to propose a topological distance measure,
the bottleneck distance, for generative models used to compare adversarial samples in the context
of credit card transactions. 