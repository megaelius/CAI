A new conceptual foundation for the notion of "information" is proposed, based on the concept of
a "distinction graph": a graph in which two nodes are connected iff they cannot be distinguished
by a particular observer. The "graphtropy" of a distinction graph is defined as the average connection
probability of two nodes; in the case where the distinction graph is a composed of disconnected components
that are fully connected subgraphs, this is equivalent to Ellerman's logical entropy, which has
straightforward relationships to Shannon entropy. Probabilistic distinction graphs and probabilistic
graphtropy are also considered, as well as connections between graphtropy and thermodynamic and
quantum entropy. The semantics of the Second Law of Thermodynamics and the Maximum Entropy Production
Principle are unfolded in a novel way, via analysis of the cognitive processes underlying the making
of distinction graphs This evokes an interpretation in which complex intelligence is seen to correspond
to states of consciousness with intermediate graphtropy, which are associated with memory imperfections
that violate the assumptions leading to derivation of the Second Law. In the case where nodes of a
distinction graph are labeled by computable entities, graphtropy is shown to be monotonically
related to the average algorithmic information of the nodes (relative to to the algorithmic information
of the observer). A quantum-mechanical version of distinction graphs is considered, in which distinctions
can exist in a superposed state; this yields to graphtropy as a measure of the impurity of a mixed state,
and to a concept of "quangraphtropy." Finally, a novel computational model called Dynamic Distinction
Graphs (DDGs) is formulated, via enhancing distinction graphs with additional links expressing
causal implications, enabling a distinction-based model of "observers." 