Stein kernel has recently shown promising performance on classifying images represented by symmetric
positive definite (SPD) matrices. It evaluates the similarity between two SPD matrices through
their eigenvalues. In this paper, we argue that directly using the original eigenvalues may be problematic
because: i) Eigenvalue estimation becomes biased when the number of samples is inadequate, which
may lead to unreliable kernel evaluation; ii) More importantly, eigenvalues only reflect the property
of an individual SPD matrix. They are not necessarily optimal for computing Stein kernel when the
goal is to discriminate different sets of SPD matrices. To address the two issues in one shot, we propose
a discriminative Stein kernel, in which an extra parameter vector is defined to adjust the eigenvalues
of the input SPD matrices. The optimal parameter values are sought by optimizing a proxy of classification
performance. To show the generality of the proposed method, three different kernel learning criteria
that are commonly used in the literature are employed respectively as a proxy. A comprehensive experimental
study is conducted on a variety of image classification tasks to compare our proposed discriminative
Stein kernel with the original Stein kernel and other commonly used methods for evaluating the similarity
between SPD matrices. The experimental results demonstrate that, the discriminative Stein kernel
can attain greater discrimination and better align with classification tasks by altering the eigenvalues.
This makes it produce higher classification performance than the original Stein kernel and other
commonly used methods. 