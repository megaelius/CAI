Facial emotion recognition is an essential and important aspect of the field of human-machine interaction.
Past research on facial emotion recognition focuses on the laboratory environment. However, it
faces many challenges in real-world conditions, i.e., illumination changes, large pose variations
and partial or full occlusions. Those challenges lead to different face areas with different degrees
of sharpness and completeness. Inspired by this fact, we focus on the authenticity of predictions
generated by different <emotion, region> pairs. For example, if only the mouth areas are available
and the emotion classifier predicts happiness, then there is a question of how to judge the authenticity
of predictions. This problem can be converted into the contribution of different face areas to different
emotions. In this paper, we divide the whole face into six areas: nose areas, mouth areas, eyes areas,
nose to mouth areas, nose to eyes areas and mouth to eyes areas. To obtain more convincing results,
our experiments are conducted on three different databases: facial expression recognition + (
FER+), real-world affective faces database (RAF-DB) and expression in-the-wild (ExpW) dataset.
Through analysis of the classification accuracy, the confusion matrix and the class activation
map (CAM), we can establish convincing results. To sum up, the contributions of this paper lie in
two areas: 1) We visualize concerned areas of human faces in emotion recognition; 2) We analyze the
contribution of different face areas to different emotions in real-world conditions through experimental
analysis. Our findings can be combined with findings in psychology to promote the understanding
of emotional expressions. 