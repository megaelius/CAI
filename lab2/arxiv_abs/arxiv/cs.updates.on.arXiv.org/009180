Recent years have witnessed a surge in the popularity of attention mechanisms encoded within deep
neural networks. Inspired by the selective attention in the visual cortex, artificial attention
is designed to focus a neural network on the most task-relevant input signal. Many works claim that
the attention mechanism offers an extra dimension of interpretability by explaining where the
neural networks look. However, recent studies demonstrate that artificial attention maps do not
always coincide with common intuition. In view of these conflicting evidences, here we make a systematic
study on using artificial attention and human attention in neural network design. With three example
computer vision tasks (i.e., salient object segmentation, video action recognition, and fine-grained
image classification), diverse representative network backbones (i.e., AlexNet, VGGNet, ResNet)
and famous architectures (i.e., Two-stream, FCN), corresponding real human gaze data, and systematically
conducted large-scale quantitative studies, we offer novel insights into existing artificial
attention mechanisms and give preliminary answers to several key questions related to human and
artificial attention mechanisms. Our overall results demonstrate that human attention is capable
of bench-marking the meaningful `ground-truth' in attention-driven tasks, where the more the
artificial attention is close to the human attention, the better the performance; for higher-level
vision tasks, it is case-by-case. We believe it would be advisable for attention-driven tasks to
explicitly force a better alignment between artificial and human attentions to boost the performance;
such alignment would also benefit making the deep networks more transparent and explainable for
higher-level computer vision tasks. 