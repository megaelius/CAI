Recent years have seen a surge of interest in developing neural networks for graphs and data supported
on graphs. The graph is leveraged at each layer of the neural network as a parameterization to capture
detail at the node level with a reduced number of parameters and computational complexity. Following
this rationale, this paper puts forth a general framework that unifies state-of-the-art graph
neural networks (GNNs) through the concept of EdgeNet. An EdgeNet is a GNN architecture that allows
different nodes to use different parameters to weigh the information of different neighbors. By
extrapolating this strategy to more iterations between neighboring nodes, the EdgeNet learns
edge- and neighbor-dependent weights to capture local detail. This is the most general linear and
local operation that a node can perform and encompasses under one formulation all existing graph
convolutional neural networks (GCNNs) as well as graph attention networks (GATs). In writing different
GNN architectures with a common language, EdgeNets highlight specific architecture advantages
and limitations, while providing guidelines to improve their capacity without compromising their
local implementation. For instance, we show that GCNNs have a parameter sharing structure that
induces permutation equivariance. This can be an advantage or a limitation, depending on the application.
In cases where it is a limitation, we propose hybrid approaches and provide insights to develop several
other solutions that promote parameter sharing without enforcing permutation equivariance.
Another interesting conclusion is the unification of GCNNs and GATs -approaches that have been
so far perceived as separate. In particular, we show that GATs are GCNNs on a graph that is learned
from the features. This particularization opens the doors to develop alternative attention mechanisms
for improving discriminatory power. 