This paper addresses the challenge of humanoid robot teleoperation in a natural indoor environment
via a Brain-Computer Interface (BCI). We leverage deep Convolutional Neural Network (CNN) based
image and signal understanding to facilitate both real-time object detection and dry-Electroencephalography
(EEG) based human cortical brain bio-signal decoding. We employ recent advances in dry-EEG technology
to stream and collect the cortical waveforms from subjects while the subjects fixate on variable
Steady-State Visual Evoked Potential (SSVEP) stimuli generated directly from the environment
the robot is navigating. To these ends, we propose the use of novel variable BCI stimuli by utilising
the real-time video streamed via the on-board robot camera as visual input for SSVEP where the CNN
detected natural scene objects are altered and flickered with differing frequencies (10Hz, 12Hz
and 15Hz). These stimuli are not akin to traditional stimuli - as both the dimensions of the flicker
regions and their on-screen position changes depending on the scene objects detected in the scene.
On-screen object selection via dry-EEG enabled SSVEP in this way, facilitates the on-line decoding
of human cortical brain signals via a secondary CNN approach into teleoperation robot commands
(approach object, move in a specific direction: right, left or back). This SSVEP decoding model
is trained via a priori offline experimental data in which very similar visual input is present for
all subjects. The resulting offline classification demonstrates extremely high performance
and with mean accuracies of 96% and 90% for the real-time robot navigation experiment across multiple
test subjects. 