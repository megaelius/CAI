Recent models for learned image compression are based on autoencoders, learning approximately
invertible mappings from pixels to a quantized latent representation. These are combined with
an entropy model, a prior on the latent representation that can be used with standard arithmetic
coding algorithms to yield a compressed bitstream. Recently, hierarchical entropy models have
been introduced as a way to exploit more structure in the latents than simple fully factorized priors,
improving compression performance while maintaining end-to-end optimization. Inspired by the
success of autoregressive priors in probabilistic generative models, we examine autoregressive,
hierarchical, as well as combined priors as alternatives, weighing their costs and benefits in
the context of image compression. While it is well known that autoregressive models come with a significant
computational penalty, we find that in terms of compression performance, autoregressive and hierarchical
priors are complementary and, together, exploit the probabilistic structure in the latents better
than all previous learned models. The combined model yields state-of-the-art rate--distortion
performance, providing a 15.8% average reduction in file size over the previous state-of-the-art
method based on deep learning, which corresponds to a 59.8% size reduction over JPEG, more than 35%
reduction compared to WebP and JPEG2000, and bitstreams 8.4% smaller than BPG, the current state-of-the-art
image codec. To the best of our knowledge, our model is the first learning-based method to outperform
BPG on both PSNR and MS-SSIM distortion metrics. 