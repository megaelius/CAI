Detecting motor activities from sensor datasets is becoming increasingly common in a wide range
of applications with the rapid commoditization of wearable sensors. To detect activities, data
scientists iteratively experiment with different classifiers before deciding on a single model.
Evaluating, comparing, and reasoning about prediction results of alternative classifiers is
a crucial step in the process of iterative model development. However, standard aggregate performance
metrics (such as accuracy score) and textual display of individual event sequences have limited
granularity and scalability to effectively perform this critical step. To ameliorate these limitations,
we introduce Track Xplorer, an interactive visualization system to query, analyze and compare
the classification output of activity detection in multi-sensor data. Track Xplorer visualizes
the results of different classifiers as well as the ground truth labels and the video of activities
as temporally-aligned linear tracks. Through coordinated track visualizations, Track Xplorer
enables users to interactively explore and compare the results of different classifiers, assess
their accuracy with respect to the ground truth labels and video. Users can brush arbitrary regions
of any classifier track, zoom in and out with ease, and playback the corresponding video segment
to contextualize the performance of the classifier within the selected region. Track Xplorer also
contributes an algebra over track representations to filter, compose, and compare classification
outputs, enabling users to effectively reason about the performance of classifiers. We demonstrate
how our tool helps data scientists debug misclassifications and improve the prediction performance
in developing activity classifiers for real-world, multi-sensor data gathered from Parkinson's
patients. 