Hypernasality is a common characteristic symptom across many motor-speech disorders. For voiced
sounds, hypernasality introduces an additional resonance in the lower frequencies and, for unvoiced
sounds, there is reduced articulatory precision due to air escaping through the nasal cavity. However,
the acoustic manifestation of these symptoms is highly variable, making hypernasality estimation
very challenging, both for human specialists and automated systems. Previous work in this area
relies on either engineered features based on statistical signal processing or machine learning
models trained on clinical ratings. Engineered features often fail to capture the complex acoustic
patterns associated with hypernasality, whereas metrics based on machine learning are prone to
overfitting to the small disease-specific speech datasets on which they are trained. Here we propose
a new set of acoustic features that capture these complementary dimensions. The features are based
on two acoustic models trained on a large corpus of healthy speech. The first acoustic model aims
to measure nasal resonance from voiced sounds, whereas the second acoustic model aims to measure
articulatory imprecision from unvoiced sounds. To demonstrate that the features derived from
these acoustic models are specific to hypernasal speech, we evaluate them across different dysarthria
corpora. Our results show that the features generalize even when training on hypernasal speech
from one disease and evaluating on hypernasal speech from another disease (e.g. training on Parkinson's
disease, evaluation on Huntington's disease), and when training on neurologically disordered
speech but evaluating on cleft palate speech. 