Despite significant advances in image-to-image (I2I) translation with Generative Adversarial
Networks (GANs) have been made, it remains challenging to effectively translate an image to a set
of diverse images in multiple target domains using a pair of generator and discriminator. Existing
multimodal I2I translation methods adopt multiple domain-specific content encoders for different
domains, where each domain-specific content encoder is trained with images from the same domain
only. Nevertheless, we argue that the content (domain-invariant) features should be learned from
images among all the domains. Consequently, each domain-specific content encoder of existing
schemes fails to extract the domain-invariant features efficiently. To address this issue, we
present a flexible and general SoloGAN model for efficient multimodal I2I translation among multiple
domains with unpaired data. In contrast to existing methods, the SoloGAN algorithm uses a single
projection discriminator with an additional auxiliary classifier, and shares the encoder and
generator for all domains. As such, the SoloGAN model can be trained effectively with images from
all domains such that the domain-invariant content representation can be efficiently extracted.
Qualitative and quantitative results over a wide range of datasets against several counterparts
and variants of the SoloGAN model demonstrate the merits of the method, especially for the challenging
I2I translation tasks, i.e., tasks that involve extreme shape variations or need to keep the complex
backgrounds unchanged after translations. Furthermore, we demonstrate the contribution of each
component using ablation studies. 