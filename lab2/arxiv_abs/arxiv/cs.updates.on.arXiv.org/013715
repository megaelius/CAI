Unsupervised text encoding models have recently fueled substantial progress in NLP. The key idea
is to use neural networks to convert words in texts to vector space representations based on word
positions in a sentence and their contexts, which are suitable for end-to-end training of downstream
tasks. We see a strikingly similar situation in spatial analysis, which focuses on incorporating
both absolute positions and spatial contexts of geographic objects such as POIs into models. A general-purpose
representation model for space is valuable for a multitude of tasks. However, no such general model
exists to date beyond simply applying discretization or feed-forward nets to coordinates, and
little effort has been put into jointly modeling distributions with vastly different characteristics,
which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows
that grid cells in mammals provide a multi-scale periodic representation that functions as a metric
for location encoding and is critical for recognizing places and for path-integration. Therefore,
we propose a representation learning model called Space2Vec to encode the absolute positions and
spatial relationships of places. We conduct experiments on two real-world geographic data for
two different tasks: 1) predicting types of POIs given their positions and context, 2) image classification
leveraging their geo-locations. Results show that because of its multi-scale representations,
Space2Vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed-forward
nets, and tile embedding approaches for location modeling and image classification tasks. Detailed
analysis shows that all baselines can at most well handle distribution at one scale but show poor
performances in other scales. In contrast, Space2Vec's multi-scale representation can handle
distributions at different scales. 