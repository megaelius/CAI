In science and beyond, numbers are omnipresent when it comes to justifying different kinds of judgments.
Which scientific author, hiring committee-member, or advisory board panelist has not been confronted
with page-long "publication manuals", "assessment reports", "evaluation guidelines", calling
for p-values, citation rates, h-indices, or other statistics in order to motivate judgments about
the "quality" of findings, applicants, or institutions? Yet, many of those relying on and calling
for statistics do not even seem to understand what information those numbers can actually convey,
and what not. Focusing on the uninformed usage of bibliometrics as worrysome outgrowth of the increasing
quantification of science and society, we place the abuse of numbers into larger historical contexts
and trends. These are characterized by a technology-driven bureaucratization of science, obsessions
with control and accountability, and mistrust in human intuitive judgment. The ongoing digital
revolution increases those trends. We call for bringing sanity back into scientific judgment exercises.
Despite all number crunching, many judgments - be it about scientific output, scientists, or research
institutions - will neither be unambiguous, uncontroversial, or testable by external standards,
nor can they be otherwise validated or objectified. Under uncertainty, good human judgment remains,
for the better, indispensable, but it can be aided, so we conclude, by a toolbox of simple judgment
tools, called heuristics. In the best position to use those heuristics are research evaluators
(1) who have expertise in the to-be-evaluated area of research, (2) who have profound knowledge
in bibliometrics, and (3) who are statistically literate. 