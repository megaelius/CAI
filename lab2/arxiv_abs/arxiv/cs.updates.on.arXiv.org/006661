The stringent requirements for low-latency and privacy of the emerging high-stake applications
with intelligent devices such as drones and smart vehicles make the cloud computing inapplicable
in these scenarios. Instead, edge machine learning becomes increasingly attractive for performing
training and inference directly at network edges without sending data to a centralized data center.
This stimulates a nascent field termed as federated learning for training a machine learning model
on computation, storage, energy and bandwidth limited mobile devices in a distributed manner.
To preserve data privacy and address the issues of unbalanced and non-IID data points across different
devices, the federated averaging algorithm has been proposed for global model aggregation by computing
the weighted average of locally updated model at each selected device. However, the limited communication
bandwidth becomes the main bottleneck for aggregating the locally computed updates. We thus propose
a novel over-the-air computation based approach for fast global model aggregation via exploring
the superposition property of a wireless multiple-access channel. This is achieved by joint device
selection and beamforming design, which is modeled as a sparse and low-rank optimization problem
to support efficient algorithms design. To achieve this goal, we provide a difference-of-convex-functions
(DC) representation for the sparse and low-rank function to enhance sparsity and accurately detect
the fixed-rank constraint in the procedure of device selection. A DC algorithm is further developed
to solve the resulting DC program with global convergence guarantees. The algorithmic advantages
and admirable performance of the proposed methodologies are demonstrated through extensive numerical
results. 