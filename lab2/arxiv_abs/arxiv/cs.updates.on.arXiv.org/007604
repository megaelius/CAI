Research in Artificial Intelligence (AI) has focused mostly on two extremes: either on small improvements
in narrow AI domains, or on universal theoretical frameworks which are usually uncomputable, incompatible
with theories of biological intelligence, or lack practical implementations. The goal of this
work is to combine the main advantages of the two: to follow a big picture view, while providing a particular
theory and its implementation. In contrast with purely theoretical approaches, the resulting
architecture should be usable in realistic settings, but also form the core of a framework containing
all the basic mechanisms, into which it should be easier to integrate additional required functionality.
In this paper, we present a novel, purposely simple, and interpretable hierarchical architecture
which combines multiple different mechanisms into one system: unsupervised learning of a model
of the world, learning the influence of one's own actions on the world, model-based reinforcement
learning, hierarchical planning and plan execution, and symbolic/sub-symbolic integration
in general. The learned model is stored in the form of hierarchical representations with the following
properties: 1) they are increasingly more abstract, but can retain details when needed, and 2) they
are easy to manipulate in their local and symbolic-like form, thus also allowing one to observe the
learning process at each level of abstraction. On all levels of the system, the representation of
the data can be interpreted in both a symbolic and a sub-symbolic manner. This enables the architecture
to learn efficiently using sub-symbolic methods and to employ symbolic inference. 