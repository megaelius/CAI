Recent advances in modern Natural Language Processing (NLP) research have been dominated by the
combination of Transfer Learning methods with large-scale language models, in particular based
on the Transformer architecture. With them came a paradigm shift in NLP with the starting point for
training a model on a downstream task moving from a blank specific model to a general-purpose pretrained
architecture. Still, creating these general-purpose models remains an expensive and time-consuming
process restricting the use of these methods to a small sub-set of the wider NLP community. In this
paper, we present HuggingFace's Transformers library, a library for state-of-the-art NLP, making
these developments available to the community by gathering state-of-the-art general-purpose
pretrained models under a unified API together with an ecosystem of libraries, examples, tutorials
and scripts targeting many downstream NLP tasks. HuggingFace's Transformers library features
carefully crafted model implementations and high-performance pretrained weights for two main
deep learning frameworks, PyTorch and TensorFlow, while supporting all the necessary tools to
analyze, evaluate and use these models in downstream tasks such as text/token classification,
questions answering and language generation among others. The library has gained significant
organic traction and adoption among both the researcher and practitioner communities. We are committed
at HuggingFace to pursue the efforts to develop this toolkit with the ambition of creating the standard
library for building NLP systems. HuggingFace's Transformers library is available at \url{https://github.com/huggingface/transformers}.
