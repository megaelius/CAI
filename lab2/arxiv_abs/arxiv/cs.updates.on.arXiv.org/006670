Deep neural networks have become state-of-the-art technology for a wide range of practical machine
learning tasks such as image classification, handwritten digit recognition, speech recognition,
or game intelligence. This paper develops the fundamental limits of learning in deep neural networks
by characterizing what is possible if no constraints on the learning algorithm and the amount of
training data are imposed. Concretely, we consider information-theoretically optimal approximation
through deep neural networks with the guiding theme being a relation between the complexity of the
function (class) to be approximated and the complexity of the approximating network in terms of
connectivity and memory requirements for storing the network topology and the associated quantized
weights. The theory we develop educes remarkable universality properties of deep networks. Specifically,
deep networks are optimal approximants for vastly different function classes such as affine systems
and Gabor systems. This universality is afforded by a concurrent invariance property of deep networks
to time-shifts, scalings, and frequency-shifts. In addition, deep networks provide exponential
approximation accuracy i.e., the approximation error decays exponentially in the number of non-zero
weights in the network of vastly different functions such as the squaring operation, multiplication,
polynomials, sinusoidal functions, general smooth functions, and even one-dimensional oscillatory
textures and fractal functions such as the Weierstrass function, both of which do not have any known
methods achieving exponential approximation accuracy. In summary, deep neural networks provide
information-theoretically optimal approximation of a very wide range of functions and function
classes used in mathematical signal processing. 