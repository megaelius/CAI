Technology and the fruition of cultural heritage are becoming increasingly more entwined, especially
with the advent of smart audio guides, virtual and augmented reality, and interactive installations.
Machine learning and computer vision are important components of this ongoing integration, enabling
new interaction modalities between user and museum. Nonetheless, the most frequent way of interacting
with paintings and statues still remains taking pictures. Yet images alone can only convey the aesthetics
of the artwork, lacking is information which is often required to fully understand and appreciate
it. Usually this additional knowledge comes both from the artwork itself (and therefore the image
depicting it) and from an external source of knowledge, such as an information sheet. While the former
can be inferred by computer vision algorithms, the latter needs more structured data to pair visual
content with relevant information. Regardless of its source, this information still must be be
effectively transmitted to the user. A popular emerging trend in computer vision is Visual Question
Answering (VQA), in which users can interact with a neural network by posing questions in natural
language and receiving answers about the visual content. We believe that this will be the evolution
of smart audio guides for museum visits and simple image browsing on personal smartphones. This
will turn the classic audio guide into a smart personal instructor with which the visitor can interact
by asking for explanations focused on specific interests. The advantages are twofold: on the one
hand the cognitive burden of the visitor will decrease, limiting the flow of information to what
the user actually wants to hear; and on the other hand it proposes the most natural way of interacting
with a guide, favoring engagement. 