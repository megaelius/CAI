Deep neural networks (DNNs) have revolutionized the field of artificial intelligence and have
achieved unprecedented success in cognitive tasks such as image and speech recognition. Training
of large DNNs, however, is computationally intensive and this has motivated the search for novel
computing architectures targeting this application. A computational memory unit with nanoscale
resistive memory devices organized in crossbar arrays could store the synaptic weights in their
conductance states and perform the expensive weighted summations in place in a non-von Neumann
manner. However, updating the conductance states in a reliable manner during the weight update
process is a fundamental challenge that limits the training accuracy of such an implementation.
Here, we propose a mixed-precision architecture that combines a computational memory unit performing
the weighted summations and imprecise conductance updates with a digital processing unit that
accumulates the weight updates in high precision. A combined hardware/software training experiment
of a multilayer perceptron based on the proposed architecture using a phase-change memory (PCM)
array achieves 97.73% test accuracy on the task of classifying handwritten digits (based on the
MNIST dataset), within 0.6% of the software baseline. The architecture is further evaluated using
accurate behavioral models of PCM on a wide class of networks, namely convolutional neural networks,
long-short-term-memory networks, and generative-adversarial networks. Accuracies comparable
to those of floating-point implementations are achieved without being constrained by the non-idealities
associated with the PCM devices. A system-level study demonstrates 173x improvement in energy
efficiency of the architecture when used for training a multilayer perceptron compared with a dedicated
fully digital 32-bit implementation. 