High fidelity digital 3D environments have been proposed in recent years; however, it remains extreme
challenging to automatically equip such environment with realistic human bodies. Existing work
utilizes images, depths, or semantic maps to represent the scene, and parametric human models to
represent 3D bodies in the scene. While being straightforward, their generated human-scene interactions
are often lack of naturalness and physical plausibility. Our key observation is that humans interact
with the world through body-scene contact. To explicitly and effectively represent the physical
contact between the body and the world is essential for modeling human-scene interaction. To that
end, we propose a novel interaction representation, which explicitly encodes the proximity between
the human body and the 3D scene around it. Specifically, given a set of basis points on a scene mesh,
we leverage a conditional variational autoencoder to synthesize the distance from every basis
point to its closest point on a human body. The synthesized proximal relationship between the human
body and the scene can indicate which region a person tends to contact. Furthermore, based on such
synthesized proximity, we can effectively obtain expressive 3D human bodies that naturally interact
with the 3D scene. Our perceptual study shows that our model significantly improves the state-of-the-art
method, approaching the realism of real human-scene interaction. We believe our method makes an
important step towards the fully automatic synthesis of realistic 3D human bodies in 3D scenes.
Our code and model will be publicly available for research purpose. 