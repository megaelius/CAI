Though convolutional neural networks have achieved state-of-the-art performance on various
vision tasks, they are extremely vulnerable to adversarial examples, which are obtained by adding
human-imperceptible perturbations to the original images. Adversarial examples can thus be used
as an useful tool to evaluate and select the most robust models in safety-critical applications.
However, most of the existing adversarial attacks only achieve relatively low success rates under
the challenging black-box setting, where the attackers have no knowledge of the model structure
and parameters. To this end, we propose to improve the transferability of adversarial examples
by creating diverse input patterns. Instead of only using the original images to generate adversarial
examples, our method applies random transformations to the input images at each iteration. Extensive
experiments on ImageNet show that the proposed attack method can generate adversarial examples
that transfer much better to different networks than existing baselines. To further improve the
transferability, we (1) integrate the recently proposed momentum method into the attack process;
and (2) attack an ensemble of networks simultaneously. By evaluating our method against top defense
submissions and official baselines from NIPS 2017 adversarial competition, this enhanced attack
reaches an average success rate of 73.0%, which outperforms the top 1 attack submission in the NIPS
competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a benchmark
for evaluating the robustness of networks to adversaries and the effectiveness of different defense
methods in future. The code is public available at https://github.com/cihangxie/DI-2-FGSM.
