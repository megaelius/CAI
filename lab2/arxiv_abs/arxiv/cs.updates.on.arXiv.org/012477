In this era of big data, data analytics and machine learning, it is imperative to find ways to compress
large data sets such that intrinsic features necessary for subsequent analysis are not lost. The
traditional workhorse for data dimensionality reduction and feature extraction has been the matrix
SVD, which presupposes that the data has been arranged in matrix format. Our main goal in this study
is to show that high-dimensional data sets are more compressible when treated as tensors (aka multiway
arrays) and compressed via tensor-SVDs under the tensor-tensor product structures in (Kilmer
and Martin, 2011; Kernfeld et al., 2015). We begin by proving Eckart Young optimality results for
families of tensor-SVDs under two different truncation strategies. As such optimality properties
can be proven in both matrix and tensor-based algebras, a fundamental question arises: does the
tensor construct subsume the matrix construct in terms of representation efficiency? The answer
is yes, as shown when we prove that a tensor-tensor representation of an equal dimensional spanning
space can be superior to its matrix counterpart. We then investigate how the compressed representation
provided by the truncated tensor-SVD is related both theoretically and in compression performance
to its closest tensor-based analogue, truncated HOSVD (De Lathauwer et al., 2000; De Lathauwer
and Vandewalle, 2004), thereby showing the potential advantages of our tensor-based algorithms.
Finally, we propose new tensor truncated SVD variants, namely multi-way tensor SVDs, provide further
approximated representation efficiency and discuss under which conditions they are considered
optimal. We conclude with a numerical study demonstrating the utility of the theory. 