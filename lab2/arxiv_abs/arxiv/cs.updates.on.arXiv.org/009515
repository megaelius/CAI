Humans are arguably innately prepared to comprehend others' emotional expressions from subtle
body movements. If robots or computers can be empowered with this capability, a number of robotic
applications become possible. Automatically recognizing human bodily expression in unconstrained
situations, however, is daunting given the incomplete understanding of the relationship between
emotional expressions and body movements. The current research, as a multidisciplinary effort
among computer and information sciences, psychology, and statistics, proposes a scalable and
reliable crowdsourcing approach for collecting in-the-wild perceived emotion data for computers
to learn to recognize body languages of humans. To accomplish this task, a large and growing annotated
dataset with 9,876 video clips of body movements and 13,239 human characters, named BoLD (Body Language
Dataset), has been created. Comprehensive statistical analysis of the dataset revealed many interesting
insights. A system to model the emotional expressions based on bodily movements, named ARBEE (Automated
Recognition of Bodily Expression of Emotion), has also been developed and evaluated. Our analysis
shows the effectiveness of Laban Movement Analysis (LMA) features in characterizing arousal,
and our experiments using LMA features further demonstrate computability of bodily expression.
We report and compare results of several other baseline methods which were developed for action
recognition based on two different modalities, body skeleton, and raw image. The dataset and findings
presented in this work will likely serve as a launchpad for future discoveries in body language understanding
that will enable future robots to interact and collaborate more effectively with humans. 