Feature selection from a large number of covariates (aka features) in a regression analysis remains
a challenge in data science, especially in terms of its potential of scaling to ever-enlarging data
and finding a group of scientifically meaningful features. For example, to develop new, responsive
drug targets for ovarian cancer, the actual false discovery rate (FDR) of a practical feature selection
procedure must also match the target FDR. The popular approach to feature selection, when true features
are sparse, is to use a penalized likelihood or a shrinkage estimation, such as a LASSO, SCAD, Elastic
Net, or MCP procedure (call them benchmark procedures). We present a different approach using a
new subsampling method, called the Subsampling Winner algorithm (SWA). The central idea of SWA
is analogous to that used for the selection of US national merit scholars. SWA uses a "base procedure"
to analyze each of the subsamples, computes the scores of all features according to the performance
of each feature from all subsample analyses, obtains the "semifinalist" based on the resulting
scores, and then determines the "finalists," i.e., the most important features. Due to its subsampling
nature, SWA can scale to data of any dimension in principle. The SWA also has the best-controlled
actual FDR in comparison with the benchmark procedures and the randomForest, while having a competitive
true-feature discovery rate. We also suggest practical add-on strategies to SWA with or without
a penalized benchmark procedure to further assure the chance of "true" discovery. Our application
of SWA to the ovarian serous cystadenocarcinoma specimens from the Broad Institute revealed functionally
important genes and pathways, which we verified by additional genomics tools. This second-stage
investigation is essential in the current discussion of the proper use of P-values. 