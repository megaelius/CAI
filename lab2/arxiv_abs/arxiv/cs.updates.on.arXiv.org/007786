We present Habitat, a new platform for research in embodied artificial intelligence (AI). Habitat
enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation,
before transferring the learned skills to reality. Specifically, Habitat consists of the following:
1. Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, multiple
sensors, and generic 3D dataset handling (with built-in support for SUNCG, Matterport3D, Gibson
datasets). Habitat-Sim is fast -- when rendering a scene from the Matterport3D dataset, Habitat-Sim
achieves several thousand frames per second (fps) running single-threaded, and can reach over
10,000 fps multi-process on a single GPU, which is orders of magnitude faster than the closest simulator.
2. Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms
-- defining embodied AI tasks (e.g. navigation, instruction following, question answering),
configuring and training embodied agents (via imitation or reinforcement learning, or via classic
SLAM), and benchmarking using standard metrics. These large-scale engineering contributions
enable us to answer scientific questions requiring experiments that were till now impracticable
or `merely' impractical. Specifically, in the context of point-goal navigation (1) we revisit
the comparison between learning and SLAM approaches from two recent works and find evidence for
the opposite conclusion -- that learning outperforms SLAM, if scaled to total experience far surpassing
that of previous investigations, and (2) we conduct the first cross-dataset generalization experiments
{train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that
only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform
and these findings will advance research in embodied AI. 