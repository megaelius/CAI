When humans interact with intelligent systems, their causal responsibility for outcomes becomes
equivocal. We analyze the descriptive abilities of a newly developed responsibility quantification
model (ResQu) to predict actual human responsibility and perceptions of responsibility in the
interaction with intelligent systems. In two laboratory experiments, participants performed
a classification task. They were aided by classification systems with different capabilities.
We compared the predicted theoretical responsibility values to the actual measured responsibility
participants took on and to their subjective rankings of responsibility. The model predictions
were strongly correlated with both measured and subjective responsibility. A bias existed only
when participants with poor classification capabilities relied less-than-optimally on a system
that had superior classification capabilities and assumed higher-than-optimal responsibility.
The study implies that when humans interact with advanced intelligent systems, with capabilities
that greatly exceed their own, their comparative causal responsibility will be small, even if formally
the human is assigned major roles. Simply putting a human into the loop does not assure that the human
will meaningfully contribute to the outcomes. The results demonstrate the descriptive value of
the ResQu model to predict behavior and perceptions of responsibility by considering the characteristics
of the human, the intelligent system, the environment and some systematic behavioral biases. The
ResQu model is a new quantitative method that can be used in system design and can guide policy and
legal decisions regarding human responsibility in events involving intelligent systems. 