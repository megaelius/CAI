Convolutional Neural Networks (CNN) have recently seen tremendous success in various computer
vision tasks. However, their application to problems with high dimensional input and output, such
as high-resolution image and video segmentation or 3D medical imaging, has been limited by various
factors. Primarily, in the training stage, it is necessary to store network activations for back
propagation. In these settings, the memory requirements associated with storing activations
can exceed what is feasible with current hardware, especially for problems in 3D. Previously proposed
reversible architectures allow one to recalculate activations in the backwards pass instead of
storing them. For computer visions tasks, only block reversible networks have been possible because
pooling operations are not reversible. Block-reversibility still requires storing a number of
activations that grows with the number of blocks. Motivated by the propagation of signals over physical
networks, that are governed by the hyperbolic Telegraph equation, in this work we introduce a fully
conservative hyperbolic network for problems with high dimensional input and output. We introduce
a coarsening operation that allows completely reversible CNNs by using the Discrete Wavelet Transform
and its inverse to both coarsen and interpolate the network state and change the number of channels.
This means that during training we do not need to store any of the activations from the forward pass,
and can train arbitrarily deep networks. We show that fully reversible networks are able to achieve
results comparable to the state of the art in image depth estimation and full 3D video segmentation,
with a much lower memory footprint that is a constant independent of the network depth. 