The influence of human judgement is ubiquitous in datasets used across the analytics industry,
yet humans are known to be sub-optimal decision makers prone to various biases. Analysing biased
datasets then leads to biased outcomes of the analysis. Bias by protected characteristics (e.g.
race) is of particular interest as it may not only make the output of analytical process sub-optimal,
but also illegal. Countering the bias by constraining the analytical outcomes to be fair is problematic
because A) fairness lacks a universally accepted definition, while at the same time some definitions
are mutually exclusive, and B) the use of optimisation constraints ensuring fairness is incompatible
with most analytical pipelines. Both problems are solved by methods which remove bias from the data
and returning an altered dataset. This approach aims to not only remove the actual bias variable
(e.g. race), but also alter all proxy variables (e.g. postcode) so the bias variable is not detectable
from the rest of the data. The advantage of using this approach is that the definition of fairness
as a lack of detectable bias in the data (as opposed to the output of analysis) is universal and therefore
solves problem (A). Furthermore, as the data is altered to remove bias the problem (B) disappears
because the analytical pipelines can remain unchanged. This approach has been adopted by several
technical solutions. None of them, however, seems to be satisfactory in terms of ability to remove
multivariate, non-linear and non-binary biases. Therefore, in this paper I propose the concept
of Fair Adversarial Networks as an easy-to-implement general method for removing bias from data.
This paper demonstrates that Fair Adversarial Networks achieve this aim. 