We introduce the Machine Translation (MT) evaluation survey that contains both manual and automatic
evaluation methods. The traditional human evaluation criteria mainly include the intelligibility,
fidelity, fluency, adequacy, comprehension, and informativeness. The advanced human assessments
include task-oriented measures, post-editing, segment ranking, and extended criteriea, etc.
We classify the automatic evaluation methods into two categories, including lexical similarity
scenario and linguistic features application. The lexical similarity methods contain edit distance,
precision, recall, F-measure, and word order. The linguistic features can be divided into syntactic
features and semantic features respectively. The syntactic features include part of speech tag,
phrase types and sentence structures, and the semantic features include named entity, synonyms,
textual entailment, paraphrase, semantic roles, and language models. The deep learning models
for evaluation are very newly proposed. Subsequently, we also introduce the evaluation methods
for MT evaluation including different correlation scores, and the recent quality estimation (QE)
tasks for MT. This paper differs from the existing works \cite{GALEprogram2009,EuroMatrixProject2007}
from several aspects, by introducing some recent development of MT evaluation measures, the different
classifications from manual to automatic evaluation measures, the introduction of recent QE tasks
of MT, and the concise construction of the content. We hope this work will be helpful for MT researchers
to easily pick up some metrics that are best suitable for their specific MT model development, and
help MT evaluation researchers to get a general clue of how MT evaluation research developed. Furthermore,
hopefully, this work can also shine some light on other evaluation tasks, except for translation,
of NLP fields. 