Brain computer interfaces (BCI) enable direct communication with a computer, using neural activity
as the control signal. This signal is generally chosen from a variety of well-studied electroencephalogram
(EEG) signals. For a given BCI paradigm, feature extractors and classifiers are tailored to the
distinct characteristics of its expected EEG control signal, limiting its application to that
specific signal. Convolutional Neural Networks (CNNs), which have been used in computer vision
and speech recognition to perform automatic feature extraction and classification, have successfully
been applied to EEG-based BCIs; however, they have mainly been applied to single BCI paradigms and
thus it remains unclear how these architectures generalize to other paradigms. Here, we ask if we
can design a single CNN architecture to accurately classify EEG signals from different BCI paradigms,
while simultaneously being as compact as possible (defined as the number of parameters in the model).
In this work we introduce EEGNet, a compact convolutional network for EEG-based BCIs. We introduce
the use of depthwise and separable convolutions to more efficiently extract relevant features
for EEG-based BCIs. We compare EEGNet, both for within-subject and cross-subject classification,
to current state-of-the-art approaches across four BCI paradigms: P300 visual-evoked potentials,
error-related negativity responses (ERN), movement-related cortical potentials (MRCP), and
sensory motor rhythms (SMR). We show that EEGNet generalizes across paradigms better than, and
achieves comparably high performance to, traditional approaches, while simultaneously fitting
up to two orders of magnitude fewer parameters. We also demonstrate ways to visualize the contents
of a trained EEGNet model to enable interpretation of the learned features. 