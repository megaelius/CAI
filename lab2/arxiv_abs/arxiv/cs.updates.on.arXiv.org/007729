An effective way to achieve intelligence is to simulate various intelligent behaviors in the human
brain. In recent years, bio-inspired learning methods have emerged, and they are different from
the classical mathematical programming principle. In the perspective of brain inspiration, reinforcement
learning has gained additional interest in solving decision-making tasks as increasing neuroscientific
research demonstrates that significant links exist between reinforcement learning and specific
neural substrates. Because of the tremendous research that focuses on human brains and reinforcement
learning, scientists have investigated how robots can autonomously tackle complex tasks in the
form of a self-driving agent control in a human-like way. In this study, we propose an end-to-end
architecture using novel deep-Q-network architecture in conjunction with a recurrence to resolve
the problem in the field of simulated self-driving. The main contribution of this study is that we
trained the driving agent using a brain-inspired trial-and-error technique, which was in line
with the real world situation. Besides, there are three innovations in the proposed learning network:
raw screen outputs are the only information which the driving agent can rely on, a weighted layer
that enhances the differences of the lengthy episode, and a modified replay mechanism that overcomes
the problem of sparsity and accelerates learning. The proposed network was trained and tested under
a third-partied OpenAI Gym environment. After training for several episodes, the resulting driving
agent performed advanced behaviors in the given scene. We hope that in the future, the proposed brain-inspired
learning system would inspire practicable self-driving control solutions. 