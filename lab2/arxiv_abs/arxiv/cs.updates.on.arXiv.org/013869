Land cover classification of remote sensing images is a challenging task due to limited amounts
of annotated data, highly imbalanced classes, frequent incorrect pixel-level annotations, and
an inherent complexity in the semantic segmentation task. In this article, we propose a novel architecture
called the dense dilated convolutions' merging network (DDCM-Net) to address this task. The proposed
DDCM-Net consists of dense dilated image convolutions merged with varying dilation rates. This
effectively utilizes rich combinations of dilated convolutions that enlarge the network's receptive
fields with fewer parameters and features compared with the state-of-the-art approaches in the
remote sensing domain. Importantly, DDCM-Net obtains fused local- and global-context information,
in effect incorporating surrounding discriminative capability for multiscale and complex-shaped
objects with similar color and textures in very high-resolution aerial imagery. We demonstrate
the effectiveness, robustness, and flexibility of the proposed DDCM-Net on the publicly available
ISPRS Potsdam and Vaihingen data sets, as well as the DeepGlobe land cover data set. Our single model,
trained on three-band Potsdam and Vaihingen data sets, achieves better accuracy in terms of both
mean intersection over union (mIoU) and F1-score compared with other published models trained
with more than three-band data. We further validate our model on the DeepGlobe data set, achieving
state-of-the-art result 56.2% mIoU with much fewer parameters and at a lower computational cost
compared with related recent work. Code available at https://github.com/samleoqh/DDCM-Semantic-Segmentation-PyTorch
