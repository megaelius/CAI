Face manipulation has shown remarkable advances with the flourish of Generative Adversarial Networks.
However, due to the difficulties of controlling structures and textures, it is challenging to model
poses and expressions simultaneously, especially for the extreme manipulation at high-resolution.
In this paper, we propose a novel framework that simplifies face manipulation into two correlated
stages: a boundary prediction stage and a disentangled face synthesis stage. The first stage models
poses and expressions jointly via boundary images. Specifically, a conditional encoder-decoder
network is employed to predict the boundary image of the target face in a semi-supervised way. Pose
and expression estimators are introduced to improve the prediction performance. In the second
stage, the predicted boundary image and the input face image are encoded into the structure and the
texture latent space by two encoder networks, respectively. A proxy network and a feature threshold
loss are further imposed to disentangle the latent space. Furthermore, due to the lack of high-resolution
face manipulation databases to verify the effectiveness of our method, we collect a new high-quality
Multi-View Face (MVF-HQ) database. It contains 120,283 images at 6000x4000 resolution from 479
identities with diverse poses, expressions, and illuminations. MVF-HQ is much larger in scale
and much higher in resolution than publicly available high-resolution face manipulation databases.
We will release MVF-HQ soon to push forward the advance of face manipulation. Qualitative and quantitative
experiments on four databases show that our method dramatically improves the synthesis quality.
