In apprenticeship learning (AL), agents learn by watching or acquiring human demonstrations on
some tasks of interest. However, the lack of human demonstrations in novel tasks where they may not
be a human expert yet, or when it is too expensive and/or time consuming to acquire human demonstrations
motivated a new algorithm: Apprenticeship bootstrapping (ABS). The basic idea is to learn from
demonstrations on sub-tasks then autonomously bootstrap a model on the main, more complex, task.
The original ABS used inverse reinforcement learning (ABS-IRL). However, the approach is not suitable
for continuous action spaces. In this paper, we propose ABS via Deep learning (ABS-DL). It is first
validated in a simulation environment on an aerial and ground coordination scenario, where an Unmanned
Aerial Vehicle (UAV) is required to maintain three Unmanned Ground Vehicles (UGVs) within a field
of view of the UAV 's camera (FoV). Moving a machine learning algorithm from a simulation environment
to an actual physical platform is challenging because `mistakes' made by the algorithm while learning
could lead to the damage of the platform. We then take this extra step to test the algorithm in a physical
environment. We propose a safety-net as a protection layer to ensure that the autonomy of the algorithm
in learning does not compromise the safety of the platform. The tests of ABS-DL in the real environment
can guarantee a damage-free, collision avoidance behaviour of autonomous bodies. The results
show that performance of the proposed approach is comparable to that of a human, and competitive
to the traditional approach using expert demonstrations performed on the composite task. The proposed
safety-net approach demonstrates its advantages when it enables the UAV to operate more safely
under the control of the ABS-DL algorithm. 