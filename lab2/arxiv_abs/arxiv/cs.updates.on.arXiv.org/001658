A method for statistical parametric speech synthesis incorporating generative adversarial networks
(GANs) is proposed. Although powerful deep neural networks (DNNs) techniques can be applied to
artificially synthesize speech waveform, the synthetic speech quality is low compared with that
of natural speech. One of the issues causing the quality degradation is an over-smoothing effect
often observed in the generated speech parameters. A GAN introduced in this paper consists of two
neural networks: a discriminator to distinguish natural and generated samples, and a generator
to deceive the discriminator. In the proposed framework incorporating the GANs, the discriminator
is trained to distinguish natural and generated speech parameters, while the acoustic models are
trained to minimize the weighted sum of the conventional minimum generation loss and an adversarial
loss for deceiving the discriminator. Since the objective of the GANs is to minimize the divergence
(i.e., distribution difference) between the natural and generated speech parameters, the proposed
method effectively alleviates the over-smoothing effect on the generated speech parameters.
We evaluated the effectiveness for text-to-speech and voice conversion, and found that the proposed
method can generate more natural spectral parameters and $F_0$ than conventional minimum generation
error training algorithm regardless its hyper-parameter settings. Furthermore, we investigated
the effect of the divergence of various GANs, and found that a Wasserstein GAN minimizing the Earth-Mover's
distance works the best in terms of improving synthetic speech quality. 