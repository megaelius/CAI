Estimating the number of signals embedded in noise is a fundamental problem in signal processing.
As a classic estimator based on random matrix theory (RMT), the RMT estimator estimates the number
of signals via sequentially testing the likelihood of an eigenvalue as arising from a signal or noise
for a given over-detection probability. However, it tends to down-estimate the number of signals
as weak signal eigenvalues may be immersed in the bias term among eigenvalues. In order to solve this
problem, in this paper we focus on developing novel RMT-based estimators by incorporating this
bias term into RMT estimator. Firstly, we derive a novel decision statistics for signal detection
by incorporating the bias term into the RMT estimator, and propose a signal-test RMT estimator for
signal number estimation for a given miss-detection probability. Secondly, we analyze the effect
of the bias term on the detection performance of the signal-test RMT estimator and the RMT estimator.
It shows that the signal-test RMT estimator has lower down-estimation probability than the RMT
estimator when weak signal eigenvalues are immersed in the bias term, but has higher over-estimation
probability than the RMT estimator when all signals are strong enough to be detected by the RMT estimator.
Thirdly, we derive analytical formulas for the increased over-estimation probability of the signal-test
RMT estimator and the increased down-estimation probability of the RMT estimator incurred by this
bias term, and then propose a signal-noise-test RMT estimator which can adaptively select its decision
criterion between the RMT estimator and the signal-test RMT estimator to make benefits of these
two estimators while avoiding their individual drawbacks. Finally, simulation results are presented
to show that the proposed signal-noise-test RMT estimator significantly outperforms the existing
estimators in all cases. 