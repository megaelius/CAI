Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images,
which can effectively reduce the cost of human and material resources. However, due to the different
distribution of synthetic images compared to real images, the desired performance cannot still
be achieved. Real images consist of multiple forms of light orientation, while synthetic images
consist of a uniform light orientation. These features are considered to be characteristic of outdoor
and indoor scenes, respectively. To solve this problem, the previous method learned a model to improve
the realism of the synthetic image. Different from the previous methods, this paper takes the first
step to purify real images. Through the style transfer task, the distribution of outdoor real images
is converted into indoor synthetic images, thereby reducing the influence of light. Therefore,
this paper proposes a real-time style transfer network that preserves image content information
(eg, gaze direction, pupil center position) of an input image (real image) while inferring style
information (eg, image color structure, semantic features) of style image (synthetic image).
In addition, the network accelerates the convergence speed of the model and adapts to multi-scale
images. Experiments were performed using mixed studies (qualitative and quantitative) methods
to demonstrate the possibility of purifying real images in complex directions. Qualitatively,
it compares the proposed method with the available methods in a series of indoor and outdoor scenarios
of the LPW dataset. In quantitative terms, it evaluates the purified image by training a gaze estimation
model on the cross data set. The results show a significant improvement over the baseline method
compared to the raw real image. 