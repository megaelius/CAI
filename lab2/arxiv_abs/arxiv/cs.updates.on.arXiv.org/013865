The ability to quantify complex relationships within multivariate time series is a key component
of modelling many physical systems, from the climate to brains and other biophysical phenomena.
Unfortunately, even testing the significance of simple dependence measures, such as Pearson correlation,
is complicated by altered sampling properties when autocorrelation is present in the individual
time series. Moreover, it has been recently established that commonly used multivariate dependence
measures---such as Granger causality---can produce substantially inaccurate results when applying
classical hypothesis-testing procedures to digitally-filtered time series. Here, we suggest
that the digital filtering-induced bias in Granger causality is an effect of autocorrelation,
and we present a principled statistical framework for the hypothesis testing of a large family of
linear-dependence measures between multiple autocorrelated time series. Our approach unifies
the theoretical foundations established by Bartlett and others on variance estimators for autocorrelated
signals with the more intricate multivariate measures of linear dependence. Specifically, we
derive the sampling distributions and subsequent hypothesis tests for any measure that can be decomposed
into terms that involve independent partial correlations, which we show includes Granger causality
and mutual information under a multivariate linear-Gaussian model. In doing so, we provide the
first exact tests for inferring linear dependence between vector autoregressive processes with
limited data. Using numerical simulations and brain-imaging datasets, we demonstrate that our
newly developed tests maintain the expected false-positive rate (FPR) with minimally-sufficient
samples, while the classical log-likelihood ratio tests can yield an unbounded FPR depending on
the parameters chosen. 