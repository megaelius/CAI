Interpretable machine learning models have received increasing interest in recent years, especially
in domains where humans are involved in the decision-making process. However, the possible loss
of the task performance for gaining interpretability is often inevitable. This performance downgrade
puts practitioners in a dilemma of choosing between a top-performing black-box model with no explanations
and an interpretable model with unsatisfying task performance. In this work, we propose a novel
framework for building a Hybrid Decision Model that integrates an interpretable model with any
black-box model to introduce explanations in the decision making process while preserving or possibly
improving the predictive accuracy. We propose a novel metric, explainability, to measure the percentage
of data that are sent to the interpretable model for decision. We also design a principled objective
function that considers predictive accuracy, model interpretability, and data explainability.
Under this framework, we develop Collaborative Black-box and RUle Set Hybrid (CoBRUSH) model that
combines logic rules and any black-box model into a joint decision model. An input instance is first
sent to the rules for decision. If a rule is satisfied, a decision will be directly generated. Otherwise,
the black-box model is activated to decide on the instance. To train a hybrid model, we design an efficient
search algorithm that exploits theoretically grounded strategies to reduce computation. Experiments
show that CoBRUSH models are able to achieve same or better accuracy than their black-box collaborator
working alone while gaining explainability. They also have smaller model complexity than interpretable
baselines. 