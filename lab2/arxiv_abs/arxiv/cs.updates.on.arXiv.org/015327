This work studies training generative adversarial networks under the federated learning setting.
Generative adversarial networks (GANs) have achieved advancement in various real-world applications,
such as image editing, style transfer, scene generations, etc. However, like other deep learning
models, GANs are also suffering from data limitation problems in real cases. To boost the performance
of GANs in target tasks, collecting images as many as possible from different sources becomes not
only important but also essential. For example, to build a robust and accurate bio-metric verification
system, huge amounts of images might be collected from surveillance cameras, and/or uploaded from
cellphones by users accepting agreements. In an ideal case, utilize all those data uploaded from
public and private devices for model training is straightforward. Unfortunately, in the real scenarios,
this is hard due to a few reasons. At first, some data face the serious concern of leakage, and therefore
it is prohibitive to upload them to a third-party server for model training; at second, the images
collected by different kinds of devices, probably have distinctive biases due to various factors,
$\textit{e.g.}$, collector preferences, geo-location differences, which is also known as "domain
shift". To handle those problems, we propose a novel generative learning scheme utilizing a federated
learning framework. Following the configuration of federated learning, we conduct model training
and aggregation on one center and a group of clients. Specifically, our method learns the distributed
generative models in clients, while the models trained in each client are fused into one unified
and versatile model in the center. We perform extensive experiments to compare different federation
strategies, and empirically examine the effectiveness of federation under different levels of
parallelism and data skewness. 