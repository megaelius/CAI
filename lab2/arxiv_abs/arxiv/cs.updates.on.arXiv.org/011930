A new prior is proposed for learning representations of high-level concepts of the kind we manipulate
with language. This prior can be combined with other priors in order to help disentangling abstract
factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen
as a bottleneck through which just a few elements, after having been selected by attention from a
broader pool, are then broadcast and condition further processing, both in perception and decision-making.
The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious
state. This conscious state is combining the few concepts constituting a conscious thought, i.e.,
what one is immediately conscious of at a particular moment. We claim that this architectural and
information-processing constraint corresponds to assumptions about the joint distribution
between high-level concepts. To the extent that these assumptions are generally true (and the form
of natural language seems consistent with them), they can form a useful prior for representation
learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves
only a few variables and yet can make a statement with very high probability of being true. This is
consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor
graph, i.e., where the dependencies captured by each factor of the factor graph involve only very
few variables while creating a strong dip in the overall energy function. The consciousness prior
also makes it natural to map conscious states to natural language utterances or to express classical
AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient
search mechanisms implemented by attention mechanisms. 