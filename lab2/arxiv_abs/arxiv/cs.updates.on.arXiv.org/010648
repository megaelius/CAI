Decision forests (DF), in particular random forests and gradient boosting trees, have demonstrated
state-of-the-art accuracy compared to other methods in many supervised learning scenarios. In
particular, DFs dominate other methods in tabular data, that is, when the feature space is unstructured,
so that the signal is invariant to permuting feature indices. However, in structured data lying
on a manifold---such as images, text, and speech---neural nets (NN) tend to outperform DFs. We conjecture
that at least part of the reason for this is that the input to NN is not simply the feature magnitudes,
but also their indices (for example, the convolution operation uses "feature locality"). In contrast,
na\"ive DF implementations fail to explicitly consider feature indices. A recently proposed DF
approach demonstrates that DFs, for each node, implicitly sample a random matrix from some specific
distribution. Here, we build on that to show that one can choose distributions in a \emph{manifold
aware fashion}. For example, for image classification, rather than randomly selecting pixels,
one can randomly select contiguous patches. We demonstrate the empirical performance of data living
on three different manifolds: images, time-series, and a torus. In all three cases, our Manifold
Forest (\Mf) algorithm empirically dominates other state-of-the-art approaches that ignore
feature space structure, achieving a lower classification error on all sample sizes. This dominance
extends to the MNIST data set as well. Moreover, both training and test time is significantly faster
for manifold forests as compared to deep nets. This approach, therefore, has promise to enable DFs
and other machine learning methods to close the gap with deep nets on manifold-valued data. 