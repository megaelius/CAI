Motivated by the problem of spam classification, we study online learning in strategic classification
settings from the perspective of the learner, who is repeatedly facing myopically rational strategic
agents. We model this interplay as a repeated Stackelberg game, where at each timestep nature selects
a feature vector observed only by the agent, the learner deploys a high-dimensional linear classifier
and the agent, after observing the classifier and according to his underlying utility function,
best responds with a potentially altered feature vector. The performance of the learner is measured
in terms of Stackelberg regret for her 0--1 loss function. In this game-theoretic setting, applying
standard online learning algorithms that minimize the external regret is not helpful; we prove
that Stackelberg and external regret are strongly incompatible, i.e., there exist worst-case
scenarios, where any sequence of actions providing sublinear external regret might result in linear
Stackelberg regret and vice versa. For that, we introduce GRINDER, an adaptive discretization
algorithm potentially of independent interest, and prove its data-dependent upper bound on the
Stackelberg regret given oracle access, while being computationally efficient. In fact, we prove
a nearly matching lower bound for the Stackelberg regret of online strategic classification against
myopically rational agents. We complement our theoretical analysis with simulation results,
which suggest that our algorithm outperforms the benchmarks, even given access to approximation
oracles. Our results advance the known state-of-the-art results in the growing literature of learning
from revealed preferences, which has so far focused on "smoother" utility and loss functions from
the perspective of the agents and the learner respectively. 