Hate speech detection or offensive language detection are well-established but controversial
NLP tasks. There is no denying the temptation to use them for law enforcement or by private actors
to censor, delete, or punish online statements. However, given the importance of freedom of expression
for the public discourse in a democracy, determining statements that would potentially be subject
to these measures requires a legal justification that outweighs the right to free speech in the respective
case. The legal concept of 'incitement to hatred' answers this question by preventing discrimination
against and segregation of a target group, thereby ensuring the members' acceptance as equal in
a society - likewise a prerequisite for democracy. In this paper, we pursue these questions based
on the criminal offense of 'incitement to hatred' in {\S} 130 of the German Criminal Code along with
the underlying EU Council Framework Decision. Under the German Network Enforcement Act, social
media providers are subject to a direct obligation to delete postings violating this offense. We
take this as a use case to study the transition from the ill-defined concepts of hate speech or offensive
language which are usually used in NLP to an operationalization of an actual legally binding obligation.
We first translate the legal assessment into a series of binary decisions and then collect, annotate,
and analyze a dataset according to our annotation scheme. Finally, we translate each of the legal
decisions into an NLP task based on the annotated data. In this way, we ultimately also explore the
extent to which the underlying value-based decisions could be carried over to NLP. 