Sparse models for high-dimensional linear regression and machine learning have received substantial
attention over the past two decades. Model selection, or determining which features or covariates
are the best explanatory variables, is critical to the interpretability of a learned model. Much
of the current literature assumes that covariates are only mildly correlated. However, in modern
applications ranging from functional MRI to genome-wide association studies, covariates are
highly correlated and do not exhibit key properties (such as the restricted eigenvalue condition,
RIP, or other related assumptions). This paper considers a high-dimensional regression setting
in which a graph governs both correlations among the covariates and the similarity among regression
coefficients. Using side information about the strength of correlations among features, we form
a graph with edge weights corresponding to pairwise covariances. This graph is used to define a graph
total variation regularizer that promotes similar weights for highly correlated features. The
graph structure encapsulated by this regularizer helps precondition correlated features to yield
provably accurate estimates. Using graph-based regularizers to develop theoretical guarantees
for highly-correlated covariates has not been previously examined. This paper shows how our proposed
graph-based regularization yields mean-squared error guarantees for a broad range of covariance
graph structures and correlation strengths which in many cases are optimal by imposing additional
structure on $\beta^{\star}$ which encourages \emph{alignment} with the covariance graph. Our
proposed approach outperforms other state-of-the-art methods for highly-correlated design
in a variety of experiments on simulated and real fMRI data. 