One of the challenges in large-scale information retrieval (IR) is to develop fine-grained and
domain-specific methods to answer natural language questions. Despite the availability of numerous
sources and datasets for answer retrieval, Question Answering (QA) remains a challenging problem
due to the difficulty of the question understanding and answer extraction tasks. One of the promising
tracks investigated in QA is to map new questions to formerly answered questions that are `similar'.
In this paper, we propose a novel QA approach based on Recognizing Question Entailment (RQE) and
we describe the QA system and resources that we built and evaluated on real medical questions. First,
we compare machine learning and deep learning methods for RQE using different kinds of datasets,
including textual inference, question similarity and entailment in both the open and clinical
domains. Second, we combine IR models with the best RQE method to select entailed questions and rank
the retrieved answers. To study the end-to-end QA approach, we built the MedQuAD collection of 47,457
question-answer pairs from trusted medical sources, that we introduce and share in the scope of
this paper. Following the evaluation process used in TREC 2017 LiveQA, we find that our approach
exceeds the best results of the medical task with a 29.8% increase over the best official score. The
evaluation results also support the relevance of question entailment for QA and highlight the effectiveness
of combining IR and RQE for future QA efforts. Our findings also show that relying on a restricted
set of reliable answer sources can bring a substantial improvement in medical QA. 