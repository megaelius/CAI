Humans naturally perceive a 3D scene in front of them through accumulation of information obtained
from multiple interconnected projections of the scene and by interpreting their correspondence.
This phenomenon has inspired artificial intelligence models to extract the depth and view angle
of the observed scene by modeling the correspondence between different views of that scene. Our
paper is built upon previous works in the field of unsupervised depth and relative camera pose estimation
from temporal consecutive video frames using deep learning (DL) models. Our approach uses a hybrid
learning framework introduced in a recent work called GeoNet, which leverages geometric constraints
in the 3D scenes to synthesize a novel view from intermediate DL-based predicted depth and relative
pose. However, the state-of-the-art unsupervised depth and pose estimation DL models are exclusively
trained/tested on a few available outdoor scene datasets and we have shown they are hardly transferable
to new scenes, especially from indoor environments, in which estimation requires higher precision
and dealing with probable occlusions. This paper introduces "Indoor GeoNet", a weakly supervised
depth and camera pose estimation model targeted for indoor scenes. In Indoor GeoNet, we take advantage
of the availability of indoor RGBD datasets collected by human or robot navigators, and added partial
(i.e. weak) supervision in depth training into the model. Experimental results showed that our
model effectively generalizes to new scenes from different buildings. Indoor GeoNet demonstrated
significant depth and pose estimation error reduction when compared to the original GeoNet, while
showing 3 times more reconstruction accuracy in synthesizing novel views in indoor environments.
