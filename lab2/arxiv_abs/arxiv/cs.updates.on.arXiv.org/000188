The availability of large volumes of remote sensing data insists on higher degree of automation
in feature extraction, making it a need of the hour.The huge quantum of data that needs to be processed
entails accelerated processing to be enabled.GPUs, which were originally designed to provide
efficient visualization, are being massively employed for computation intensive parallel processing
environments. Image processing in general and hence automated feature extraction, is highly computation
intensive, where performance improvements have a direct impact on societal needs. In this context,
an algorithm has been formulated for automated feature extraction from a panchromatic or multispectral
image based on image processing techniques. Two Laplacian of Guassian (LoG) masks were applied
on the image individually followed by detection of zero crossing points and extracting the pixels
based on their standard deviation with the surrounding pixels. The two extracted images with different
LoG masks were combined together which resulted in an image with the extracted features and edges.
Finally the user is at liberty to apply the image smoothing step depending on the noise content in
the extracted image. The image is passed through a hybrid median filter to remove the salt and pepper
noise from the image. This paper discusses the aforesaid algorithm for automated feature extraction,
necessity of deployment of GPUs for the same; system-level challenges and quantifies the benefits
of integrating GPUs in such environment. The results demonstrate that substantial enhancement
in performance margin can be achieved with the best utilization of GPU resources and an efficient
parallelization strategy. Performance results in comparison with the conventional computing
scenario have provided a speedup of 20x, on realization of this parallelizing strategy. 