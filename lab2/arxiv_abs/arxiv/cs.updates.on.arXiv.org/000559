We introduce a novel approach to statistical relational learning; it is incorporated in the logical
and relational learning language, kLog. While traditionally statistical relational learning
combines probabilistic (graphical) models with logical and relational representations, kLog
combines a kernel-based approach with expressive logical and relational representations. kLog
allows users to specify logical and relational learning problems at a high level in a declarative
way. It builds on simple but powerful concepts that are well-known in statistical relational learning:
learning from interpretations, entity/relationship data modeling, logic programming, and deductive
databases (Prolog and Datalog). However, unlike standard statistical relational models, kLog
does not represent a probability distribution directly. It is rather a kernel-based approach to
learning that employs features derived from a grounded entity/relationship diagram. These features
are derived using a technique called graphicalization: first, relational representations are
transformed into graph-based representations; subsequently, graph kernels are employed for
defining feature spaces. Graphicalization is the equivalent of knowledge-based model construction
but for kernel-based learning rather than for graphical models. kLog can use numerical and symbolic
data, background knowledge in the form of Prolog or Datalog programs (as in inductive logic programming
systems) and several statistical procedures can be used to fit the model parameters. The kLog framework
can --- in principle --- be applied to tackle the same range of tasks that has made statistical relational
learning so popular, including classification, regression, multitask learning, and collective
classification. kLog is available at this http URL 