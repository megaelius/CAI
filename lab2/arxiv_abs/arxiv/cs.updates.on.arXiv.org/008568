Recently, Convolutional Neural Networks (CNNs) demonstrate a considerable vulnerability to
adversarial attacks, which can be easily misled by adversarial perturbations. With more aggressive
methods proposed, adversarial attacks can be also applied to the physical world, causing practical
issues to various CNN powered applications. Most existing defense works for physical adversarial
attacks only focus on eliminating explicit perturbation patterns from inputs, ignoring interpretation
and solution to CNN's intrinsic vulnerability. Therefore, most of them depend on considerable
data processing costs and lack the expected versatility to different attacks. In this paper, we
propose DoPa - a fast and comprehensive CNN defense methodology against physical adversarial attacks.
By interpreting the CNN's vulnerability, we find that non-semantic adversarial perturbations
can activate CNN with significantly abnormal activations and even overwhelm other semantic input
patterns' activations. We improve the CNN recognition process by adding a self-verification stage
to analyze the semantics of distinguished activation patterns with only one CNN inference involved.
Based on the detection result, we further propose a data recovery methodology to defend the physical
adversarial attacks. We apply such detection and defense methodology into both image and audio
CNN recognition process. Experiments show that our methodology can achieve an average rate of 90%
success for attack detection and 81% accuracy recovery for image physical adversarial attacks.
Also, the proposed defense method can achieve a 92% detection successful rate and 77.5% accuracy
recovery for audio recognition applications. Moreover, the proposed defense methods are at most
2.3x faster compared to the state-of-the-art defense methods, making them feasible to resource-constrained
platforms, such as mobile devices. 