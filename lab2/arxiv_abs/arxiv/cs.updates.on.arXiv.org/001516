Fine-grained image classification is to recognize hundreds of subcategories belonging to the
same basic-level category, which is a highly challenging task due to the quite subtle visual distinctions
among similar subcategories. Most existing methods generally learn part detectors to discover
discriminative regions for better performance. However, not all localized parts are beneficial
and indispensable for classification, and the setting for number of part detectors relies heavily
on prior knowledge as well as experimental results. As is known to all, when we describe the object
of an image into text via natural language, we only focus on the pivotal characteristics, and rarely
pay attention to common characteristics as well as the background areas. This is an involuntary
transfer from human visual attention to textual attention, which leads to the fact that textual
attention tells us how many and which parts are discriminative and significant. So textual attention
of natural language descriptions could help us to discover visual attention in image. Inspired
by this, we propose a visual-textual attention driven fine-grained representation learning (VTA)
approach, and its main contributions are: (1) Fine-grained visual-textual pattern mining devotes
to discovering discriminative visual-textual pairwise information for boosting classification
through jointly modeling vision and text with generative adversarial networks (GANs), which automatically
and adaptively discovers discriminative parts. (2) Visual-textual representation learning
jointly combine visual and textual information, which preserves the intra-modality and inter-modality
information to generate complementary fine-grained representation, and further improve classification
performance. Experiments on the two widely-used datasets demonstrate the effectiveness of our
VTA approach, which achieves the best classification accuracy. 