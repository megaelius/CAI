Neural Architecture Search (NAS) has seen an explosion of research in the past few years, with techniques
spanning reinforcement learning, evolutionary search, Gaussian process (GP) Bayesian optimization
(BO), and gradient descent. While BO with GPs has seen great success in hyperparameter optimization,
there are many challenges applying BO to NAS, such as the requirement of a distance function between
neural networks. In this work, we develop a suite of techniques for high-performance BO applied
to NAS that allows us to achieve state-of-the-art NAS results. We develop a BO procedure that leverages
a novel architecture representation (which we term the path encoding) and a neural network-based
predictive uncertainty model on this representation. On popular search spaces, we can predict
the validation accuracy of a new architecture to within one percent of its true value using only 200
training points. This may be of independent interest beyond NAS. We also show experimentally and
theoretically that our method scales far better than existing techniques. We test our algorithm
on the NASBench (Ying et al. 2019) and DARTS (Liu et al. 2018) search spaces and show that our algorithm
outperforms a variety of NAS methods including regularized evolution, reinforcement learning,
BOHB, and DARTS. Our method achieves state-of-the-art performance on the NASBench dataset and
is over 100x more efficient than random search. We adhere to the recent NAS research checklist (Lindauer
and Hutter 2019) to facilitate NAS research. In particular, our implementation is publicly available
and includes all details needed to fully reproduce our results. 