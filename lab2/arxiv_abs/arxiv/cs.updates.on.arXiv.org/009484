The problem of estimating Wasserstein distances in high-dimensional spaces suffers from the curse
of dimensionality: One needs an exponential (w.r.t. dimension) number of samples for the distance
between two measures to be comparable to that evaluated using i.i.d samples. Therefore, the optimal
transport (OT) geometry can only be used in machine learning if it is regularized, one way or another.
One of the greatest achievements of the OT literature in recent years lies in regularity theory:
A first result by~\citet{caffarelli2000monotonicity} showed that the OT map between two well
behaved measures is Lipschitz, or, equivalently when considering 2-Wasserstein distances, that
the Brenier convex potential (whose gradient yields an optimal map) is a smooth function. We propose
in this work to go backwards, and adopt instead regularity as a regularization tool. We propose algorithms
working on two discrete measures that can recover nearly optimal transport maps between them that
have small distortion, or, equivalently, nearly optimal Brenier potential that are strongly convex
and smooth. For univariate measures, we show that computing these potentials is equivalent to solving
an isotonic regression problem under Lipschitz and strong monotonicity constraints. For multivariate
measures the problem boils down to solving alternatively a convex QCQP and an OT problem. Most importantly,
we recover as the result of this optimization the values and gradients of the Brenier potential on
sampled points, but also show more generally that values and gradients can be computed at any new
point, at the cost of solving a QCQP for each evaluation. Building on these two formulations we propose
practical algorithms to estimate and evaluate transport maps with desired smoothness/strong
convexity properties, illustrate their statistical performance and visualize the action of such
maps on a color transfer task. 