Deep domain adaptation methods have achieved appealing performance by learning transferable
representations from a well-labeled source domain to a different but related unlabeled target
domain. Most existing works assume source and target data share the identical label space, which
is often difficult to be satisfied in many real-world applications. With the emergence of big data,
there is a more practical scenario called partial domain adaptation, where we are always accessible
to a more large-scale source domain while working on a relative small-scale target domain. In this
case, the conventional domain adaptation assumption should be relaxed, and the target label space
tends to be a subset of the source label space. Intuitively, reinforcing the positive effects of
the most relevant source subclasses and reducing the negative impacts of irrelevant source subclasses
are of vital importance to address partial domain adaptation challenge. This paper proposes an
efficiently-implemented Deep Residual Correction Network (DRCN) by plugging one residual block
into the source network along with the task-specific feature layer, which effectively enhances
the adaptation from source to target and explicitly weakens the influence from the irrelevant source
classes. Specifically, the plugged residual block, which consists of several fully-connected
layers, could deepen basic network and boost its feature representation capability correspondingly.
Moreover, we design a weighted class-wise domain alignment loss to couple two domains by matching
the feature distributions of shared classes between source and target. Comprehensive experiments
on partial, traditional and fine-grained cross-domain visual recognition demonstrate that DRCN
is superior to the competitive deep domain adaptation approaches. 