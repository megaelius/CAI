Emerging technologies like the Internet of Things (IoT) require latency-aware computation for
real-time application processing. In IoT environments, connected things generate a huge amount
of data, which are generally referred to as big data. Data generated from IoT devices are generally
processed in a cloud infrastructure because of the on-demand services and scalability features
of the cloud computing paradigm. However, processing IoT application requests on the cloud exclusively
is not an efficient solution for some IoT applications, especially time-sensitive ones. To address
this issue, Fog computing, which resides in between cloud and IoT devices, was proposed. In general,
in the Fog computing environment, IoT devices are connected to Fog devices. These Fog devices are
located in close proximity to users and are responsible for intermediate computation and storage.
Fog computing research is still in its infancy, and taxonomy-based investigation into the requirements
of Fog infrastructure, platform, and applications mapped to current research is still required.
This paper starts with an overview of Fog computing in which the definition of Fog computing, research
trends, and the technical differences between Fog and cloud are reviewed. Then, we investigate
numerous proposed Fog computing architecture and describe the components of these architectures
in detail. From this, the role of each component will be defined, which will help in the deployment
of Fog computing. Next, a taxonomy of Fog computing is proposed by considering the requirements
of the Fog computing paradigm. We also discuss existing research works and gaps in resource allocation
and scheduling, fault tolerance, simulation tools, and Fog-based microservices. Finally, by
addressing the limitations of current research works, we present some open issues, which will determine
the future research direction. 