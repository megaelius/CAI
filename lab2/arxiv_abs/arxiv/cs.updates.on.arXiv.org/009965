Cloud computing has motivated renewed interest in resource allocation problems with new consumption
models. A common goal is to share a resource, such as CPU or I/O bandwidth, among distinct users with
different demand patterns as well as different quality of service requirements. To ensure these
service requirements, cloud offerings often come with a service level agreement (SLA) between
the provider and the users. An SLA specifies the amount of a resource a user is entitled to utilize.
In many cloud settings, providers would like to operate resources at high utilization while simultaneously
respecting individual SLAs. There is typically a tradeoff between these two objectives; for example,
utilization can be increased by shifting away resources from idle users to "scavenger" workload,
but with the risk of the former then becoming active again. We study this fundamental tradeoff by
formulating a resource allocation model that captures basic properties of cloud computing systems,
including SLAs, highly limited feedback about the state of the system, and variable and unpredictable
input sequences. Our main result is a simple and practical algorithm that achieves near-optimal
performance on the above two objectives. First, we guarantee nearly optimal utilization of the
resource even if compared to the omniscient offline dynamic optimum. Second, we simultaneously
satisfy all individual SLAs up to a small error. The main algorithmic tool is a multiplicative weight
update algorithm, and a duality argument to obtain its guarantees. Experiments on both synthetic
and real production traces demonstrate the merits of our algorithm in practical settings. 