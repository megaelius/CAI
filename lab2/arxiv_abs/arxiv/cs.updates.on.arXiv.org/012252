Compared with Generative Adversarial Networks (GAN), Energy-Based generative Models (EBMs)
possess two appealing properties: i) they can be directly optimized without requiring an auxiliary
network during the learning and synthesizing; ii) they can better approximate underlying distribution
of the observed data by learning explicitly potential functions. This paper studies a branch of
EBMs, i.e., energy-based Generative ConvNets (GCNs), which minimize their energy function defined
by a bottom-up ConvNet. From the perspective of particle physics, we solve the problem of unstable
energy dissipation that might damage the quality of the synthesized samples during the maximum
likelihood learning. Specifically, we firstly establish a connection between the classical FRAME
model [1] and the dynamic physics process. Then, we generalize the GCN in discrete flow with a certain
metric measure from particle perspective. To address KL-vanishing issue, we further reformulate
GCN from the KL discrete flow with KL divergence measure to a Jordan-Kinderleher-Otto (JKO) discrete
flow with Wasserastein distance metric and derive a Wasserastein GCN (wGCN). Based on these theoretical
studies on GCN, we finally derive a Generalized GCN (GGCN) to further improve the model generalization
and learning capability. GGCN introduces a hidden space mapping strategy by employing a normal
distribution for the reference distribution to address the learning bias issue. Due to MCMC sampling
in GCNs, it still suffers from a serious time-consuming issue when sampling steps increase; thus
a trainable non-linear upsampling function and an amortized learning are proposed to improve the
learning efficiency in such a symmetrical learning manner. Extensive experiments on several widely-used
image datasets demonstrate that our GGCN surpasses those of existing models in both model stability
and the quality of generated samples. 