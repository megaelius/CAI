Cross-modal retrieval has become a highlighted research topic for retrieval across multimedia
data such as image and text. A two-stage learning framework is widely adopted by most existing methods
based on Deep Neural Network (DNN): The first learning stage is to generate separate representation
for each modality, and the second learning stage is to get the cross-modal common representation.
However, the existing methods have three limitations: (1) In the first learning stage, they only
model intra-modality correlation, but ignore inter-modality correlation with rich complementary
context. (2) In the second learning stage, they only adopt shallow networks with single-loss regularization,
but ignore the intrinsic relevance of intra-modality and inter-modality correlation. (3) Only
original instances are considered while the complementary fine-grained clues provided by their
patches are ignored. For addressing the above problems, this paper proposes a cross-modal correlation
learning (CCL) approach with multi-grained fusion by hierarchical network, and the contributions
are as follows: (1) In the first learning stage, CCL exploits multi-level association with joint
optimization to preserve the complementary context from intra-modality and inter-modality correlation
simultaneously. (2) In the second learning stage, a multi-task learning strategy is designed to
adaptively balance the intra-modality semantic category constraints and inter-modality pairwise
similarity constraints. (3) CCL adopts multi-grained modeling, which fuses the coarse-grained
instances and fine-grained patches to make cross-modal correlation more precise. Comparing with
13 state-of-the-art methods on 6 widely-used cross-modal datasets, the experimental results
show our CCL approach achieves the best performance. 