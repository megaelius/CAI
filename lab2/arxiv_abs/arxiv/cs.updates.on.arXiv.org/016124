Online metric learning has been widely exploited for large-scale data classification due to the
low computational cost. However, amongst online practical scenarios where the features are evolving
(e.g., some features are vanished and some new features are augmented), most metric learning models
cannot be successfully applied into these scenarios although they can tackle the evolving instances
efficiently. To address the challenge, we propose a new online Evolving Metric Learning (EML) model
for incremental and decremental features, which can handle the instance and feature evolutions
simultaneously by incorporating with a smoothed Wasserstein metric distance. Specifically,
our model contains two essential stages: the Transforming stage (T-stage) and the Inheriting stage
(I-stage). For the T-stage, we propose to extract important information from vanished features
while neglecting non-informative knowledge, and forward it into survived features by transforming
them into a low-rank discriminative metric space. It further explores the intrinsic low-rank structure
of heterogeneous samples to reduce the computation and memory burden especially for highly-dimensional
large-scale data. For the I-stage, we inherit the metric performance of survived features from
the T-stage and then expand to include the augmented new features. Moreover, the smoothed Wasserstein
distance is utilized to characterize the similarity relations among the complex and heterogeneous
data, since the evolving features in the different stages are not strictly aligned. In addition
to tackling the challenges in one-shot case, we also extend our model into multi-shot scenario.
After deriving an efficient optimization method for both T-stage and I-stage, extensive experiments
on several benchmark datasets verify the superiority of our model. 