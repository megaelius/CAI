Current state-of-the-art automatic software repair (ASR) techniques rely heavily on incomplete
specifications, e.g., test suites, to generate repairs. This, however, may render ASR tools to
generate incorrect repairs that do not generalize. To assess patch correctness, researchers have
been following two typical ways separately: (1) Automated annotation, wherein patches are automatically
labeled by an independent test suite (ITS) - a patch passing the ITS is regarded as correct or generalizable,
and incorrect otherwise, (2) Author annotation, wherein authors of ASR techniques annotate correctness
labels of patches generated by their and competing tools by themselves. While automated annotation
fails to prove that a patch is actually correct, author annotation is prone to subjectivity. This
concern has caused an on-going debate on appropriate ways to assess the effectiveness of numerous
ASR techniques proposed recently. To address this concern, we propose to assess reliability of
author and automated annotations on patch correctness assessment. We do this by first constructing
a gold set of correctness labels for 189 randomly selected patches generated by 8 state-of-the-art
ASR techniques through a user study involving 35 professional developers as independent annotators.
By measuring inter-rater agreement as a proxy for annotation quality - as commonly done in the literature
- we demonstrate that our constructed gold set is on par with other high-quality gold sets. We then
compare labels generated by author and automated annotations with this gold set to assess reliability
of the patch assessment methodologies. We subsequently report several findings and highlight
implications for future studies. 