Distance metric learning (DML) plays a crucial role in diverse machine learning algorithms and
applications. When the labeled information in target domain is limited, transfer metric learning
(TML) helps to learn the metric by leveraging the sufficient information from other related domains.
Multi-task metric learning (MTML), which can be regarded as a special case of TML, performs transfer
across all related domains. Current TML tools usually assume that the same feature representation
is exploited for different domains. However, in real-world applications, data may be drawn from
heterogeneous domains. Heterogeneous transfer learning approaches can be adopted to remedy this
drawback by deriving a metric from the learned transformation across different domains. But they
are often limited in that only two domains can be handled. To appropriately handle multiple domains,
we develop a novel heterogeneous multi-task metric learning (HMTML) framework. In HMTML, the metrics
of all different domains are learned together. The transformations derived from the metrics are
utilized to induce a common subspace, and the high-order covariance among the predictive structures
of these domains is maximized in this subspace. There do exist a few heterogeneous transfer learning
approaches that deal with multiple domains, but the high-order statistics (correlation information),
which can only be exploited by simultaneously examining all domains, is ignored in these approaches.
Compared with them, the proposed HMTML can effectively explore such high-order information, thus
obtaining more reliable feature transformations and metrics. Effectiveness of our method is validated
by the extensive and intensive experiments on text categorization, scene classification, and
social image annotation. 