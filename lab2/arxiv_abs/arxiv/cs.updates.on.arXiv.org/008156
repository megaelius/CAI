Anomaly detection from a driver's perspective when driving is important to autonomous vehicles.
As a part of Advanced Driver Assistance Systems (ADAS), it can remind the driver about dangers timely.
Compared with traditional studied scenes such as the university campus and market surveillance
videos, it is difficult to detect abnormal event from a driver's perspective due to camera waggle,
abidingly moving background, drastic change of vehicle velocity, etc. To tackle these specific
problems, this paper proposes a spatial localization constrained sparse coding approach for anomaly
detection in traffic scenes, which firstly measures the abnormality of motion orientation and
magnitude respectively and then fuses these two aspects to obtain a robust detection result. The
main contributions are threefold: 1) This work describes the motion orientation and magnitude
of the object respectively in a new way, which is demonstrated to be better than the traditional motion
descriptors. 2) The spatial localization of object is taken into account of the sparse reconstruction
framework, which utilizes the scene's structural information and outperforms the conventional
sparse coding methods. 3) Results of motion orientation and magnitude are adaptively weighted
and fused by a Bayesian model, which makes the proposed method more robust and handle more kinds of
abnormal events. The efficiency and effectiveness of the proposed method are validated by testing
on nine difficult video sequences captured by ourselves. Observed from the experimental results,
the proposed method is more effective and efficient than the popular competitors, and yields a higher
performance. 