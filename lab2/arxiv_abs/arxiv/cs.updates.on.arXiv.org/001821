In recent years, we witnessed a huge success of Convolutional Neural Networks on the task of the image
classification. However, these models are notoriously data hungry and require tons of training
images to learn the parameters. In contrast, people are far better learner who can learn a new concept
very fast with only a few samples. The plausible mysteries making the difference are two fundamental
learning mechanisms: learning to learn and learning by analogy. In this paper, we attempt to investigate
a new human-like learning method by organically combining these two mechanisms. In particular,
we study how to generalize the classification parameters of previously learned concepts to a new
concept. we first propose a novel Visual Analogy Network Embedded Regression (VANER) model to jointly
learn a low-dimensional embedding space and a linear mapping function from the embedding space
to classification parameters for base classes. We then propose an out-of-sample embedding method
to learn the embedding of a new class represented by a few samples through its visual analogy with
base classes. By inputting the learned embedding into VANER, we can derive the classification parameters
for the new class.These classification parameters are purely generalized from base classes (i.e.
transferred classification parameters), while the samples in the new class, although only a few,
can also be exploited to generate a set of classification parameters (i.e. model classification
parameters). Therefore, we further investigate the fusion strategy of the two kinds of parameters
so that the prior knowledge and data knowledge can be fully leveraged. We also conduct extensive
experiments on ImageNet and the results show that our method can consistently and significantly
outperform state-of-the-art baselines. 