In recent years there has been a push to integrate symbolic AI and deep learning, as it is argued that
the strengths and weaknesses of these approaches are complementary. One such trend in the literature
are weakly supervised learning techniques that use operators from fuzzy logics. They employ prior
background knowledge described in logic to benefit the training of a neural network from unlabeled
and noisy data. By interpreting logical symbols using neural networks, this background knowledge
can be added to regular loss functions used in deep learning to integrate reasoning and learning.
In this paper, we analyze how a large collection of logical operators from the fuzzy logic literature
behave in a differentiable setting. We find large differences between the formal properties of
these operators that are of crucial importance in a differentiable learning setting. We show that
many of these operators, including some of the best known, are highly unsuitable for use in a differentiable
learning setting. A further finding concerns the treatment of implication in these fuzzy logics,
with a strong imbalance between gradients driven by the antecedent and the consequent of the implication.
Finally, we empirically show that it is possible to use Differentiable Fuzzy Logics for semi-supervised
learning. However, to achieve the most significant performance improvement over a supervised
baseline, we have to resort to non-standard combinations of logical operators which perform well
in learning, but which no longer satisfy the usual logical laws. We end with a discussion on extensions
to large-scale problems. 