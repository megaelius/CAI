How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled
representation learning approach posits that such an agent would benefit from separating out (disentangling)
the underlying structure of the world into disjoint parts of its representation. However, there
is no generally agreed-upon definition of disentangling, not least because it is unclear how to
formalise the notion of world structure beyond toy datasets with a known ground truth generative
process. Here we propose that a principled solution to characterising disentangled representations
can be found by focusing on the transformation properties of the world. In particular, we suggest
that those transformations that change only some properties of the underlying world state, while
leaving all other properties invariant, are what gives exploitable structure to any kind of data.
Similar ideas have already been successfully applied in physics, where the study of symmetry transformations
has revolutionised the understanding of the world structure. By connecting symmetry transformations
to vector representations using the formalism of group and representation theory we arrive at the
first formal definition of disentangled representations. Our new definition is in agreement with
many of the current intuitions about disentangling, while also providing principled resolutions
to a number of previous points of contention. While this work focuses on formally defining disentangling
- as opposed to solving the learning problem - we believe that the shift in perspective to studying
data transformations can stimulate the development of better representation learning algorithms.
