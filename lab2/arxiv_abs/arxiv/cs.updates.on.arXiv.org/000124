Since its introduction by Valiant in 1984, PAC learning of DNF expressions remains one of the central
problems in learning theory. We consider this problem in the setting where the underlying distribution
is uniform, or more generally, a product distribution. Kalai, Samorodnitsky and Teng (2009) showed
that in this setting a DNF expression can be efficiently approximated from its "heavy" low-degree
Fourier coefficients alone. This is in contrast to previous approaches where boosting was used
and thus Fourier coefficients of the target function modified by various distributions were needed.
This property is crucial for learning of DNF expressions over smoothed product distributions,
a learning model introduced by Kalai et al. (2009) and inspired by the seminal smoothed analysis
model of Spielman and Teng (2001). We introduce a new approach to learning (or approximating) a polynomial
threshold functions which is based on creating a function with range [-1,1] that approximately
agrees with the unknown function on low-degree Fourier coefficients. We then describe conditions
under which this is sufficient for learning polynomial threshold functions. Our approach yields
a new, simple algorithm for approximating any polynomial-size DNF expression from its "heavy"
low-degree Fourier coefficients alone. Our algorithm greatly simplifies the proof of learnability
of DNF expressions over smoothed product distributions. We also describe an application of our
algorithm to learning monotone DNF expressions over product distributions. Building on the work
of Servedio (2001), we give an algorithm that runs in time $\poly((s \cdot \log{(s/\eps)})^{\log{(s/\eps)}},
n)$, where $s$ is the size of the target DNF expression and $\eps$ is the accuracy. This improves on
$\poly((s \cdot \log{(ns/\eps)})^{\log{(s/\eps)} \cdot \log{(1/\eps)}}, n)$ bound of Servedio
(2001). 