Most topic models are constructed under the assumption that documents follow a multinomial distribution.
The Poisson distribution is an alternative distribution to describe the probability of count data.
For topic modelling, the Poisson distribution describes the number of occurrences of a word in documents
of fixed length. The Poisson distribution has been successfully applied in text classification,
but its application to topic modelling is not well documented, specifically in the context of a generative
probabilistic model. Furthermore, the few Poisson topic models in literature are admixture models,
making the assumption that a document is generated from a mixture of topics. In this study, we focus
on short text. Many studies have shown that the simpler assumption of a mixture model fits short text
better. With mixture models, as opposed to admixture models, the generative assumption is that
a document is generated from a single topic. One topic model, which makes this one-topic-per-document
assumption, is the Dirichlet-multinomial mixture model. The main contributions of this work are
a new Gamma-Poisson mixture model, as well as a collapsed Gibbs sampler for the model. The benefit
of the collapsed Gibbs sampler derivation is that the model is able to automatically select the number
of topics contained in the corpus. The results show that the Gamma-Poisson mixture model performs
better than the Dirichlet-multinomial mixture model at selecting the number of topics in labelled
corpora. Furthermore, the Gamma-Poisson mixture produces better topic coherence scores than
the Dirichlet-multinomial mixture model, thus making it a viable option for the challenging task
of topic modelling of short text. 