The state of the art in many computer vision tasks is represented by Convolutional Neural Networks
(CNNs). Although their hierarchical organization and local feature extraction are inspired by
the structure of primate visual systems, the lack of lateral connections in such architectures
critically distinguishes their analysis from biological object processing. The idea of enriching
CNNs with recurrent lateral connections of convolutional type has been put into practice in recent
years, in the form of learned recurrent kernels with no geometrical constraints. In the present
work, we introduce biologically plausible lateral kernels encoding a notion of correlation between
the feedforward filters of a CNN: at each layer, the associated kernel acts as a transition kernel
on the space of activations. The lateral kernels are defined in terms of the filters, thus providing
a parameter-free approach to assess the geometry of horizontal connections based on the feedforward
structure. We then test this new architecture, which we call KerCNN, on a generalization task related
to global shape analysis and pattern completion: once trained for performing basic image classification,
the network is evaluated on corrupted testing images. The image perturbations examined are designed
to undermine the recognition of the images via local features, thus requiring an integration of
context information - which in biological vision is critically linked to lateral connectivity.
Our KerCNNs turn out to be far more stable than CNNs and recurrent CNNs to such degradations, thus
validating this biologically inspired approach to reinforce object recognition under challenging
conditions. 