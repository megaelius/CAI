Object detection remains an active area of research in the field of computer vision, and considerable
advances and successes has been achieved in this area through the design of deep convolutional neural
networks for tackling object detection. Despite these successes, one of the biggest challenges
to widespread deployment of such object detection networks on edge and mobile scenarios is the high
computational and memory requirements. As such, there has been growing research interest in the
design of efficient deep neural network architectures catered for edge and mobile usage. In this
study, we introduce YOLO Nano, a highly compact deep convolutional neural network for the task of
object detection. A human-machine collaborative design strategy is leveraged to create YOLO Nano,
where principled network design prototyping, based on design principles from the YOLO family of
single-shot object detection network architectures, is coupled with machine-driven design exploration
to create a compact network with highly customized module-level macroarchitecture and microarchitecture
designs tailored for the task of embedded object detection. The proposed YOLO Nano possesses a model
size of ~4.0MB (>15.1x and >8.3x smaller than Tiny YOLOv2 and Tiny YOLOv3, respectively) and requires
4.57B operations for inference (>34% and ~17% lower than Tiny YOLOv2 and Tiny YOLOv3, respectively)
while still achieving an mAP of ~69.1% on the VOC 2007 dataset (~12% and ~10.7% higher than Tiny YOLOv2
and Tiny YOLOv3, respectively). Experiments on inference speed and power efficiency on a Jetson
AGX Xavier embedded module at different power budgets further demonstrate the efficacy of YOLO
Nano for embedded scenarios. 