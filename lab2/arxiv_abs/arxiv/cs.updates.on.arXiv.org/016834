Biological neural networks are equipped with an inherent capability to continuously adapt through
online learning. This aspect remains in stark contrast to learning with error backpropagation
through time (BPTT) applied to recurrent neural networks (RNNs), or recently even to biologically-inspired
spiking neural networks (SNNs), because the unrolling through time of BPTT leads to system-locking
problems. Online learning has recently regained the attention of the research community, focusing
either on approaches that approximate BPTT or on biologically-plausible schemes applied in SNNs.
Here we present an alternative perspective that is based on a clear separation of spatial and temporal
gradient components. Combined with insights from biology, we derive from first principles a novel
online learning algorithm, called online spatio-temporal learning (OSTL), which is gradient-equivalent
to BPTT for shallow networks. We apply OSTL to SNNs allowing them for the first time to be trained online
with BPTT-equivalent gradients. In addition, the proposed formulation uncovers a class of SNN
architectures trainable online at low complexity. Moreover, we extend OSTL to deep networks while
maintaining its key characteristics. Besides SNNs, the generic form of OSTL is applicable to a wide
range of network architectures, including networks comprising long short-term memory (LSTM)
and gated recurrent units (GRU). We demonstrate the operation of our algorithm on various tasks
from language modelling to speech recognition, and obtain results on par with the BPTT baselines.
The proposed algorithm provides a framework for developing succinct and efficient online training
approaches for SNNs and in general deep RNNs. 