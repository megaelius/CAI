Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node
feature information with the graph structure by using neural networks to pass messages through
edges in the graph. However, incorporating both graph structure and feature information leads
to complex non-linear models and explaining predictions made by GNNs remains to be a challenging
task. Here we propose GnnExplainer, a general model-agnostic approach for providing interpretable
explanations for predictions of any GNN-based model on any graph-based machine learning task (node
and graph classification, link prediction). In order to explain a given node's predicted label,
GnnExplainer provides a local interpretation by highlighting relevant features as well as an important
subgraph structure by identifying the edges that are most relevant to the prediction. Additionally,
the model provides single-instance explanations when given a single prediction as well as multi-instance
explanations that aim to explain predictions for an entire class of instances/nodes. We formalize
GnnExplainer as an optimization task that maximizes the mutual information between the prediction
of the full model and the prediction of simplified explainer model. We experiment on synthetic as
well as real-world data. On synthetic data we demonstrate that our approach is able to highlight
relevant topological structures from noisy graphs. We also demonstrate GnnExplainer to provide
a better understanding of pre-trained models on real-world tasks. GnnExplainer provides a variety
of benefits, from the identification of semantically relevant structures to explain predictions
to providing guidance when debugging faulty graph neural network models. 