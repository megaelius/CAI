An important objective for analyzing real-world graphs is to achieve scalable performance on large,
streaming graphs. A challenging and relevant example is the graph partition problem. As a combinatorial
problem, graph partition is NP-hard, but existing relaxation methods provide reasonable approximate
solutions that can be scaled for large graphs. Competitive benchmarks and challenges have proven
to be an effective means to advance state-of-the-art performance and foster community collaboration.
This paper describes a graph partition challenge with a baseline partition algorithm of sub-quadratic
complexity. The algorithm employs rigorous Bayesian inferential methods based on a statistical
model that captures characteristics of the real-world graphs. This strong foundation enables
the algorithm to address limitations of well-known graph partition approaches such as modularity
maximization. This paper describes various aspects of the challenge including: (1) the data sets
and streaming graph generator, (2) the baseline partition algorithm with pseudocode, (3) an argument
for the correctness of parallelizing the Bayesian inference, (4) different parallel computation
strategies such as node-based parallelism and matrix-based parallelism, (5) evaluation metrics
for partition correctness and computational requirements, (6) preliminary timing of a Python-based
demonstration code and the open source C++ code, and (7) considerations for partitioning the graph
in streaming fashion. Data sets and source code for the algorithm as well as metrics, with detailed
documentation are available at GraphChallenge.org. 