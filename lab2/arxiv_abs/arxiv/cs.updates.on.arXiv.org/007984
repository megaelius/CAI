Many sleep studies suffer from the problem of insufficient data to fully utilize deep neural networks
as different labs use different recordings set ups, leading to the need of training automated algorithms
on rather small databases, whereas large annotated databases are around but cannot be directly
included into these studies for data compensation due to channel mismatch. This work presents a
deep transfer learning approach to overcome the channel mismatch problem and transfer knowledge
from a large dataset to a small cohort to study automatic sleep staging with single-channel input.
We employ the state-of-the-art SeqSleepNet and train the network in the source domain, i.e. the
large dataset. Afterwards, the pretrained network is finetuned in the target domain, i.e. the small
cohort, to complete knowledge transfer. We study two transfer learning scenarios with slight and
heavy channel mismatch between the source and target domains. We also investigate whether, and
if so, how finetuning entirely or partially the pretrained network would affect the performance
of sleep staging on the target domain. Using the Montreal Archive of Sleep Studies (MASS) database
consisting of 200 subjects as the source domain and the Sleep-EDF Expanded database consisting
of 20 subjects as the target domain in this study, our experimental results show significant performance
improvement on sleep staging achieved with the proposed deep transfer learning approach. Furthermore,
these results also reveal the essential of finetuning the feature-learning parts of the pretrained
network to be able to bypass the channel mismatch problem. 