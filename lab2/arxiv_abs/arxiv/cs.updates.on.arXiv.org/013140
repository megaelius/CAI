Accurate and efficient models for rainfall runoff (RR) simulations are crucial for flood risk management.
Most rainfall models in use today are process-driven; i.e. they solve either simplified empirical
formulas or some variation of the St. Venant (shallow water) equations. With the development of
machine-learning techniques, we may now be able to emulate rainfall models using, for example,
neural networks. In this study, a data-driven RR model using a sequence-to-sequence Long-short-Term-Memory
(LSTM) network was constructed. The model was tested for a watershed in Houston, TX, known for severe
flood events. The LSTM network's capability in learning long-term dependencies between the input
and output of the network allowed modeling RR with high resolution in time (15 minutes). Using 10-years
precipitation from 153 rainfall gages and river channel discharge data (more than 5.3 million data
points), and by designing several numerical tests the developed model performance in predicting
river discharge was tested. The model results were also compared with the output of a process-driven
model Gridded Surface Subsurface Hydrologic Analysis (GSSHA). Moreover, physical consistency
of the LSTM model was explored. The model results showed that the LSTM model was able to efficiently
predict discharge and achieve good model performance. When compared to GSSHA, the data-driven
model was more efficient and robust in terms of prediction and calibration. Interestingly, the
performance of the LSTM model improved (test Nash-Sutcliffe model efficiency from 0.666 to 0.942)
when a selected subset of rainfall gages based on the model performance, were used as input instead
of all rainfall gages. 