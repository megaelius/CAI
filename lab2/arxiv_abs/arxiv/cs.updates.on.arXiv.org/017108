General Game Playing (GGP) is a framework in which an artificial intelligence program is required
to play a variety of games successfully. It acts as a test bed for AI and motivator of research. The
AI is given a random game description at runtime which it then plays. The framework includes repositories
of game rules. The Inductive General Game Playing (IGGP) problem challenges machine learning systems
to learn these GGP game rules by watching the game being played. In other words, IGGP is the problem
of inducing general game rules from specific game observations. Inductive Logic Programming (ILP)
has shown to be a promising approach to this problem though it has been demonstrated that it is still
a hard problem for ILP systems. Existing work on IGGP has always assumed that the game player being
observed makes random moves. This is not representative of how a human learns to play a game. With
random gameplay situations that would normally be encountered when humans play are not present.
To address this limitation, we analyse the effect of using intelligent versus random gameplay traces
as well as the effect of varying the number of traces in the training set. We use Sancho, the 2014 GGP
competition winner, to generate intelligent game traces for a large number of games. We then use
the ILP systems, Metagol, Aleph and ILASP to induce game rules from the traces. We train and test the
systems on combinations of intelligent and random data including a mixture of both. We also vary
the volume of training data. Our results show that whilst some games were learned more effectively
in some of the experiments than others no overall trend was statistically significant. The implications
of this work are that varying the quality of training data as described in this paper has strong effects
on the accuracy of the learned game rules; however one solution does not work for all games. 