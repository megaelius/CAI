Background: Many efforts have been put into the use of automated approaches, such as natural language
processing (NLP), to mine or extract data from free-text medical records to construct comprehensive
patient profiles for delivering better health-care. Reusing NLP models in new settings, however,
remains cumbersome - requiring validation and/or retraining on new data iteratively to achieve
convergent results. Objective: The aim of this work is to minimize the effort involved in reusing
NLP models on free-text medical records. Methods: We formally define and analyse the model adaptation
problem in phenotype-mention identification tasks. We identify "duplicate waste" and "imbalance
waste", which collectively impede efficient model reuse. We propose a phenotype embedding based
approach to minimize these sources of waste without the need for labelled data from new settings.
Results: We conduct experiments on data from a large mental health registry to reuse NLP models in
four phenotype-mention identification tasks. The proposed approach can choose the best model
for a new task, identifying up to 76% (duplicate waste), i.e. phenotype mentions without the need
for validation and model retraining, and with very good performance (93-97% accuracy). It can also
provide guidance for validating and retraining the selected model for novel language patterns
in new tasks, saving around 80% (imbalance waste), i.e. the effort required in "blind" model-adaptation
approaches. Conclusions: Adapting pre-trained NLP models for new tasks can be more efficient and
effective if the language pattern landscapes of old settings and new settings can be made explicit
and comparable. Our experiments show that the phenotype-mention embedding approach is an effective
way to model language patterns for phenotype-mention identification tasks and that its use can
guide efficient NLP model reuse. 