In recent years, Siamese network based trackers have significantly advanced the state-of-the-art
in real-time tracking. However, state-of-the-art Siamese trackers suffer from high memory cost
which restricts their applicability in mobile applications having strict constraints on memory
budget. To address this issue, we propose a novel distilled Siamese tracking framework to learn
small, fast yet accurate trackers (students), which capture critical knowledge from large Siamese
trackers (teachers) by a teacher-students knowledge distillation model. This model is intuitively
inspired by a one-teacher vs multi-students learning mechanism, which is the most usual teaching
method in the school. In particular, it contains a single teacher-student distillation model and
a student-student knowledge sharing mechanism. The first one is designed by a tracking-specific
distillation strategy to transfer knowledge from the teacher to students. The later is utilized
for mutual learning between students to enable an in-depth knowledge understanding. To the best
of our knowledge, we are the first to investigate knowledge distillation for Siamese trackers and
propose a distilled Siamese tracking framework. We demonstrate the generality and effectiveness
of our framework by conducting a theoretical analysis and extensive empirical evaluations on several
popular Siamese trackers. The results on five tracking benchmarks clearly show that the proposed
distilled trackers achieve compression rates up to 18$\times$ and frame-rates of $265$ FPS with
speedups of 3$\times$, while obtaining similar or even slightly improved tracking accuracy. 