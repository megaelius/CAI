Canonical Correlation Analysis (CCA) is a classic technique for multi-view data analysis. To overcome
the deficiency of linear correlation in practical multi-view learning tasks, various CCA variants
were proposed to capture nonlinear dependency. However, it is non-trivial to have an in-principle
understanding of these variants due to their inherent restrictive assumption on the data and latent
code distributions. Although some works have studied probabilistic interpretation for CCA, these
models still require the explicit form of the distributions to achieve a tractable solution for
the inference. In this work, we study probabilistic interpretation for CCA based on implicit distributions.
We present Conditional Mutual Information (CMI) as a new criterion for CCA to consider both linear
and nonlinear dependency for arbitrarily distributed data. To eliminate direct estimation for
CMI, in which explicit form of the distributions is still required, we derive an objective which
can provide an estimation for CMI with efficient inference methods. To facilitate Bayesian inference
of multi-view analysis, we propose Adversarial CCA (ACCA), which achieves consistent encoding
for multi-view data with the consistent constraint imposed on the marginalization of the implicit
posteriors. Such a model would achieve superiority in the alignment of the multi-view data with
implicit distributions. It is interesting to note that most of the existing CCA variants can be connected
with our proposed CCA model by assigning specific form for the posterior and likelihood distributions.
Extensive experiments on nonlinear correlation analysis and cross-view generation on benchmark
and real-world datasets demonstrate the superiority of our model. 