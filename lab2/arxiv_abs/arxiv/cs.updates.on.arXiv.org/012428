Context plays a crucial role in visual recognition as it provides complementary clues for different
learning tasks including image classification and annotation. As the performances of these tasks
are currently reaching a plateau, any extra knowledge, including context, should be leveraged
in order to seek significant leaps in these performances. In the particular scenario of kernel machines,
context-aware kernel design aims at learning positive semi-definite similarity functions which
return high values not only when data share similar contents, but also similar structures (a.k.a
contexts). However, the use of context in kernel design has not been fully explored; indeed, context
in these solutions is handcrafted instead of being learned. In this paper, we introduce a novel deep
network architecture that learns context in kernel design. This architecture is fully determined
by the solution of an objective function mixing a content term that captures the intrinsic similarity
between data, a context criterion which models their structure and a regularization term that helps
designing smooth kernel network representations. The solution of this objective function defines
a particular deep network architecture whose parameters correspond to different variants of learned
contexts including layerwise, stationary and classwise; larger values of these parameters correspond
to the most influencing contextual relationships between data. Extensive experiments conducted
on the challenging ImageCLEF Photo Annotation and Corel5k benchmarks show that our deep context
networks are highly effective for image classification and the learned contexts further enhance
the performance of image annotation. 