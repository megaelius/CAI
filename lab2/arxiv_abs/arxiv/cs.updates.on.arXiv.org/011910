As the development of neural networks, more and more deep neural networks are adopted in various
tasks, such as image classification. However, as the huge computational overhead, these networks
could not be applied on mobile devices or other low latency scenes. To address this dilemma, multi-classifier
convolutional network is proposed to allow faster inference via early classifiers with the corresponding
classifiers. These networks utilize sophisticated designing to increase the early classifier
accuracy. However, naively training the multi-classifier network could hurt the performance
(accuracy) of deep neural networks as early classifiers throughout interfere with the feature
generation process. In this paper, we propose a general training framework named multi-self-distillation
learning (MSD), which mining knowledge of different classifiers within the same network and increase
every classifier accuracy. Our approach can be applied not only to multi-classifier networks,
but also modern CNNs (e.g., ResNet Series) augmented with additional side branch classifiers.
We use sampling-based branch augmentation technique to transform a single-classifier network
into a multi-classifier network. This reduces the gap of capacity between different classifiers,
and improves the effectiveness of applying MSD. Our experiments show that MSD improves the accuracy
of various networks: enhancing the accuracy of every classifier significantly for existing multi-classifier
network (MSDNet), improving vanilla single-classifier networks with internal classifiers with
high accuracy, while also improving the final accuracy. 