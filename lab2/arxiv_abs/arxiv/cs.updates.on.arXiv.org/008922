Recurrent neural networks (RNNs) are commonly applied to clinical time-series data with the goal
of learning patient risk stratification models. Their effectiveness is due, in part, to their use
of parameter sharing over time (i.e., cells are repeated hence the name recurrent). We hypothesize,
however, that this trait also contributes to the increased difficulty such models have with learning
relationships that change over time. Conditional shift, i.e., changes in the relationship between
the input X and the output y, arises if the risk factors for the event of interest change over the course
of a patient admission. While in theory, RNNs and gated RNNs (e.g., LSTMs) in particular should be
capable of learning time-varying relationships, when training data are limited, such models often
fail to accurately capture these dynamics. We illustrate the advantages and disadvantages of complete
weight sharing (RNNs) by comparing an LSTM with shared parameters to a sequential architecture
with time-varying parameters on three clinically-relevant prediction tasks: acute respiratory
failure (ARF), shock, and in-hospital mortality. In experiments using synthetic data, we demonstrate
how weight sharing in LSTMs leads to worse performance in the presence of conditional shift. To improve
upon the dichotomy between complete weight sharing vs. no weight sharing, we propose a novel RNN
formulation based on a mixture model in which we relax weight sharing over time. The proposed method
outperforms standard LSTMs and other state-of-the-art baselines across all tasks. In settings
with limited data, relaxed weight sharing can lead to improved patient risk stratification performance.
