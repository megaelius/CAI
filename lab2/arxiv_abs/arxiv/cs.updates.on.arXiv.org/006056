Recent studies using deep neural networks have shown remarkable success in style transfer especially
for artistic and photo-realistic images. However, the approaches using global feature correlations
fail to capture small, intricate textures and maintain correct texture scales of the artworks,
and the approaches based on local patches are defective on global effect. In this paper, we present
a novel feature pyramid fusion neural network, dubbed GLStyleNet, which sufficiently takes into
consideration multi-scale and multi-level pyramid features by best aggregating layers across
a VGG network, and performs style transfer hierarchically with multiple losses of different scales.
Our proposed method retains high-frequency pixel information and low frequency construct information
of images from two aspects: loss function constraint and feature fusion. Our approach is not only
flexible to adjust the trade-off between content and style, but also controllable between global
and local. Compared to state-of-the-art methods, our method can transfer not just large-scale,
obvious style cues but also subtle, exquisite ones, and dramatically improves the quality of style
transfer. We demonstrate the effectiveness of our approach on portrait style transfer, artistic
style transfer, photo-realistic style transfer and Chinese ancient painting style transfer tasks.
Experimental results indicate that our unified approach improves image style transfer quality
over previous state-of-the-art methods, while also accelerating the whole process in a certain
extent. Our code is available at https://github.com/EndyWon/GLStyleNet. 