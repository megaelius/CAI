Deep learning has made major breakthroughs and progress in many fields. This is due to the powerful
automatic representation capabilities of deep learning. It has been proved that the design of the
network architecture is crucial to the feature representation of data and the final performance.
In order to obtain a good feature representation of data, the researchers designed various complex
network architectures. However, the design of the network architecture relies heavily on the researchers'
prior knowledge and experience. Therefore, a natural idea is to reduce human intervention as much
as possible and let the algorithm automatically design the architecture of the network. Thus going
further to the strong intelligence. In recent years, a large number of related algorithms for \textit{Neural
Architecture Search} (NAS) have emerged. They have made various improvements to the NAS algorithm,
and the related research work is complicated and rich. In order to reduce the difficulty for beginners
to conduct NAS-related research, a comprehensive and systematic survey on the NAS is essential.
Previously related surveys began to classify existing work mainly from the basic components of
NAS: search space, search strategy and evaluation strategy. This classification method is more
intuitive, but it is difficult for readers to grasp the challenges and the landmark work in the middle.
Therefore, in this survey, we provide a new perspective: starting with an overview of the characteristics
of the earliest NAS algorithms, summarizing the problems in these early NAS algorithms, and then
giving solutions for subsequent related research work. In addition, we conducted a detailed and
comprehensive analysis, comparison and summary of these works. Finally, we give possible future
research directions. 