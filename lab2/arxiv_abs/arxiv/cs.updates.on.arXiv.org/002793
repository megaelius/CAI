Generalization performance of classifiers in deep learning has recently become a subject of intense
study. Heavily over-parametrized deep models tend to fit training data exactly. Despite overfitting,
they perform well on test data, a phenomenon not yet fully understood. The first point of our paper
is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using
real-world and synthetic datasets, we establish that kernel classifiers trained to have zero classification
error (overfitting) or even zero regression error (interpolation) perform very well on test data.
We proceed to prove lower bounds on the norm of overfitted solutions for smooth kernels, showing
that they increase nearly exponentially with the data size. Since most generalization bounds depend
polynomially on the norm of the solution, this result implies that they diverge as data increases.
Furthermore, the existing bounds do not apply to interpolated classifiers. We also show experimentally
that (non-smooth) Laplacian kernels easily fit random labels using a version of SGD, a finding that
parallels results reported for ReLU neural networks. In contrast, fitting noisy data requires
many more epochs for smooth Gaussian kernels. The observation that the performance of overfitted
Laplacian and Gaussian classifiers on the test is quite similar, suggests that generalization
is tied to the properties of the kernel function rather than the optimization process. We see that
some key phenomena of deep learning are manifested similarly in kernel methods in the overfitted
regime. We argue that progress on understanding deep learning will be difficult, until more analytically
tractable "shallow" kernel methods are better understood. The experimental and theoretical results
presented in this paper indicate a need for new theoretical ideas for understanding classical kernel
methods. 