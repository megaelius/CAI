Fluoroscopy is the standard imaging modality used to guide hip surgery and is therefore a natural
sensor for computer-assisted navigation. In order to efficiently solve the complex registration
problems presented during navigation, human-assisted annotations of the intraoperative image
are typically required. This manual initialization interferes with the surgical workflow and
diminishes any advantages gained from navigation. We propose a method for fully automatic registration
using annotations produced by a neural network. Neural networks are trained to simultaneously
segment anatomy and identify landmarks in fluoroscopy. Training data is obtained using an intraoperatively
incompatible 2D/3D registration of hip anatomy. Ground truth 2D labels are established using projected
3D annotations. Intraoperative registration couples an intensity-based strategy with annotations
inferred by the network and requires no human assistance. Ground truth labels were obtained in 366
fluoroscopic images across 6 cadaveric specimens. In a leave-one-subject-out experiment, networks
obtained mean dice coefficients for left and right hemipelves, left and right femurs of 0.86, 0.87,
0.90, and 0.84. The mean 2D landmark error was 5.0 mm. The pelvis was registered within 1 degree for
86% of the images when using the proposed intraoperative approach with an average runtime of 7 seconds.
In comparison, an intensity-only approach without manual initialization, registered the pelvis
to 1 degree in 18% of images. We have created the first accurately annotated, non-synthetic, dataset
of hip fluoroscopy. By using these annotations as training data for neural networks, state of the
art performance in fluoroscopic segmentation and landmark localization was achieved. Integrating
these annotations allows for a robust, fully automatic, and efficient intraoperative registration
during fluoroscopic navigation of the hip. 