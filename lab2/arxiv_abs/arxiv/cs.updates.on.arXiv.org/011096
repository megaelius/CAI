Paper-intensive industries like insurance, law, and government have long leveraged optical character
recognition (OCR) to automatically transcribe hordes of scanned documents into text strings for
downstream processing. Even in 2019, there are still many scanned documents and mail that come into
businesses in non-digital format. Text to be extracted from real world documents is often nestled
inside rich formatting, such as tabular structures or forms with fill-in-the-blank boxes or underlines
whose ink often touches or even strikes through the ink of the text itself. Further, the text region
could have random ink smudges or spurious strokes. Such ink artifacts can severely interfere with
the performance of recognition algorithms or other downstream processing tasks. In this work,
we propose DeepErase, a neural-based preprocessor to erase ink artifacts from text images. We devise
a method to programmatically assemble real text images and real artifacts into realistic-looking
"dirty" text images, and use them to train an artifact segmentation network in a weakly supervised
manner, since pixel-level annotations are automatically obtained during the assembly process.
In addition to high segmentation accuracy, we show that our cleansed images achieve a significant
boost in recognition accuracy by popular OCR software such as Tesseract 4.0. Finally, we test DeepErase
on out-of-distribution datasets (NIST SDB) of scanned IRS tax return forms and achieve double-digit
improvements in accuracy. All experiments are performed on both printed and handwritten text.
Code for all experiments is available at https://github.com/yikeqicn/DeepErase 