Early detection of psychological distress is key to effective treatment. Automatic detection
of distress, such as depression, is an active area of research. Current approaches utilise vocal,
facial, and bodily modalities. Of these, the bodily modality is the least investigated, partially
due to the difficulty in extracting bodily representations from videos, and partially due to the
lack of viable datasets. Existing body modality approaches use automatic categorization of expressions
to represent body language as a series of specific expressions, much like words within natural language.
In this dissertation I present a new type of feature, within the body modality, that represents meta
information of gestures, such as speed, and use it to predict a non-clinical depression label. This
differs to existing work by representing overall behaviour as a small set of aggregated meta features
derived from a person's movement. In my method I extract pose estimation from videos, detect gestures
within body parts, extract meta information from individual gestures, and finally aggregate these
features to generate a small feature vector for use in prediction tasks. I introduce a new dataset
of 65 video recordings of interviews with self-evaluated distress, personality, and demographic
labels. This dataset enables the development of features utilising the whole body in distress detection
tasks. I evaluate my newly introduced meta-features for predicting depression, anxiety, perceived
stress, somatic stress, five standard personality measures, and gender. A linear regression based
classifier using these features achieves a 82.70% F1 score for predicting depression within my
novel dataset. 