Deep learning methods that extract answers for non-factoid questions from QA sites are seen as critical
since they can assist users in reaching their next decisions through conversations with AI systems.
The current methods, however, have the following two problems: (1) They can not understand the ambiguous
use of words in the questions as word usage can strongly depend on the context. As a result, the accuracies
of their answer selections are not good enough. (2) The current methods can only select from among
the answers held by QA sites and can not generate new ones. Thus, they can not answer the questions
that are somewhat different with those stored in QA sites. Our solution, Neural Answer Construction
Model, tackles these problems as it: (1) Incorporates the biases of semantics behind questions
into word embeddings while also computing them regardless of the semantics. As a result, it can extract
answers that suit the contexts of words used in the question as well as following the common usage
of words across semantics. This improves the accuracy of answer selection. (2) Uses biLSTM to compute
the embeddings of questions as well as those of the sentences often used to form answers. It then simultaneously
learns the optimum combination of those sentences as well as the closeness between the question
and those sentences. As a result, our model can construct an answer that corresponds to the situation
that underlies the question; it fills the gap between answer selection and generation and is the
first model to move beyond the current simple answer selection model for non-factoid QAs. Evaluations
using datasets created for love advice stored in the Japanese QA site, Oshiete goo, indicate that
our model achieves 20% higher accuracy in answer creation than the strong baselines. Our model is
practical and has already been applied to the love advice service in Oshiete goo. 