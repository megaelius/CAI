Robot-assisted minimally invasive surgery is improving surgeon performance and patient outcomes.
This innovation is also turning what has been a subjective practice into motion sequences that can
be precisely measured. A growing number of studies have used machine learning to analyze video and
kinematic data captured from surgical robots. In these studies, models are typically trained on
benchmark datasets for representative surgical tasks to assess surgeon skill levels. While they
have shown that novices and experts can be accurately classified, it is not clear whether machine
learning can separate highly proficient surgeons from one another, especially without video data.
In this study, we explore the possibility of using only kinematic data to predict surgeons of similar
skill levels. We focus on a new dataset created from surgical exercises on a simulation device for
skill training. A simple, efficient encoding scheme was devised to encode kinematic sequences
so that they were amenable to edge learning. We report that it is possible to identify surgical fellows
receiving near perfect scores in the simulation exercises based on their motion characteristics
alone. Further, our model could be converted to a spiking neural network to train and infer on the
Nengo simulation framework with no loss in accuracy. Overall, this study suggests that building
neuromorphic models from sparse motion features may be a potentially useful strategy for identifying
surgeons and gestures with chips deployed on robotic systems to offer adaptive assistance during
surgery and training with additional latency and privacy benefits. 