Background: Despite recent significant progress in the development of automatic sleep staging
methods, building a good model still remains a big challenge for sleep studies with a small cohort
due to the data-variability and data-inefficiency issues. This work presents a deep transfer learning
approach to overcome these issues and enable transferring knowledge from a large dataset to a small
cohort for automatic sleep staging. Methods: We start from a generic end-to-end deep learning framework
for sequence-to-sequence sleep staging and derive two networks as the means for transfer learning.
The networks are first trained in the source domain (i.e. the large database). The pretrained networks
are then finetuned in the target domain (i.e. the small cohort) to complete knowledge transfer.
We employ the Montreal Archive of Sleep Studies (MASS) database consisting of 200 subjects as the
source domain and study deep transfer learning on three different target domains: the Sleep Cassette
subset and the Sleep Telemetry subset of the Sleep-EDF Expanded database, and the Surrey-cEEGrid
database. The target domains are purposely adopted to cover different degrees of data mismatch
to the source domains. Results: Our experimental results show significant performance improvement
on automatic sleep staging on the target domains achieved with the proposed deep transfer learning
approach. Conclusions: These results suggest the efficacy of the proposed approach in addressing
the above-mentioned data-variability and data-inefficiency issues. Significance: As a consequence,
it would enable one to improve the quality of automatic sleep staging models when the amount of data
is relatively small. The source code and the pretrained models are available at this http URL 