In an era when big data is becoming the norm, there is less concern with the quantity but more with the
quality and completeness of data. In many disciplines, data are collected from heterogeneous sources,
leading to multi-view or multi-modal datasets. The missing data problem has been challenging to
address in multi-view data analysis. Especially, when certain samples miss an entire view of data,
it creates the missing view problem. Classic multiple imputation or matrix completion methods
are hardly effective here when no information can be based on to impute data for such samples in the
specific view. The commonly-used simple method of removing samples with the missing view issue
can dramatically reduce sample size, diminishing the statistical power of an analysis. In this
paper, we propose a novel approach for view imputation via generative adversarial networks (GANs),
which we name as VIGAN. This approach first treats each view as a separate domain and identifies domain-to-domain
mappings through a GAN using randomly-sampled data from each view, and then employs a multi-modal
denoising autoencoder (DAE) to reconstruct the missing view from the GAN outputs based on paired
data from each view. Then, by optimizing the GANs and DAE jointly, our model enables the knowledge
integration learned for domain mappings and view correspondences to effectively recover the missing
view. Empirical results on benchmark datasets validate the VIGAN approach by comparing against
the state of the art, and an evaluation of VIGAN in a genetic study of substance use disorders further
proves the effectiveness and utility of this approach in life science. 