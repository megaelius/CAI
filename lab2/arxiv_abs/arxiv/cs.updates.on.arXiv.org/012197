Many large scale problems in computational fluid dynamics such as uncertainty quantification,
Bayesian inversion, data assimilation and PDE constrained optimization are considered very challenging
computationally as they require a large number of expensive (forward) numerical solutions of the
corresponding PDEs. We propose a machine learning algorithm, based on deep artificial neural networks,
that predicts the underlying \emph{input parameters to observable} map from a few training samples
(computed realizations of this map). By a judicious combination of theoretical arguments and empirical
observations, we find suitable network architectures and training hyperparameters that result
in robust and efficient neural network approximations of the parameters to observable map. Numerical
experiments are presented to demonstrate low prediction errors for the trained network networks,
even when the network has been trained with a few samples, at a computational cost which is several
orders of magnitude lower than the underlying PDE solver. Moreover, we combine the proposed deep
learning algorithm with Monte Carlo (MC) and Quasi-Monte Carlo (QMC) methods to efficiently compute
uncertainty propagation for nonlinear PDEs. Under the assumption that the underlying neural networks
generalize well, we prove that the deep learning MC and QMC algorithms are guaranteed to be faster
than the baseline (quasi-) Monte Carlo methods. Numerical experiments demonstrating one to two
orders of magnitude speed up over baseline QMC and MC algorithms, for the intricate problem of computing
probability distributions of the observable, are also presented. 