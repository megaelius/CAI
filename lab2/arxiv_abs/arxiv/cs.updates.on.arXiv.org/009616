The presumed data owners' right to explanations brought about by the General Data Protection Regulation
in Europe has shed light on the social challenges of explainable artificial intelligence (XAI).
In this paper, we present a case study with Deep Learning (DL) experts from a research and development
laboratory focused on the delivery of industrial-strength AI technologies. Our aim was to investigate
the social meaning (i.e. meaning to others) that DL experts assign to what they do, given a richly
contextualized and familiar domain of application. Using qualitative research techniques to
collect and analyze empirical data, our study has shown that participating DL experts did not spontaneously
engage into considerations about the social meaning of machine learning models that they build.
Moreover, when explicitly stimulated to do so, these experts expressed expectations that, with
real-world DL application, there will be available mediators to bridge the gap between technical
meanings that drive DL work, and social meanings that AI technology users assign to it. We concluded
that current research incentives and values guiding the participants' scientific interests and
conduct are at odds with those required to face some of the scientific challenges involved in advancing
XAI, and thus responding to the alleged data owners' right to explanations or similar societal demands
emerging from current debates. As a concrete contribution to mitigate what seems to be a more general
problem, we propose three preliminary XAI Mediation Challenges with the potential to bring together
technical and social meanings of DL applications, as well as to foster much needed interdisciplinary
collaboration among AI and the Social Sciences researchers. 