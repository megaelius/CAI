The ability to generate complex and realistic human body animations at scale, while following specific
artistic constraints, has been a fundamental goal for the game and animation industry for decades.
Popular techniques include key-framing, physics-based simulation, and database methods via
motion graphs. Recently, motion generators based on deep learning have been introduced. Although
these learning models can automatically generate highly intricate stylized motions of arbitrary
length, they still lack user control. To this end, we introduce the problem of long-term inbetweening,
which involves automatically synthesizing complex motions over a long time interval given very
sparse keyframes by users. We identify a number of challenges related to this problem, including
maintaining biomechanical and keyframe constraints, preserving natural motions, and designing
the entire motion sequence holistically while considering all constraints. We introduce a biomechanically
constrained generative adversarial network that performs long-term inbetweening of human motions,
conditioned on keyframe constraints. This network uses a novel two-stage approach where it first
predicts local motion in the form of joint angles, and then predicts global motion, i.e. the global
path that the character follows. Since there are typically a number of possible motions that could
satisfy the given user constraints, we also enable our network to generate a variety of outputs with
a scheme that we call Motion DNA. This approach allows the user to manipulate and influence the output
content by feeding seed motions (DNA) to the network. Trained with 79 classes of captured motion
data, our network performs robustly on a variety of highly complex motion styles. 