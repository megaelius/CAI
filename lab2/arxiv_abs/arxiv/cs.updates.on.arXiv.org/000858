Maximum likelihood is the most widely used statistical estimation technique. Recent work by the
authors introduced a general methodology for the construction of estimators for functionals in
parametric models, and demonstrated improvements - both in theory and in practice - over the maximum
likelihood estimator (MLE), particularly in high dimensional scenarios involving parameter
dimension comparable to or larger than the number of samples. This approach to estimation, building
on results from approximation theory, is shown to yield minimax rate-optimal estimators for a wide
class of functionals, implementable with modest computational requirements. In a nutshell, a
message of this recent work is that, for a wide class of functionals, the performance of these essentially
optimal estimators with $n$ samples is comparable to that of the MLE with $n \ln n$ samples. In the
present paper, we highlight the applicability of the aforementioned methodology to statistical
problems beyond functional estimation, and show that it can yield substantial gains. For example,
we demonstrate that for learning tree-structured graphical models, our approach achieves a significant
reduction of the required data size compared with the classical Chow--Liu algorithm, which is an
implementation of the MLE, to achieve the same accuracy. The key step in improving the Chow--Liu
algorithm is to replace the empirical mutual information with the estimator for mutual information
proposed by the authors. Further, applying the same replacement approach to classical Bayesian
network classification, the resulting classifiers uniformly outperform the previous classifiers
on 26 widely used datasets. 