Recent research efforts on adversarial ML have investigated problem-space attacks, focusing
on the generation of real evasive objects in domains where, unlike images, there is no clear inverse
mapping to the feature space (e.g., software). However, the design, comparison, and real-world
implications of problem-space attacks remain underexplored. This paper makes two major contributions.
First, we propose a novel formalization for adversarial ML evasion attacks in the problem-space,
which includes the definition of a comprehensive set of constraints on available transformations,
preserved semantics, robustness to preprocessing, and plausibility. We shed light on the relationship
between feature space and problem space, and we introduce the concept of side-effect features as
the byproduct of the inverse feature-mapping problem. This enables us to define and prove necessary
and sufficient conditions for the existence of problem-space attacks. We further demonstrate
the expressive power of our formalization by using it to describe several attacks from related literature
across different domains. Second, building on our formalization, we propose a novel problem-space
attack on Android malware that overcomes past limitations. Experiments on a dataset with 170K Android
apps from 2017 and 2018 show the practical feasibility of evading a state-of-the-art malware classifier
along with its hardened version. Our results demonstrate that "adversarial-malware as a service"
is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial
applications at scale, where on average it takes only a few minutes to generate an adversarial app.
Our formalization of problem-space attacks paves the way to more principled research in this domain.
