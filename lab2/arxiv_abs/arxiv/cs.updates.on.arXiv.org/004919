Despite huge success of artificial intelligence, hardware systems running these algorithms consume
orders of magnitude higher energy compared to the human brain, mainly due to heavy data movements
between the memory unit and the computation cores. Spiking neural networks (SNNs) built using bio-plausible
neuron and synaptic models have emerged as the power-efficient choice for designing cognitive
applications. These algorithms involve several lookup-table (LUT) based function evaluations
such as high-order polynomials and transcendental functions for solving complex neuro-synaptic
models, that typically require additional storage. To that effect, we propose `SPARE' - an in-memory,
distributed processing architecture built on ROM-embedded RAM technology, for accelerating
SNNs. ROM-embedded RAMs allow storage of LUTs, embedded within a typical memory array, without
additional area overhead. Our proposed architecture consists of a 2-D array of Processing Elements
(PEs). Since most of the computations are done locally within each PE, unnecessary data transfers
are restricted, thereby alleviating the von-Neumann bottleneck. We evaluate SPARE for two different
ROM-Embedded RAM structures - CMOS based ROM-Embedded SRAMs (R-SRAMs) and STT-MRAM based ROM-Embedded
MRAMs (R-MRAMs). Moreover, we analyze trade-offs in terms of energy, area and performance, for
using the two technologies on a range of image classification benchmarks. Furthermore, we leverage
the additional storage density to implement complex neuro-synaptic functionalities. This enhances
the utility of the proposed architecture by provisioning implementation of any neuron/synaptic
behavior as necessitated by the application. Our results show up-to 1.75x, 1.95x and 1.95x improvement
in energy, iso-storage area, and iso-area performance, respectively, by using neural network
accelerators built on ROM-embedded RAM primitives. 