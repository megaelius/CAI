This paper tackles a new problem setting: reinforcement learning with pixel-wise rewards (pixelRL)
for image processing. After the introduction of the deep Q-network, deep RL has been achieving great
success. However, the applications of deep reinforcement learning (RL) for image processing are
still limited. Therefore, we extend deep RL to pixelRL for various image processing applications.
In pixelRL, each pixel has an agent, and the agent changes the pixel value by taking an action. We also
propose an effective learning method for pixelRL that significantly improves the performance
by considering not only the future states of the own pixel but also those of the neighbor pixels. The
proposed method can be applied to some image processing tasks that require pixel-wise manipulations,
where deep RL has never been applied. Besides, it is possible to visualize what kind of operation
is employed for each pixel at each iteration, which would help us understand why and how such an operation
is chosen. We also believe that our technology can enhance the explainability and interpretability
of the deep neural networks. In addition, because the operations executed at each pixels are visualized,
we can change or modify the operations if necessary. We apply the proposed method to a variety of image
processing tasks: image denoising, image restoration, local color enhancement, and saliency-driven
image editing. Our experimental results demonstrate that the proposed method achieves comparable
or better performance, compared with the state-of-the-art methods based on supervised learning.
The source code is available on https://github.com/rfuruta/pixelRL. 