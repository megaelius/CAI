Probabilistic models are a critical part of the modern deep learning toolbox - ranging from generative
models (VAEs, GANs), sequence to sequence models used in machine translation and speech processing
to models over functional spaces (conditional neural processes, neural processes). Given the
size and complexity of these models, safely deploying them in applications requires the development
of tools to analyze their behavior rigorously and provide some guarantees that these models are
consistent with a list of desirable properties or specifications. For example, a machine translation
model should produce semantically equivalent outputs for innocuous changes in the input to the
model. A functional regression model that is learning a distribution over monotonic functions
should predict a larger value at a larger input. Verification of these properties requires a new
framework that goes beyond notions of verification studied in deterministic feedforward networks,
since requiring worst-case guarantees in probabilistic models is likely to produce conservative
or vacuous results. We propose a novel formulation of verification for deep probabilistic models
that take in conditioning inputs and sample latent variables in the course of producing an output:
We require that the output of the model satisfies a linear constraint with high probability over
the sampling of latent variables and for every choice of conditioning input to the model. We show
that rigorous lower bounds on the probability that the constraint is satisfied can be obtained efficiently.
Experiments with neural processes show that several properties of interest while modeling functional
spaces can be modeled within this framework (monotonicity, convexity) and verified efficiently
using our algorithms 