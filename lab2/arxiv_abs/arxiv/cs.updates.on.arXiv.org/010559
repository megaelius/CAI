As machine learning (ML) becomes more and more powerful and easily accessible, attackers increasingly
leverage ML to perform automated large-scale inference attacks in various domains. In such an ML-equipped
inference attack, an attacker has access to some data (called public data) of an individual, a software,
or a system; and the attacker uses an ML classifier to automatically infer their private data. Inference
attacks pose severe privacy and security threats to individuals and systems. Inference attacks
are successful because private data are statistically correlated with public data, and ML classifiers
can capture such statistical correlations. In this chapter, we discuss the opportunities and challenges
of defending against ML-equipped inference attacks via adversarial examples. Our key observation
is that attackers rely on ML classifiers in inference attacks. The adversarial machine learning
community has demonstrated that ML classifiers have various vulnerabilities. Therefore, we can
turn the vulnerabilities of ML into defenses against inference attacks. For example, ML classifiers
are vulnerable to adversarial examples, which add carefully crafted noise to normal examples such
that an ML classifier makes predictions for the examples as we desire. To defend against inference
attacks, we can add carefully crafted noise into the public data to turn them into adversarial examples,
such that attackers' classifiers make incorrect predictions for the private data. However, existing
methods to construct adversarial examples are insufficient because they did not consider the unique
challenges and requirements for the crafted noise at defending against inference attacks. In this
chapter, we take defending against inference attacks in online social networks as an example to
illustrate the opportunities and challenges. 