In this work we present a formal theoretical framework for assessing and analyzing two classes of
malevolent action towards generic Artificial Intelligence (AI) systems. Our results apply to
general multi-class classifiers that map from an input space into a decision space, including artificial
neural networks used in deep learning applications. Two classes of attacks are considered. The
first class involves adversarial examples and concerns the introduction of small perturbations
of the input data that cause misclassification. The second class, introduced here for the first
time and named stealth attacks, involves small perturbations to the AI system itself. Here the perturbed
system produces whatever output is desired by the attacker on a specific small data set, perhaps
even a single input, but performs as normal on a validation set (which is unknown to the attacker).
We show that in both cases, i.e., in the case of an attack based on adversarial examples and in the case
of a stealth attack, the dimensionality of the AI's decision-making space is a major contributor
to the AI's susceptibility. For attacks based on adversarial examples, a second crucial parameter
is the absence of local concentrations in the data probability distribution, a property known as
Smeared Absolute Continuity. According to our findings, robustness to adversarial examples requires
either (a) the data distributions in the AI's feature space to have concentrated probability density
functions or (b) the dimensionality of the AI's decision variables to be sufficiently small. We
also show how to construct stealth attacks on high-dimensional AI systems that are hard to spot unless
the validation set is made exponentially large. 