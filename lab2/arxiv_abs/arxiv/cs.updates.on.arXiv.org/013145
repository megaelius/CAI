Recent successes in training word embeddings for NLP tasks have encouraged a wave of research on
representation learning for source code, which builds on similar NLP methods. The overall objective
is then to produce code embeddings that capture the maximum of program semantics. State-of-the-art
approaches invariably rely on a syntactic representation (i.e., raw lexical tokens, abstract
syntax trees, or intermediate representation tokens) to generate embeddings, which are criticized
in the literature as non-robust or non-generalizable. In this work, we investigate a novel embedding
approach based on the intuition that source code has visual patterns of semantics. We further use
these patterns to address the outstanding challenge of identifying semantic code clones. We propose
the WYSIWIM ("What You See Is What It Means") approach where visual representations of source code
are fed into powerful pre-trained image classification neural networks from the field of computer
vision to benefit from the practical advantages of transfer learning. We evaluate the proposed
embedding approach on two variations of the task of semantic code clone identification: code clone
detection (a binary classification problem), and code classification (a multi-classification
problem). We show with experiments on the BigCloneBench (Java) and Open Judge (C) datasets that
although simple, our WYSIWIM approach performs as effectively as state of the art approaches such
as ASTNN or TBCNN. We further explore the influence of different steps in our approach, such as the
choice of visual representations or the classification algorithm, to eventually discuss the promises
and limitations of this research direction. 