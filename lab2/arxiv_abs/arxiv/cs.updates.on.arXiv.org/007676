In this paper we contribute a novel algorithm family, which generalizes many unsupervised techniques
including unnormalized and energy models, and allows to infer different statistical modalities
(e.g.~data likelihood and ratio between densities) from data samples. The proposed unsupervised
technique Probabilistic Surface Optimization (PSO) views a neural network (NN) as a flexible surface
which can be pushed according to loss-specific virtual stochastic forces, where a dynamical equilibrium
is achieved when the point-wise forces on the surface become equal. Concretely, the surface is pushed
up and down at points sampled from two different distributions, with overall up and down forces becoming
functions of these two distribution densities and of force intensity magnitudes defined by loss
of a particular PSO instance. The eventual force equilibrium upon convergence enforces the NN to
be equal to various statistical functions depending on the used magnitude functions, such as data
density. Furthermore, this dynamical-statistical equilibrium is extremely intuitive and useful,
providing many implications and possible usages in probabilistic inference. Further, we provide
new PSO-based approaches as demonstration of PSO exceptional usability. We also analyze PSO convergence
and optimization stability, and relate them to the gradient similarity function over NN input space.
Further, we propose new ways to improve the above stability. Finally, we present new instances of
PSO, termed PSO-LDE, for data density estimation on logarithmic scale and also provide a new NN block-diagonal
architecture for increased surface flexibility, which significantly improves estimation accuracy.
Both PSO-LDE and the new architecture are combined together as a new density estimation technique.
In our experiments we demonstrate this technique to produce highly accurate density estimation
for 20D data. 