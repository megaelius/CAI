In many scenarios in computer vision, machine learning, and computer graphics, there is a requirement
to learn the mapping from an image of one domain to an image of another domain, called Image-to-image
translation. For example, style transfer, object transfiguration, visually altering the appearance
of weather conditions in an image, changing the appearance of a day image into a night image or vice
versa, photo enhancement, to name a few. In this paper, we propose two machine learning techniques
to solve the embroidery image-to-image translation. Our goal is to generate a preview image which
looks similar to an embroidered image, from a user-uploaded image. Our techniques are modifications
of two existing techniques, neural style transfer, and cycle-consistent generative-adversarial
network. Neural style transfer renders the semantic content of an image from one domain in the style
of a different image in another domain, whereas a cycle-consistent generative adversarial network
learns the mapping from an input image to output image without any paired training data, and also
learn a loss function to train this mapping. Furthermore, the techniques we propose are independent
of any embroidery attributes, such as elevation of the image, light-source, start, and endpoints
of a stitch, type of stitch used, fabric type, etc. Given the user image, our techniques can generate
a preview image which looks similar to an embroidered image. We train and test our propose techniques
on an embroidery dataset which consist of simple 2D images. To do so, we prepare an unpaired embroidery
dataset with more than 8000 user-uploaded images along with embroidered images. Empirical results
show that these techniques successfully generate an approximate preview of an embroidered version
of a user image, which can help users in decision making. 