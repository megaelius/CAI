Most state-of-the-art semantic segmentation or scene parsing approaches only achieve high accuracy
rates in good environmental conditions. The performance decrease enormously if images with unknown
disturbances occur, which is less discussed but appears more in real applications. Most existing
research works cast the handling of the challenging adverse conditions as a post-processing step
of signal restoration or enhancement after sensing, then feed the restored images for visual understanding.
However, the performance will largely depend on the quality of restoration or enhancement. Whether
restoration-based approaches would actually boost the semantic segmentation performance remains
questionable. In this paper, we propose a novel net framework to tackle semantic Segmentation and
image Restoration in adverse environmental conditions (SR-Net). The proposed approach contains
two components: Semantically-Guided Adaptation, which exploits and leverages semantic information
from degraded images then help to refine the segmentation; and Exemplar-Guided Synthesis, which
synthesizes restored or enhanced images from semantic label maps given specific degraded exemplars.
SR-Net exploits the possibility of building connections of low-level image processing and high
level computer vision tasks, achieving image restoration via segmentation refinement. Extensive
experiments on several datasets demonstrate that our approach can not only improve the accuracy
of high-level vision tasks with image adaption, but also boosts the perceptual quality and structural
similarity of degraded images with image semantic guidance. 