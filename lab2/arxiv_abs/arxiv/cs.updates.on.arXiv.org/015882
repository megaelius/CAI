A collaborative task is assigned to a multiagent system (MAS) in which agents are allowed to communicate.
The MAS runs over an underlying Markov decision process and its task is to maximize the averaged sum
of discounted one-stage rewards. Although knowing the global state of the environment is necessary
for the optimal action selection of the MAS, agents are limited to individual observations. The
inter-agent communication can tackle the issue of local observability, however, the limited rate
of the inter-agent communication prevents the agent from acquiring the precise global state information.
To overcome this challenge, agents need to communicate their observations in a compact way such
that the MAS compromises the minimum possible sum of rewards. We show that this problem is equivalent
to a form of rate-distortion problem which we call the task-based information compression. We introduce
two schemes for task-based information compression (i) Learning-based information compression
(LBIC) which leverages reinforcement learning to compactly represent the observation space of
the agents, and (ii) State aggregation for information compression (SAIC), for which a state aggregation
algorithm is analytically designed. The SAIC is shown, conditionally, to be capable of achieving
the optimal performance in terms of the attained sum of discounted rewards. The proposed algorithms
are applied to a rendezvous problem and their performance is compared with two benchmarks; (i) conventional
source coding algorithms and the (ii) centralized multiagent control using reinforcement learning.
Numerical experiments confirm the superiority of the proposed algorithms. 