Training a model to perform a task typically requires a large amount of data from the domains in which
the task will be applied. However, it is often the case that data are abundant in some domains but scarce
in others. Domain adaptation deals with the challenge of adapting a model trained from a data-rich
source domain to perform well in a data-poor target domain. In general, this requires learning plausible
mappings between domains. CycleGAN is a powerful framework that efficiently learns to map inputs
from one domain to another using adversarial training and a cycle-consistency constraint. However,
the conventional approach of enforcing cycle-consistency via reconstruction may be overly restrictive
in cases where one or more domains have limited training data. In this paper, we propose an augmented
cyclic adversarial learning model that enforces the cycle-consistency constraint through an
external task specific model, which encourages the preservation of task-relevant content as opposed
to exact reconstruction. This task specific model both relaxes the cycle-consistency constraint
and complements the role of the discriminator during training, serving as an augmented information
source for learning the mapping. In the experiment, we adopt a speech recognition model from each
domain as the task specific model. Our approach improves absolute performance of speech recognition
by $2\%$ for female speakers in the TIMIT dataset, where the majority of training samples are from
male voices. We also explore digit classification with MNIST and SVHN in a low-resource setting
and show that our approach improves absolute performance by $14\%$ and $4\%$ when adapting SVHN
to MNIST and vice versa, respectively. Our approach also outperforms unsupervised domain adaptation
methods, which require high-resource unlabeled target domain. 