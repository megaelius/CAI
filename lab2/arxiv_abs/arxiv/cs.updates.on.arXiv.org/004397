Visual repetition is ubiquitous in our world. It appears in human activity (sports, cooking), animal
behavior (a bee's waggle dance), natural phenomena (leaves in the wind) and in urban environments
(flashing lights). Estimating visual repetition from realistic video is challenging as periodic
motion is rarely perfectly static and stationary. To better deal with realistic video, we elevate
the static and stationary assumptions often made by existing work. Our spatiotemporal filtering
approach, established on the theory of periodic motion, effectively handles a wide variety of appearances
and requires no learning. Starting from motion in 3D we derive three periodic motion types by decomposition
of the motion field into its fundamental components. In addition, three temporal motion continuities
emerge from the field's temporal dynamics. For the 2D perception of 3D motion we consider the viewpoint
relative to the motion; what follows are 18 cases of recurrent motion perception. To estimate repetition
under all circumstances, our theory implies constructing a mixture of differential motion maps:
gradient, divergence and curl. We temporally convolve the motion maps with wavelet filters to estimate
repetitive dynamics. Our method is able to spatially segment repetitive motion directly from the
temporal filter responses densely computed over the motion maps. For experimental verification
of our claims, we use our novel dataset for repetition estimation, better-reflecting reality with
non-static and non-stationary repetitive motion. On the task of repetition counting, we obtain
favorable results compared to a deep learning alternative. 