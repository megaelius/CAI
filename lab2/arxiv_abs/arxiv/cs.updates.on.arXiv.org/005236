On-board estimation of the pose of an uncooperative target spacecraft is an essential task for future
on-orbit servicing and close-proximity formation flying missions. However, two issues hinder
reliable on-board monocular vision based pose estimation: robustness to illumination conditions
due to a lack of reliable visual features and scarcity of image datasets required for training and
benchmarking. To address these two issues, this work details the design and validation of a monocular
vision based pose determination architecture for spaceborne applications. The primary contribution
to the state-of-the-art of this work is the introduction of a novel pose determination method based
on Convolutional Neural Networks (CNN) to provide an initial guess of the pose in real-time on-board.
The method involves discretizing the pose space and training the CNN with images corresponding
to the resulting pose labels. Since reliable training of the CNN requires massive image datasets
and computational resources, the parameters of the CNN must be determined prior to the mission with
synthetic imagery. Moreover, reliable training of the CNN requires datasets that appropriately
account for noise, color, and illumination characteristics expected in orbit. Therefore, the
secondary contribution of this work is the introduction of an image synthesis pipeline, which is
tailored to generate high fidelity images of any spacecraft 3D model. The proposed technique is
scalable to spacecraft of different structural and physical properties as well as robust to the
dynamic illumination conditions of space. Through metrics measuring classification and pose
accuracy, it is shown that the presented architecture has desirable robustness and scalable properties.
