Advances in compressive sensing provided reconstruction algorithms of sparse signals from linear
measurements with optimal sample complexity, but natural extensions of this methodology to nonlinear
inverse problems have been met with potentially fundamental sample complexity bottlenecks. In
particular, tractable algorithms for compressive phase retrieval with sparsity priors have not
been able to achieve optimal sample complexity. This has created an open problem in compressive
phase retrieval: under generic, phaseless linear measurements, are there tractable reconstruction
algorithms that succeed with optimal sample complexity? Meanwhile, progress in machine learning
has led to the development of new data-driven signal priors in the form of generative models, which
can outperform sparsity priors with significantly fewer measurements. In this work, we resolve
the open problem in compressive phase retrieval and demonstrate that generative priors can lead
to a fundamental advance by permitting optimal sample complexity by a tractable algorithm in this
challenging nonlinear inverse problem. We additionally provide empirics showing that exploiting
generative priors in phase retrieval can significantly outperform sparsity priors. These results
provide support for generative priors as a new paradigm for signal recovery in a variety of contexts,
both empirically and theoretically. The strengths of this paradigm are that (1) generative priors
can represent some classes of natural signals more concisely than sparsity priors, (2) generative
priors allow for direct optimization over the natural signal manifold, which is intractable under
sparsity priors, and (3) the resulting non-convex optimization problems with generative priors
can admit benign optimization landscapes at optimal sample complexity, perhaps surprisingly,
even in cases of nonlinear measurements. 