Recently there has been an increasing interest in the multivariate Gaussian process (MGP) which
extends the Gaussian process (GP) to deal with multiple outputs. One approach to construct the MGP
and account for non-trivial commonalities amongst outputs employs a convolution process (CP).
The CP is based on the idea of sharing latent functions across several convolutions. Despite the
elegance of the CP construction, it provides new challenges that need yet to be tackled. First, even
with a moderate number of outputs, model building is extremely prohibitive due to the huge increase
in computational demands and number of parameters to be estimated. Second, the negative transfer
of knowledge may occur when some outputs do not share commonalities. In this paper we address these
issues. We propose a regularized pairwise modeling approach for the MGP established using CP. The
key feature of our approach is to distribute the estimation of the full multivariate model into a
group of bivariate GPs which are individually built. Interestingly pairwise modeling turns out
to possess unique characteristics, which allows us to tackle the challenge of negative transfer
through penalizing the latent function that facilitates information sharing in each bivariate
model. Predictions are then made through combining predictions from the bivariate models within
a Bayesian framework. The proposed method has excellent scalability when the number of outputs
is large and minimizes the negative transfer of knowledge between uncorrelated outputs. Statistical
guarantees for the proposed method are studied and its advantageous features are demonstrated
through numerical studies. 