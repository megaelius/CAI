Two different approaches to dealing with probabilistic knowledge are examined -models and inductive
inference. Examples of the first are: influence diagrams [1], Bayesian networks [2], log-linear
models [3, 4]. Examples of the second are: games-against nature [5, 6] varieties of maximum-entropy
methods [7, 8, 9], and the author's min-score induction [10]. In the modeling approach, the basic
issue is manageability, with respect to data elicitation and computation. Thus, it is assumed that
the pertinent set of users in some sense knows the relevant probabilities, and the problem is to format
that knowledge in a way that is convenient to input and store and that allows computation of the answers
to current questions in an expeditious fashion. The basic issue for the inductive approach appears
at first sight to be very different. In this approach it is presumed that the relevant probabilities
are only partially known, and the problem is to extend that incomplete information in a reasonable
way to answer current questions. Clearly, this approach requires that some form of induction be
invoked. Of course, manageability is an important additional concern. Despite their seeming differences,
the two approaches have a fair amount in common, especially with respect to the structural framework
they employ. Roughly speaking, this framework involves identifying clusters of variables which
strongly interact, establishing marginal probability distributions on the clusters, and extending
the subdistributions to a more complete distribution, usually via a product formalism. The product
extension is justified on the modeling approach in terms of assumed conditional independence;
in the inductive approach the product form arises from an inductive rule. 