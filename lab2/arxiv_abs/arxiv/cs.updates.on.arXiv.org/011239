While reinforcement learning provides an appealing formalism for learning individual skills,
a general-purpose robotic system must be able to master an extensive repertoire of behaviors. Instead
of learning a large collection of skills individually, can we instead enable a robot to propose and
practice its own behaviors automatically, learning about the affordances and behaviors that it
can perform in its environment, such that it can then repurpose this knowledge once a new task is commanded
by the user? In this paper, we study this question in the context of self-supervised goal-conditioned
reinforcement learning. A central challenge in this learning regime is the problem of goal setting:
in order to practice useful skills, the robot must be able to autonomously set goals that are feasible
but diverse. When the robot's environment and available objects vary, as they do in most open-world
settings, the robot must propose to itself only those goals that it can accomplish in its present
setting with the objects that are at hand. Previous work only studies self-supervised goal-conditioned
RL in a single-environment setting, where goal proposals come from the robot's past experience
or a generative model are sufficient. In more diverse settings, this frequently leads to impossible
goals and, as we show experimentally, prevents effective learning. We propose a conditional goal-setting
model that aims to propose goals that are feasible from the robot's current state. We demonstrate
that this enables self-supervised goal-conditioned off-policy learning with raw image observations
in the real world, enabling a robot to manipulate a variety of objects and generalize to new objects
that were not seen during training. 