We investigate the properties of a Block Decomposition Method (BDM), which extends the power of
a Coding Theorem Method (CTM) that approximates local estimations of algorithmic complexity based
upon Solomonoff-Levin's theory of algorithmic probability providing a closer connection to algorithmic
complexity than previous attempts based on statistical regularities e.g. as spotted by some popular
lossless compression schemes. The strategy behind BDM is to find small computer programs that produce
the components of a larger, decomposed object. The set of short computer programs can then be artfully
arranged in sequence so as to produce the original object and to estimate an upper bound on the length
of the shortest computer program that produces said original object. We show that the method provides
efficient estimations of algorithmic complexity but that it performs like Shannon entropy when
it loses accuracy. We estimate errors and study the behaviour of BDM for different boundary conditions,
all of which are compared and assessed in detail. The measure may be adapted for use with more multi-dimensional
objects than strings, objects such as arrays and tensors. To test the measure we demonstrate the
power of CTM on low algorithmic-randomness objects that are assigned maximal entropy (e.g. $\pi$)
but whose numerical approximations are closer to the theoretical low algorithmic-randomness
expectation. We also test the measure on larger objects including dual, isomorphic and cospectral
graphs for which we know that algorithmic randomness is low. We also release implementations of
the methods in most major programming languages---Wolfram Language (Mathematica), Matlab, R,
Perl, Python, Pascal, C++, and Haskell---and a free online algorithmic complexity calculator.
