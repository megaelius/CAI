Understanding the 3D geometric structure of the Earth's surface has been an active research topic
in photogrammetry and remote sensing community for decades, serving as an essential building block
for various applications such as 3D digital city modeling, change detection, and city management.
Previous researches have extensively studied the problem of height estimation from aerial images
based on stereo or multi-view image matching. These methods require two or more images from different
perspectives to reconstruct 3D coordinates with camera information provided. In this paper, we
deal with the ambiguous and unsolved problem of height estimation from a single aerial image. Driven
by the great success of deep learning, especially deep convolution neural networks (CNNs), some
researches have proposed to estimate height information from a single aerial image by training
a deep CNN model with large-scale annotated datasets. These methods treat height estimation as
a regression problem and directly use an encoder-decoder network to regress the height values.
In this paper, we proposed to divide height values into spacing-increasing intervals and transform
the regression problem into an ordinal regression problem, using an ordinal loss for network training.
To enable multi-scale feature extraction, we further incorporate an Atrous Spatial Pyramid Pooling
(ASPP) module to extract features from multiple dilated convolution layers. After that, a post-processing
technique is designed to transform the predicted height map of each patch into a seamless height
map. Finally, we conduct extensive experiments on ISPRS Vaihingen and Potsdam datasets. Experimental
results demonstrate significantly better performance of our method compared to the state-of-the-art
methods. 