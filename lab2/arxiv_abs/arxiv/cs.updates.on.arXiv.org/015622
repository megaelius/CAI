Traditional von Neumann architecture based processors become inefficient in terms of energy and
throughput as they involve separate processing and memory units, also known as~\textit{memory
wall}. The memory wall problem is further exacerbated when massive parallelism and frequent data
movement are required between processing and memory units for real-time implementation of artificial
neural network (ANN) that enables many intelligent applications. One of the most promising approach
to address the memory wall problem is to carry out computations inside the memory core itself that
enhances the memory bandwidth and energy efficiency for extensive computations. This paper presents
an in-memory computing architecture for ANN enabling artificial intelligence (AI) and machine
learning (ML) applications. The proposed architecture utilizes deep in-memory architecture
based on standard six transistor (6T) static random access memory (SRAM) core for the implementation
of a multi-layered perceptron. Our novel on-chip training and inference in-memory architecture
reduces energy cost and enhances throughput by simultaneously accessing the multiple rows of SRAM
array per precharge cycle and eliminating the frequent access of data. The proposed architecture
realizes backpropagation which is the keystone during the network training using newly proposed
different building blocks such as weight updation, analog multiplication, error calculation,
signed analog to digital conversion, and other necessary signal control units. The proposed architecture
was trained and tested on the IRIS dataset which exhibits $\approx46\times$ more energy efficient
per MAC (multiply and accumulate) operation compared to earlier classifiers. 