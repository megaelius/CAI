Federated learning is a technique that enables distributed clients to collaboratively learn a
shared machine learning model while keeping their training data localized. This reduces data privacy
risks, however, privacy concerns still exist since it is possible to leak information about the
training dataset from the trained model's weights or parameters. Setting up a federated learning
environment, especially with security and privacy guarantees, is a time-consuming process with
numerous configurations and parameters that can be manipulated. In order to help clients ensure
that collaboration is feasible and to check that it improves their model accuracy, a real-world
simulator for privacy-preserving and secure federated learning is required. In this paper, we
introduce PrivacyFL, which is an extensible, easily configurable and scalable simulator for federated
learning environments. Its key features include latency simulation, robustness to client departure,
support for both centralized and decentralized learning, and configurable privacy and security
mechanisms based on differential privacy and secure multiparty computation. In this paper, we
motivate our research, describe the architecture of the simulator and associated protocols, and
discuss its evaluation in numerous scenarios that highlight its wide range of functionality and
its advantages. Our paper addresses a significant real-world problem: checking the feasibility
of participating in a federated learning environment under a variety of circumstances. It also
has a strong practical impact because organizations such as hospitals, banks, and research institutes,
which have large amounts of sensitive data and would like to collaborate, would greatly benefit
from having a system that enables them to do so in a privacy-preserving and secure manner. 