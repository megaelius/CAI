Advances in deep learning (DL) have resulted in impressive accuracy in some medical image classification
tasks, but often deep models lack interpretability. The ability of these models to explain their
decisions is important for fostering clinical trust and facilitating clinical translation. Furthermore,
for many problems in medicine there is a wealth of existing clinical knowledge to draw upon, which
may be useful in generating explanations, but it is not obvious how this knowledge can be encoded
into DL models - most models are learnt either from scratch or using transfer learning from a different
domain. In this paper we address both of these issues. We propose a novel DL framework for image-based
classification based on a variational autoencoder (VAE). The framework allows prediction of the
output of interest from the latent space of the autoencoder, as well as visualisation (in the image
domain) of the effects of crossing the decision boundary, thus enhancing the interpretability
of the classifier. Our key contribution is that the VAE disentangles the latent space based on `explanations'
drawn from existing clinical knowledge. The framework can predict outputs as well as explanations
for these outputs, and also raises the possibility of discovering new biomarkers that are separate
(or disentangled) from the existing knowledge. We demonstrate our framework on the problem of predicting
response of patients with cardiomyopathy to cardiac resynchronization therapy (CRT) from cine
cardiac magnetic resonance images. The sensitivity and specificity of the proposed model on the
task of CRT response prediction are 88.43% and 84.39% respectively, and we showcase the potential
of our model in enhancing understanding of the factors contributing to CRT response. 