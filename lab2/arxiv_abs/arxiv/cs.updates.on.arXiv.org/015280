We give the first outlier-robust efficient algorithm for clustering a mixture of k statistically
separated d-dimensional Gaussians (k-GMMs). Concretely, our algorithm takes input an $\epsilon$-corrupted
sample from a k-GMM and outputs an approximate clustering that misclassifies at most $O(k\epsilon)+\eta$
fraction of the points whenever every pair of components is separated by $1-\exp(-\textrm{poly}(k/\eta))$
in total variation distance. This is the statistically weakest possible notion of separation and
allows, for e.g., clustering of mixtures with components with the same mean with covariances differing
in a single unknown direction or separated in Frobenius distance. The running time of our algorithm
is $d^{O(\log(\kappa)) \textrm{poly}(k/\eta)}$ where $\kappa$ is a measure of spread of the mixture
in any direction. For k=2, our algorithms run in time and samples $\textrm{poly}(d)$ with no dependence
on the spread $\kappa$. Such a results were not known prior to our work, even for k=2. More generally,
our algorithm succeeds for mixtures of any distribution that satisfies two well-studied analytic
assumptions - certifiable hypercontractivity and anti-concentration. Thus, they extend to clustering
mixtures of arbitrary affine transforms of the uniform distribution on the $d$-dimensional unit
sphere. Even the information theoretic clusterability of distributions satisfying our analytic
assumptions was not known and is likely to be of independent interest. Our algorithms build on the
recent flurry of work relying on certifiable anti-concentration, first introduced in [KKK'19,
RY'20]. Our techniques expand the sum-of-squares toolkit to show robust certifiability of TV-separated
Gaussian clusters in data. This involves a low-degree sum-of-squares proof of statements that
relate parameter distance to total variation distance simply relying on hypercontractivity and
anti-concentration. 