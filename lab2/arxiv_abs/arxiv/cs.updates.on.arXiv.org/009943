We study the robust one-bit compressed sensing problem whose goal is to design an algorithm that
faithfully recovers any sparse target vector $\theta_0\in\mathbb{R}^d$ uniformly $m$ quantized
noisy measurements. Under the assumption that the measurements are sub-Gaussian random vectors,
to recover any $k$-sparse $\theta_0$ ($k\ll d$) uniformly up to an error $\varepsilon$ with high
probability, the best known computationally tractable algorithm requires $m\geq\tilde{\mathcal{O}}(k\log
d/\varepsilon^4)$ measurements. In this paper, we consider a new framework for the one-bit sensing
problem where the sparsity is implicitly enforced via mapping a low dimensional representation
$x_0 \in \mathbb{R}^k$ through a known $n$-layer ReLU generative network $G:\mathbb{R}^k\rightarrow\mathbb{R}^d$.
Such a framework poses low-dimensional priors on $\theta_0$ without a known basis. We propose to
recover the target $G(x_0)$ via an unconstrained empirical risk minimization (ERM) problem under
a much weaker sub-exponential measurement assumption. For such a problem, we establish a joint
statistical and computational analysis. In particular, we prove that the ERM estimator in this
new framework achieves an improved statistical rate of $m=\tilde{\mathcal{O}}(kn \log d /\varepsilon^2)$
recovering any $G(x_0)$ uniformly up to an error $\varepsilon$. Moreover, from the lens of computation,
despite non-convexity, we prove that the objective of our ERM problem has no spurious stationary
point, that is, any stationary point are equally good for recovering the true target up to scaling
with a certain accuracy. Furthermore, our analysis also shed lights on the possibility of inverting
a deep generative model under partial and quantized measurements, complementing the recent success
of using deep generative models for inverse problems. 