Graphs are widely used as a natural framework that captures interactions between individual elements
represented as nodes in a graph. In medical applications, specifically, nodes can represent individuals
within a potentially large population (patients or healthy controls) accompanied by a set of features,
while the graph edges incorporate associations between subjects in an intuitive manner. This representation
allows to incorporate the wealth of imaging and non-imaging information as well as individual subject
features simultaneously in disease classification tasks. Previous graph-based approaches for
supervised or unsupervised learning in the context of disease prediction solely focus on pairwise
similarities between subjects, disregarding individual characteristics and features, or rather
rely on subject-specific imaging feature vectors and fail to model interactions between them.
In this paper, we present a thorough evaluation of a generic framework that leverages both imaging
and non-imaging information and can be used for brain analysis in large populations. This framework
exploits Graph Convolutional Networks (GCNs) and involves representing populations as a sparse
graph, where its nodes are associated with imaging-based feature vectors, while phenotypic information
is integrated as edge weights. The extensive evaluation explores the effect of each individual
component of this framework on disease prediction performance and further compares it to different
baselines. The framework performance is tested on two large datasets with diverse underlying data,
ABIDE and ADNI, for the prediction of Autism Spectrum Disorder and conversion to Alzheimer's disease,
respectively. Our analysis shows that our novel framework can improve over state-of-the-art results
on both databases, with 70.4% classification accuracy for ABIDE and 80.0% for ADNI. 