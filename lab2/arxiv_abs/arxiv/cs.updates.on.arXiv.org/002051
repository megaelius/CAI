One of the major terminological forces driving ICT integration in research today is that of "big
data." While the phrase sounds inclusive and integrative, "big data" approaches are highly selective,
excluding input that cannot be effectively structured, represented, or digitised. Data of this
complex sort is precisely the kind that human activity produces, but the technological imperative
to enhance signal through the reduction of noise does not accommodate this richness. Data and the
computational approaches that facilitate "big data" have acquired a perceived objectivity that
belies their curated, malleable, reactive, and performative nature. In an input environment where
anything can "be data" once it is entered into the system as "data," data cleaning and processing,
together with the metadata and information architectures that structure and facilitate our cultural
archives acquire a capacity to delimit what data are. This engenders a process of simplification
that has major implications for the potential for future innovation within research environments
that depend on rich material yet are increasingly mediated by digital technologies. This paper
presents the preliminary findings of the European-funded KPLEX (Knowledge Complexity) project
which investigates the delimiting effect digital mediation and datafication has on rich, complex
cultural data. The paper presents a systematic review of existing implicit definitions of data,
elaborating on the implications of these definitions and highlighting the ways in which metadata
and computational technologies can restrict the interpretative potential of data. It sheds light
on the gap between analogue or augmented digital practices and fully computational ones, and the
strategies researchers have developed to deal with this gap. The paper proposes a reconceptualisation
of data as it is functionally employed within digitally-mediated research so as to incorporate
and acknowledge the richness and complexity of our source materials. 