This paper describes a procedure for the creation of large-scale video datasets for action classification
and localization from unconstrained, realistic web data. The scalability of the proposed procedure
is demonstrated by building a novel video benchmark, named SLAC (Sparsely Labeled ACtions), consisting
of over 520K untrimmed videos and 1.75M clip annotations spanning 200 action categories. Using
our proposed framework, annotating a clip takes merely 8.8 seconds on average. This represents
a saving in labeling time of over 95% compared to the traditional procedure of manual trimming and
localization of actions. Our approach dramatically reduces the amount of human labeling by automatically
identifying hard clips, i.e., clips that contain coherent actions but lead to prediction disagreement
between action classifiers. A human annotator can disambiguate whether such a clip truly contains
the hypothesized action in a handful of seconds, thus generating labels for highly informative
samples at little cost. We show that our large-scale dataset can be used to effectively pre-train
action recognition models, significantly improving final metrics on smaller-scale benchmarks
after fine-tuning. On Kinetics, UCF-101 and HMDB-51, models pre-trained on SLAC outperform baselines
trained from scratch, by 2.0%, 20.1% and 35.4% in top-1 accuracy, respectively when RGB input is
used. Furthermore, we introduce a simple procedure that leverages the sparse labels in SLAC to pre-train
action localization models. On THUMOS14 and ActivityNet-v1.3, our localization model improves
the mAP of baseline model by 8.6% and 2.5%, respectively. 