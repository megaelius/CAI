Deep Neural Networks (DNNs) have become a powerful tool for a wide range of problems. Yet recent work
has shown an increasing variety of adversarial samples that can fool them. Most existing detection
mechanisms impose significant costs, either by using additional classifiers to spot adversarial
samples, or by requiring the DNN to be restructured. In this paper, we introduce a novel defence.
We train our DNN so that, as long as it is working as intended on the kind of inputs we expect, its behavior
is constrained, in that a set of behaviors are taboo. If it is exposed to adversarial samples, they
will often cause a taboo behavior, which we can detect. As an analogy, we can imagine that we are teaching
our robot good manners; if it's ever rude, we know it's come under some bad influence. This defence
mechanism is very simple and, although it involves a modest increase in training, has almost zero
computation overhead at runtime -- making it particularly suitable for use in embedded systems.
Taboos can be both subtle and diverse. Just as humans' choice of language can convey a lot of information
about location, affiliation, class and much else that can be opaque to outsiders but that enables
members of the same group to recognise each other, so also taboo choice can encode and hide information.
We can use this to make adversarial attacks much harder. It is a well-established design principle
that the security of a system should not depend on the obscurity of its design, but of some variable
(the key) which can differ between implementations and be changed as necessary. We explain how taboos
can be used to equip a classifier with just such a key, and to tune the keying mechanism to adversaries
of various capabilities. We evaluate the performance of a prototype against a wide range of attacks
and show how our simple defense can work well in practice. 