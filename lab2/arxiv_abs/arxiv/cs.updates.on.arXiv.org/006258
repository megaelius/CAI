Assessing the quality of an evolving knowledge base is a challenging task as it often requires to
identify correct quality assessment procedures. Since data is often derived from autonomous,
and increasingly large data sources, it is impractical to manually curate the data, and challenging
to continuously and automatically assess their quality. In this paper, we explore two main areas
of quality assessment related to evolving knowledge bases: (i) identification of completeness
issues using knowledge base evolution analysis, and (ii) identification of consistency issues
based on integrity constraints, such as minimum and maximum cardinality, and range constraints.
For completeness analysis, we use data profiling information from consecutive knowledge base
releases to estimate completeness measures that allow predicting quality issues. Then, we perform
consistency checks to validate the results of the completeness analysis using integrity constraints
and learning models. The approach has been tested both quantitatively and qualitatively by using
a subset of datasets from both DBpedia and 3cixty knowledge bases. The performance of the approach
is evaluated using precision, recall, and F1 score. From completeness analysis, we observe a 94%
precision for the English DBpedia KB and 95% precision for the 3cixty Nice KB. We also assessed the
performance of our consistency analysis by using five learning models over three sub-tasks, namely
minimum cardinality, maximum cardinality, and range constraint. We observed that the best performing
model in our experimental setup is the Random Forest, reaching an F1 score greater than 90% for minimum
and maximum cardinality and 84% for range constraints. 