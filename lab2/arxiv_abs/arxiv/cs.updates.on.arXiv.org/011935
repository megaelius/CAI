Neural networks have become increasingly prevalent within the geosciences for applications ranging
from numerical model parameterizations to the prediction of extreme weather. A common limitation
of neural networks has been the lack of methods to interpret what the networks learn and how they make
decisions. As such, neural networks have typically been used within the geosciences to accurately
identify a desired output given a set of inputs, with the interpretation of what the network learns
being used - if used at all - as a secondary metric to ensure the network is making the right decision
for the right reason. Network interpretation techniques have become more advanced in recent years,
however, and we therefore propose that the ultimate objective of using a neural network can also
be the interpretation of what the network has learned rather than the output itself. We show that
the interpretation of a neural network can enable the discovery of scientifically meaningful connections
within geoscientific data. By training neural networks to use one or more components of the earth
system to identify another, interpretation methods can be used to gain scientific insights into
how and why the two components are related. In particular, we use two methods for neural network interpretation.
These methods project the decision pathways of a network back onto the original input dimensions,
and are called "optimal input" and layerwise relevance propagation (LRP). We then show how these
interpretation techniques can be used to reliably infer scientifically meaningful information
from neural networks by applying them to common climate patterns. These results suggest that combining
interpretable neural networks with novel scientific hypotheses will open the door to many new avenues
in neural network-related geoscience research. 