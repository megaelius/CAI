Automation, the ability to run processes without human supervision, is one of the most important
drivers of increased scalability and productivity. Modern automation largely relies on forms
of closed loop control, wherein a controller interacts with a controlled process via actions, based
on observations. Despite an increase in the use of machine learning for process control, most deployed
controllers still are linear Proportional-Integral-Derivative (PID) controllers. PID controllers
perform well on linear and near-linear systems but are not robust enough for more complex processes.
As a main contribution of this paper, we examine the utility of extending standard PID controllers
with General Dynamic Neural Networks (GDNN); we show that GDNN (neural) PID controllers perform
well on a range of control systems and highlight what is needed to make them a stable, scalable, and
interpretable option for control. To do so, we provide a comprehensive study using four different
benchmark processes. All control environments are evaluated with and without noise as well as with
and without disturbances. The neural PID controller performs better than standard PID control
in 15 of 16 tasks and better than model-based control in 13 of 16 tasks. As a second contribution of
this work, we address the Achilles heel that prevents neural networks from being used in real-world
control processes so far: lack of interpretability. We use bounded-input bounded-output stability
analysis to evaluate the parameters suggested by the neural network, thus making them understandable
for human engineers. This combination of rigorous evaluation paired with better explainability
is an important step towards the acceptance of neural-network-based control approaches for real-world
systems. It is furthermore an important step towards explainable and safe applied artificial intelligence.
