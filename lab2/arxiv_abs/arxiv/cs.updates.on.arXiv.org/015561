Visual Place Recognition (VPR) is the process of recognising a previously visited place using visual
information, often under varying appearance conditions and viewpoint changes and with computational
constraints. VPR is a critical component of many autonomous navigation systems ranging from autonomous
vehicles to drones. While the concept of place recognition has been around for many years, VPR research
has grown rapidly as a field over the past decade due to both improving camera hardware technologies
and its suitability for application of deep learning-based techniques. With this growth however
has come field fragmentation, lack of standardisation and a disconnect between current performance
metrics and the actual utility of a VPR technique at application-deployment. In this paper we address
these key challenges through a new comprehensive open-source evaluation framework, dubbed 'VPR-Bench'.
VPR-Bench introduces two much-needed capabilities for researchers: firstly, quantification
of viewpoint and illumination variation, replacing what has largely been assessed qualitatively
in the past, and secondly, new metrics 'Extended precision' (EP), 'Performance-Per-Compute-Unit'
(PCU) and 'Number of Prospective Place Matching Candidates' (NPPMC). These new metrics complement
the limitations of traditional Precision-Recall curves, by providing measures that are more informative
to the wide range of potential VPR applications. Mechanistically, we develop new unified templates
that facilitate the implementation, deployment and evaluation of a wide range of VPR techniques
and datasets. We incorporate the most comprehensive combination of state-of-the-art VPR techniques
and datasets to date into VPR-Bench and demonstrate how it provides a rich range of previously inaccessible
insights, such as the nuanced relationship between viewpoint invariance, different types of VPR
techniques and datasets. 