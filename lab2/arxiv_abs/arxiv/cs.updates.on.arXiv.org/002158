Social media platforms have recently seen an increase in the occurrence of hate speech discourse
which has led to calls for improved detection methods. Most of these rely on annotated data, keywords,
and a classification technique. While this approach provides good coverage, it can fall short when
dealing with new terms produced by online extremist communities which act as original sources of
words which have alternate hate speech meanings. These code words (which can be both created and
adopted words) are designed to evade automatic detection and often have benign meanings in regular
discourse. As an example, "skypes", "googles", and "yahoos" are all instances of words which have
an alternate meaning that can be used for hate speech. This overlap introduces additional challenges
when relying on keywords for both the collection of data that is specific to hate speech, and downstream
classification. In this work, we develop a community detection approach for finding extremist
hate speech communities and collecting data from their members. We also develop a word embedding
model that learns the alternate hate speech meaning of words and demonstrate the candidacy of our
code words with several annotation experiments, designed to determine if it is possible to recognize
a word as being used for hate speech without knowing its alternate meaning. We report an inter-annotator
agreement rate of K=0.871, and K=0.676 for data drawn from our extremist community and the keyword
approach respectively, supporting our claim that hate speech detection is a contextual task and
does not depend on a fixed list of keywords. Our goal is to advance the domain by providing a high quality
hate speech dataset in addition to learned code words that can be fed into existing classification
approaches, thus improving the accuracy of automated detection. 