Deep learning has revolutionized speech recognition, image recognition, and natural language
processing since 2010, each involving a single modality in the input signal. However, many applications
in artificial intelligence involve more than one modality. It is therefore of broad interest to
study the more difficult and complex problem of modeling and learning across multiple modalities.
In this paper, a technical review of the models and learning methods for multimodal intelligence
is provided. The main focus is the combination of vision and natural language, which has become an
important area in both computer vision and natural language processing research communities.
This review provides a comprehensive analysis of recent work on multimodal deep learning from three
angles --- learning multimodal representations, the fusion of multimodal signals at various levels,
and multimodal applications. On multimodal representation learning, we review the key concept
of embedding, which unifies the multimodal signals into the same vector space and thus enables cross-modality
signal processing. We also review the properties of the many types of embedding constructed and
learned for general downstream tasks. On multimodal fusion, this review focuses on special architectures
for the integration of the representation of unimodal signals for a particular task. On applications,
selected areas of a broad interest in current literature are covered, including image-to-text
caption generation, text-to-image generation, and visual question answering. We believe this
review can facilitate future studies in the emerging field of multimodal intelligence for the community.
