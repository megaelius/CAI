Emotion plays a key role in many applications like healthcare, to gather patients emotional behavior.
There are certain emotions which are given more importance due to their effectiveness in understanding
human feelings. In this paper, we propose an approach that models human stress from audio signals.
The research challenge in speech emotion detection is defining the very meaning of stress and being
able to categorize it in a precise manner. Supervised Machine Learning models, including state
of the art Deep Learning classification methods, rely on the availability of clean and labelled
data. One of the problems in affective computation and emotion detection is the limited amount of
annotated data of stress. The existing labelled stress emotion datasets are highly subjective
to the perception of the annotator. We address the first issue of feature selection by exploiting
the use of traditional MFCC features in Convolutional Neural Network. Our experiments show that
Emo-CNN consistently and significantly outperforms the popular existing methods over multiple
datasets. It achieves 90.2% categorical accuracy on the Emo-DB dataset. To tackle the second and
the more significant problem of subjectivity in stress labels, we use Lovheim's cube, which is a
3-dimensional projection of emotions. The cube aims at explaining the relationship between these
neurotransmitters and the positions of emotions in 3D space. The learnt emotion representations
from the Emo-CNN are mapped to the cube using three component PCA (Principal Component Analysis)
which is then used to model human stress. This proposed approach not only circumvents the need for
labelled stress data but also complies with the psychological theory of emotions given by Lovheim's
cube. We believe that this work is the first step towards creating a connection between Artificial
Intelligence and the chemistry of human emotions. 