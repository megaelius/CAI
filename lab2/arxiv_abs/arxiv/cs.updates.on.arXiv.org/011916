We study learning control in an online lifelong learning scenario, where mistakes can compound
catastrophically into the future and the underlying dynamics of the environment may change. Traditional
model-free policy learning methods have achieved successes in difficult tasks due to their broad
flexibility, and capably condense broad experiences into compact networks, but struggle in this
setting, as they can activate failure modes early in their lifetimes which are difficult to recover
from and face performance degradation as dynamics change. On the other hand, model-based planning
methods learn and adapt quickly, but require prohibitive levels of computational resources. Under
constrained computation limits, the agent must allocate its resources wisely, which requires
the agent to understand both its own performance and the current state of the environment: knowing
that its mastery over control in the current dynamics is poor, the agent should dedicate more time
to planning. We present a new algorithm, Adaptive Online Planning (AOP), that achieves strong performance
in this setting by combining model-based planning with model-free learning. By measuring the performance
of the planner and the uncertainty of the model-free components, AOP is able to call upon more extensive
planning only when necessary, leading to reduced computation times. We show that AOP gracefully
deals with novel situations, adapting behaviors and policies effectively in the face of unpredictable
changes in the world -- challenges that a continual learning agent naturally faces over an extended
lifetime -- even when traditional reinforcement learning methods fail. 