The field of Few-Shot Learning (FSL), or learning from very few (typically $1$ or $5$) examples per
novel class (unseen during training), has received a lot of attention and significant performance
advances in the recent literature. While number of techniques have been proposed for FSL, several
factors have emerged as most important for FSL performance, awarding SOTA even to the simplest of
techniques. These are: the backbone architecture (bigger is better), type of pre-training on the
base classes (meta-training vs regular multi-class, currently regular wins), quantity and diversity
of the base classes set (the more the merrier, resulting in richer and better adaptive features),
and the use of self-supervised tasks during pre-training (serving as a proxy for increasing the
diversity of the base set). In this paper we propose yet another simple technique that is important
for the few shot learning performance - a search for a compact feature sub-space that is discriminative
for a given few-shot test task. We show that the Task-Adaptive Feature Sub-Space Learning (TAFSSL)
can significantly boost the performance in FSL scenarios when some additional unlabeled data accompanies
the novel few-shot task, be it either the set of unlabeled queries (transductive FSL) or some additional
set of unlabeled data samples (semi-supervised FSL). Specifically, we show that on the challenging
miniImageNet and tieredImageNet benchmarks, TAFSSL can improve the current state-of-the-art
in both transductive and semi-supervised FSL settings by more than $5\%$, while increasing the
benefit of using unlabeled data in FSL to above $10\%$ performance gain. 