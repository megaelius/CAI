In image quality enhancement processing, it is the most important to predict how humans perceive
processed images since human observers are the ultimate receivers of the images. Thus, objective
image quality assessment (IQA) methods based on human visual sensitivity from psychophysical
experiments have been extensively studied. Thanks to the powerfulness of deep convolutional neural
networks (CNN), many CNN based IQA models have been studied. However, previous CNN-based IQA models
have not fully utilized the characteristics of human visual systems (HVS) for IQA problems by simply
entrusting everything to CNN where the CNN-based models are often trained as a regressor to predict
the scores of subjective quality assessment obtained from IQA datasets. In this paper, we propose
a novel JND-based saliency-channel attention residual network for image quality assessment,
called JND-SalCAR, where the human psychophysical characteristics such as visual saliency and
just noticeable difference (JND) are effectively incorporated. We newly propose a SalCAR block
so that perceptually important features can be extracted by using a saliency-based spatial attention
and a channel attention. In addition, the visual saliency map is further used as a guideline for predicting
the patch weight map in order to afford a stable training of end-to-end optimization for the JND-SalCAR.
To our best knowledge, our work is the first HVS-inspired trainable IQA network that considers both
the visual saliency and JND characteristics of HVS. We evaluate the proposed JND-SalCAR on large
IQA datasets where it outperforms all the recent state-of-the-art IQA methods. 