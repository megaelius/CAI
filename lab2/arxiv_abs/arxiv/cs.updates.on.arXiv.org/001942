Previous studies have shown that spike-timing-dependent plasticity (STDP) can be used in spiking
neural networks (SNN) to extract visual features of low or intermediate complexity in an unsupervised
manner. These studies, however, used relatively shallow architectures, and only one layer was
trainable. Another line of research has demonstrated - using rate-based neural networks trained
with back-propagation - that having many layers increases the recognition robustness, an approach
known as deep learning. We thus designed a deep SNN, comprising several convolutional (trainable
with STDP) and pooling layers. We used a temporal coding scheme where the most strongly activated
neurons fire first, and less activated neurons fire later or not at all. The network was exposed to
natural images. Thanks to STDP, neurons progressively learned features corresponding to prototypical
patterns that were both salient and frequent. Only a few tens of examples per category were required
and no label was needed. After learning, the complexity of the extracted features increased along
the hierarchy, from edge detectors in the first layer to object prototypes in the last layer. Coding
was very sparse, with only a few thousands spikes per image, and in some cases the object category
could be reasonably well inferred from the activity of a single higher-order neuron. More generally,
the activity of a few hundreds of such neurons contained robust category information, as demonstrated
using a classifier on Caltech 101, ETH-80, and MNIST databases. We also demonstrate the superiority
of STDP over other unsupervised techniques such as random crops (HMAX) or auto-encoders. Taken
together, our results suggest that the combination of STDP with latency coding may be a key to understanding
the way that the primate visual system learns, its remarkable processing speed and its low energy
consumption. 