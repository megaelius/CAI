A novel evolutionary approach for Explainable Artificial Intelligence is presented: the "Evolved
Explanations" model (EvEx). This methodology consists in combining Local Interpretable Model
Agnostic Explanations (LIME) with Multi-Objective Genetic Algorithms to allow for automated
segmentation parameter tuning in image classification tasks. In this case, the dataset studied
is Patch-Camelyon, comprised of patches from pathology whole slide images. A publicly available
Convolutional Neural Network (CNN) was trained on this dataset to provide a binary classification
for presence/absence of lymph node metastatic tissue. In turn, the classifications are explained
by means of evolving segmentations, seeking to optimize three evaluation goals simultaneously.
The final explanation is computed as the mean of all explanations generated by Pareto front individuals,
evolved by the developed genetic algorithm. To enhance reproducibility and traceability of the
explanations, each of them was generated from several different seeds, randomly chosen. The observed
results show remarkable agreement between different seeds. Despite the stochastic nature of LIME
explanations, regions of high explanation weights proved to have good agreement in the heat maps,
as computed by pixel-wise relative standard deviations. The found heat maps coincide with expert
medical segmentations, which demonstrates that this methodology can find high quality explanations
(according to the evaluation metrics), with the novel advantage of automated parameter fine tuning.
These results give additional insight into the inner workings of neural network black box decision
making for medical data. 