Recent discoveries in the field of adversarial machine learning have shown that Artificial Neural
Networks (ANNs) are susceptible to adversarial attacks. These attacks cause misclassification
of specially crafted adversarial samples. In light of this phenomenon, it is worth investigating
whether other types of neural networks are less susceptible to adversarial attacks. In this work,
we applied standard attack methods originally aimed at conventional ANNs, towards stochastic
ANNs and also towards Spiking Neural Networks (SNNs), across three different datasets namely MNIST,
CIFAR-10 and Patch Camelyon. We analysed their adversarial robustness against attacks performed
in the raw image space of the different model variants. We employ a variety of attacks namely Basic
Iterative Method (BIM), Carlini & Wagner L2 attack (CWL2) and Boundary attack. Our results suggests
that SNNs and stochastic ANNs exhibit some degree of adversarial robustness as compared to their
ANN counterparts under certain attack methods. Namely, we found that the Boundary and the state-of-the-art
CWL2 attacks are largely ineffective against stochastic ANNs. Following this observation, we
proposed a modified version of the CWL2 attack and analysed the impact of this attack on the models'
adversarial robustness. Our results suggest that with this modified CWL2 attack, many models are
more easily fooled as compared to the vanilla CWL2 attack, albeit observing an increase in L2 norms
of adversarial perturbations. Lastly, we also investigate the resilience of alternative neural
networks against adversarial samples transferred from ResNet18. We show that the modified CWL2
attack provides an improved cross-architecture transferability compared to other attacks. 