Deep learning methods have received increasing interest by the remote sensing community for multi-temporal
land cover classification in recent years. Convolutional Neural networks that elementwise compare
a time series with learned kernels, and recurrent neural networks that sequentially process temporal
data have dominated the state-of-the-art in the classification of vegetation from satellite time
series. Self-attention allows a neural network to selectively extract features from specific
times in the input sequence thus suppressing non-classification relevant information. Today,
self-attention based neural networks dominate the state-of-the-art in natural language processing
but are hardly explored and tested in the remote sensing context. In this work, we embed self-attention
in the canon of deep learning mechanisms for satellite time series classification for vegetation
modeling and crop type identification. We compare it quantitatively to convolution, and recurrence
and test four models that each exclusively relies on one of these mechanisms. The models are trained
to identify the type of vegetation on crop parcels using raw and preprocessed Sentinel 2 time series
over one entire year. To obtain an objective measure we find the best possible performance for each
of the models by a large-scale hyperparameter search with more than 2400 validation runs. Beyond
the quantitative comparison, we qualitatively analyze the models by an easy-to-implement, but
yet effective feature importance analysis based on gradient back-propagation that exploits the
differentiable nature of deep learning models. Finally, we look into the self-attention transformer
model and visualize attention scores as bipartite graphs in the context of the input time series
and a low-dimensional representation of internal hidden states using t-distributed stochastic
neighborhood embedding (t-SNE). 