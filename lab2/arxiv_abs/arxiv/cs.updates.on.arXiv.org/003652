We present a novel framework for domain adaptation, whereby both geometric and statistical differences
between a labeled source domain and unlabeled target domain can be integrated by exploiting the
curved Riemannian geometry of statistical manifolds. Our approach is based on formulating transfer
from source to target as a problem of geometric mean metric learning on manifolds. Specifically,
we exploit the curved Riemannian manifold geometry of symmetric positive definite (SPD) covariance
matrices. We exploit a simple but important observation that as the space of covariance matrices
is both a Riemannian space as well as a homogeneous space, the shortest path geodesic between two
covariances on the manifold can be computed analytically. Statistics on the SPD matrix manifold,
such as the geometric mean of two matrices can be reduced to solving the well-known Riccati equation.
We show how the Ricatti-based solution can be constrained to not only reduce the statistical differences
between the source and target domains, such as aligning second order covariances and minimizing
the maximum mean discrepancy, but also the underlying geometry of the source and target domains
using diffusions on the underlying source and target manifolds. A key strength of our proposed approach
is that it enables integrating multiple sources of variation between source and target in a unified
way, by reducing the combined objective function to a nested set of Ricatti equations where the solution
can be represented by a cascaded series of geometric mean computations. In addition to showing the
theoretical optimality of our solution, we present detailed experiments using standard transfer
learning testbeds from computer vision comparing our proposed algorithms to past work in domain
adaptation, showing improved results over a large variety of previous methods. 