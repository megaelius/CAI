Machine learning is yielding unprecedented interest in research and industry, due to recent success
in many applied contexts such as image classification and object recognition. However, the deployment
of these systems requires huge computing capabilities, thus making them unsuitable for embedded
systems. To deal with this limitation, many researchers are investigating brain-inspired computing,
which would be a perfect alternative to the conventional Von Neumann architecture based computers
(CPU/GPU) that meet the requirements for computing performance, but not for energy-efficiency.
Therefore, neuromorphic hardware circuits that are adaptable for both parallel and distributed
computations need to be designed. In this paper, we focus on Spiking Neural Networks (SNNs) with
a comprehensive study of information coding methods and hardware exploration. In this context,
we propose a framework for neuromorphic hardware design space exploration, which allows to define
a suitable architecture based on application-specific constraints and starting from a wide variety
of possible architectural choices. For this framework, we have developed a behavioral level simulator
for neuromorphic hardware architectural exploration named NAXT. Moreover, we propose modified
versions of the standard Rate Coding technique to make trade-offs with the Time Coding paradigm,
which is characterized by the low number of spikes propagating in the network. Thus, we are able to
reduce the number of spikes while keeping the same neuron's model, which results in an SNN with fewer
events to process. By doing so, we seek to reduce the amount of power consumed by the hardware. Furthermore,
we present three neuromorphic hardware architectures in order to quantitatively study the implementation
of SNNs. These architectures are derived from a novel funnel-like Design Space Exploration framework
for neuromorphic hardware. 