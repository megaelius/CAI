Biomedical text mining is becoming increasingly important as the number of biomedical documents
rapidly grows. With the progress in machine learning, extracting valuable information from biomedical
literature has gained popularity among researchers, and deep learning has boosted the development
of effective biomedical text mining models. However, as deep learning models require a large amount
of training data, applying deep learning to biomedical text mining is often unsuccessful due to
the lack of training data in biomedical fields. Recent researches on training contextualized language
representation models on text corpora shed light on the possibility of leveraging a large number
of unannotated biomedical text corpora. We introduce BioBERT (Bidirectional Encoder Representations
from Transformers for Biomedical Text Mining), which is a domain specific language representation
model pre-trained on large-scale biomedical corpora. Based on the BERT architecture, BioBERT
effectively transfers the knowledge from a large amount of biomedical texts to biomedical text
mining models with minimal task-specific architecture modifications. While BERT shows competitive
performances with previous state-of-the-art models, BioBERT significantly outperforms them
on the following three representative biomedical text mining tasks: biomedical named entity recognition
(1.86% absolute improvement), biomedical relation extraction (3.33% absolute improvement),
and biomedical question answering (9.61% absolute improvement). We make the pre-trained weights
of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source
code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert. 