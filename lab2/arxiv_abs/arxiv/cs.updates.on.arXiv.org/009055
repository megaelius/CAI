Multi-modal data fusion has recently been shown promise in classification tasks in remote sensing.
Optical data and radar data, two important yet intrinsically different data sources, are attracting
more and more attention for potential data fusion. It is already widely known that, a machine learning
based methodology often yields excellent performance. However, the methodology relies on a large
training set, which is very expensive to achieve in remote sensing. The semi-supervised manifold
alignment (SSMA), a multi-modal data fusion algorithm, has been designed to amplify the impact
of an existing training set by linking labeled data to unlabeled data via unsupervised techniques.
In this paper, we explore the potential of SSMA in fusing optical data and polarimetric SAR data,
which are multi-sensory data sources. Furthermore, we propose a MAPPER-induced manifold alignment
(MIMA) for semi-supervised fusion of multi-sensory data sources. Our proposed method unites SSMA
with MAPPER, which is developed from the emerging topological data analysis (TDA) field. To our
best knowledge, this is the first time that SSMA has been applied on fusing optical data and SAR data,
and also the first time that TDA has been applied in remote sensing. The conventional SSMA derives
a topological structure using k-nearest-neighbor (kNN), while MIMA employs MAPPER, which considers
the field knowledge and derives a novel topological structure through the spectral clustering
in a data-driven fashion. Experiment results on data fusion with respect to land cover land use classification
and local climate zone classification suggest superior performance of MIMA. 