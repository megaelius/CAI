Extracting relations from text corpora is an important task in text mining. It becomes particularly
challenging when focusing on weakly-supervised relation extraction, that is, utilizing a few
relation instances (i.e., a pair of entities and their relation) as seeds to extract more instances
from corpora. Existing distributional approaches leverage the corpus-level co-occurrence statistics
of entities to predict their relations, and require large number of labeled instances to learn effective
relation classifiers. Alternatively, pattern-based approaches perform bootstrapping or apply
neural networks to model the local contexts, but still rely on large number of labeled instances
to build reliable models. In this paper, we study integrating the distributional and pattern-based
methods in a weakly-supervised setting, such that the two types of methods can provide complementary
supervision for each other to build an effective, unified model. We propose a novel co-training
framework with a distributional module and a pattern module. During training, the distributional
module helps the pattern module discriminate between the informative patterns and other patterns,
and the pattern module generates some highly-confident instances to improve the distributional
module. The whole framework can be effectively optimized by iterating between improving the pattern
module and updating the distributional module. We conduct experiments on two tasks: knowledge
base completion with text corpora and corpus-level relation extraction. Experimental results
prove the effectiveness of our framework in the weakly-supervised setting. 