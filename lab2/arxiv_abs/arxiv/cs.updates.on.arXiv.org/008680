In this paper we introduce DELTR, a learning-to-rank framework that addresses potential issues
of discrimination and unequal opportunity in rankings. Following long-standing empirical observations
showing that users of information retrieval systems rarely look past the first few results, we measure
these problems in terms of discrepancies in the average group exposure. Specifically, we define
our notion of group exposure as the average probability of items from a legally protected social
group to be ranked at the top position. With this we design a ranker that optimizes search results
in terms of relevance, while at the same time reducing potential discrimination or inequality of
opportunity. We describe this objective formally, how to optimize it efficiently, and how to implement
it. We perform an extensive experimental study showing that being "colorblind," i.e. ignoring
protected attributes such as race or gender, can be among the best choices or the worst choices from
the perspective of relevance and exposure, depending on how much and which kind of bias is present
in the training set. As baselines for benchmarking our in-processing method we use pre-processing
and post-processing methods based on FA*IR, a state-of-the-art algorithm to re-rank search results
according to predefined fairness constraints. We show that our in-processing method performs
better in terms of relevance and equality of exposure than pre-processing and post-processing
across all tested scenarios. Our proposed method neither makes assumptions about biases in the
training data, nor does it ignore relevance scores of items and thus can reduce discrimination and
inequality of opportunity without having to introduce large distortions in ranking relevance.
