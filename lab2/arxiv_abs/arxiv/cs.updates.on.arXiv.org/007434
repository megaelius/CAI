In data mining, the data in various business cases (e.g., sales, marketing, and demography) gets
refreshed periodically. During the refresh, the old dataset is replaced by a new one. Confirming
the quality of the new dataset can be challenging because changes are inevitable. How do analysts
distinguish reasonable real-world changes vs. errors related to data capture or data transformation?
While some of the errors are easy to spot, the others may be more subtle. In order to detect such types
of errors, an analyst will typically have to examine the data manually and assess if the data produced
are "believable". Due to the scale of data, such examination is tedious and laborious. Thus, to save
the analyst's time, it is important to detect these errors automatically. However, both the literature
and the industry are still lacking methods to assess the difference between old and new versions
of a dataset during the refresh process. In this paper, we present a comprehensive set of tests for
the detection of abnormalities in a refreshed dataset, based on the information obtained from a
previous vintage of the dataset. We implement these tests in automated test harness made available
as an open-source package, called RESTORE, for R language. The harness accepts flat or hierarchical
numeric datasets. We also present a validation case study, where we apply our test harness to hierarchical
demographic datasets. The results of the study and feedback from data scientists using the package
suggest that RESTORE enables fast and efficient detection of errors in the data as well as decreases
the cost of testing. 