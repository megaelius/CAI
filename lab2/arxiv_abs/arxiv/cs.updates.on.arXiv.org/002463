Tensor completion is a technique of filling missing elements of the incomplete data tensors. It
being actively studied based on the convex optimization scheme such as nuclear-norm minimization.
When given data tensors include some noises, the nuclear-norm minimization problem is usually
converted to the nuclear-norm `regularization' problem which simultaneously minimize penalty
and error terms with some trade-off parameter. However, the good value of trade-off is not easily
determined because of the difference of two units and the data dependence. In the sense of trade-off
tuning, the noisy tensor completion problem with the `noise inequality constraint' is better choice
than the `regularization' because the good noise threshold can be easily bounded with noise standard
deviation. In this study, we tackle to solve the convex tensor completion problems with two types
of noise inequality constraints: Gaussian and Laplace distributions. The contributions of this
study are follows: (1) New tensor completion and denoising models using tensor total variation
and nuclear-norm are proposed which can be characterized as a generalization/extension of many
past matrix and tensor completion models, (2) proximal mappings for noise inequalities are derived
which are analytically computable with low computational complexity, (3) convex optimization
algorithm is proposed based on primal-dual splitting framework, (4) new step-size adaptation
method is proposed to accelerate the optimization, and (5) extensive experiments demonstrated
the advantages of the proposed method for visual data retrieval such as for color images, movies,
and 3D-volumetric data. 