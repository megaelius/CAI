Understanding the coordinated activity underlying brain computations requires large-scale,
simultaneous recordings from distributed neuronal structures at a cellular-level resolution.
One major hurdle to design high-bandwidth, high-precision, large-scale neural interfaces lies
in the formidable data streams that are generated by the recorder chip and need to be online transferred
to a remote computer. The data rates can require hundreds to thousands of I/O pads on the recorder
chip and power consumption on the order of Watts for data streaming alone. We developed a deep learning-based
compression model to reduce the data rate of multichannel action potentials. The proposed model
is built upon a deep compressive autoencoder (CAE) with discrete latent embeddings. The encoder
is equipped with residual transformations to extract representative features from spikes, which
are mapped into the latent embedding space and updated via vector quantization (VQ). The decoder
network reconstructs spike waveforms from the quantized latent embeddings. Experimental results
show that the proposed model consistently outperforms conventional methods by achieving much
higher compression ratios (20-500x) and better or comparable reconstruction accuracies. Testing
results also indicate that CAE is robust against a diverse range of imperfections, such as waveform
variation and spike misalignment, and has minor influence on spike sorting accuracy. Furthermore,
we have estimated the hardware cost and real-time performance of CAE and shown that it could support
thousands of recording channels simultaneously without excessive power/heat dissipation. The
proposed model can reduce the required data transmission bandwidth in large-scale recording experiments
and maintain good signal qualities. The code of this work has been made available at https://github.com/tong-wu-umn/spike-compression-autoencoder.
