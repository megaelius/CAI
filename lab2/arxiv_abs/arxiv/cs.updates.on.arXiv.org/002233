It is well established that humans decision making and instrumental control uses multiple systems,
some which use habitual action selection and some which require deliberate planning. Deliberate
planning systems use predictions of action-outcomes using an internal model of the agent's environment,
while habitual action selection systems learn to automate by repeating previously rewarded actions.
Habitual control is computationally efficient but may be inflexible in changing environments.
Conversely, deliberate planning may be computationally expensive, but flexible in dynamic environments.
This paper proposes a general architecture comprising both control paradigms by introducing an
arbitrator that controls which subsystem is used at any time. This system is implemented for a target-reaching
task with a simulated two-joint robotic arm that comprises a supervised internal model and deep
reinforcement learning. Through permutation of target-reaching conditions, we demonstrate
that the proposed is capable of rapidly learning kinematics of the system without a priori knowledge,
and is robust to (A) changing environmental reward and kinematics, and (B) occluded vision. The
arbitrator model is compared to exclusive deliberate planning with the internal model and exclusive
habitual control instances of the model. The results show how such a model can harness the benefits
of both systems, using fast decisions in reliable circumstances while optimizing performance
in changing environments. In addition, the proposed model learns very fast. Finally, the system
which includes internal models is able to reach the target under the visual occlusion, while the
pure habitual system is unable to operate sufficiently under such conditions. 