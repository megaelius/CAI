Common pitfalls in visualization projects include lack of data availability and the domain users'
needs and focus changing too rapidly for the design process to complete. While it is often prudent
to avoid such projects, we argue it can be beneficial to engage them in some cases as the visualization
process can help refine data collection, solving a `chicken and egg' problem of having the data and
tools to analyze it. We found this to be the case in the domain of task parallel computing. Parallel
and distributed programs orchestrate the cooperation of many computation resources to produce
results that would be impossible to collect on a single machine. The complexity of orchestrating
and fine-tuning the execution of these parallel and distributed computations requires careful
consideration from the programmer. However, few performance data collection and analysis tools
are geared towards the specifics of task parallel paradigms. What changes need to be made are an open
area of research. Despite these hurdles, we conducted a design study. Through a tightly-coupled
iterative design process, we built Atria, a multi-view execution graph visualization to support
performance analysis. Atria simplifies the initial representation of the execution graph by aggregating
nodes as related to their line of code. We deployed Atria on multiple platforms, some requiring design
alteration. Our evaluation is augmented with user interviews. We further describe how we adapted
the design study methodology to the `moving target' of both the data and the domain experts' concerns
and how this movement kept both the visualization and programming project healthy. We reflect on
our process and discuss what factors allow the project to be successful in the presence of changing
data and user needs. 