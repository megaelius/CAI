Today's systems are overwhelmingly designed to move data to computation. This design choice goes
directly against at least three key trends in systems that cause performance, scalability and energy
bottlenecks: (1) data access from memory is already a key bottleneck as applications become more
data-intensive and memory bandwidth and energy do not scale well, (2) energy consumption is a key
constraint in especially mobile and server systems, (3) data movement is very expensive in terms
of bandwidth, energy and latency, much more so than computation. At the same time, conventional
memory technology is facing many scaling challenges in terms of reliability, energy, and performance.
As a result, memory system architects are open to organizing memory in different ways and making
it more intelligent, at the expense of higher cost. The emergence of 3D-stacked memory plus logic
as well as the adoption of error correcting codes inside DRAM chips, and the necessity for designing
new solutions to serious reliability and security issues, such as the RowHammer phenomenon, are
an evidence of this trend. Recent research aims to practically enable computation close to data.
We discuss at least two promising directions for processing-in-memory (PIM): (1) performing massively-parallel
bulk operations in memory by exploiting the analog operational properties of DRAM, with low-cost
changes, (2) exploiting the logic layer in 3D-stacked memory technology to accelerate important
data-intensive applications. In both approaches, we describe and tackle relevant cross-layer
research, design, and adoption challenges in devices, architecture, systems, and programming
models. Our focus is on the development of in-memory processing designs that can be adopted in real
computing platforms at low cost. 