Mean-field models are a popular tool in a variety of fields. They provide an understanding of the
impact of interactions among a large number of particles or people or other "self-interested agents",
and are an increasingly popular tool in distributed control. This paper considers a particular
randomized distributed control architecture introduced in our own recent work. In numerical results
it was found that the associated mean-field model had attractive properties for purposes of control.
In particular, when viewed as an input-output system, its linearization was found to be minimum
phase. In this paper we take a closer look at the control model. The results are summarized as follows:
(i) The Markov Decision Process framework of Todorov is extended to continuous time models, in which
the "control cost" is based on relative entropy. This is the basis of the construction of a family
of Markovian generators, parameterized by a scalar $\zeta\in\Re$. (ii) A decentralized control
architecture is proposed in which each agent evolves as a controlled Markov process. A central authority
broadcasts a common control signal $\{\zeta_t\}$ to each agent. The central authority chooses
$\{\zeta_t\}$ based on an aggregate scalar output of the Markovian agents. \textit{This is the
basis of the mean field model. } (iii) Provided the control-free system (with $\zeta\equiv 0$) is
a reversible Markov process, the following identity holds for the transfer function $G$ obtained
from the linearization, \[ \text{Real} (G(j\omega)) = \text{PSD}_Y(\omega)\ge 0 \qquad \omega\in\Re\,,
\] where the right hand side denotes the power spectral density for the output of any one of the individual
Markov processes (with $\zeta\equiv 0$). 