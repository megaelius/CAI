We propose a new class of Linear Threshold Model-based information-diffusion model that incorporates
the formation and spread of negative attitude. We call such models negativity-aware.. We show that
in these models, the influence function is a monotone submodular function. Thus we can use the greedy
algorithm to construct seed sets with constant approximation guarantees, when the objective is
to select a seed set of fixed size $K$ to maximize total influence. Our models are flexible enough
to account for both the features of local users and the features of the information being propagated
in the diffusion. We analyze an online-learning setting for a multi-round influence-maximization
problem, where an agent is actively learning the diffusion parameters over time while trying to
maximize total cumulative influence. We assume that in each diffusion step, the agent can only observe
whether a node becomes positively or negatively influenced, or remains inactive. In particular,
he does not observe the particular edge that brought about the activation of a node, if any. This model
of feedback is called node-level feedback, as opposed to the more common edge-level feedback model
in which he is able to observe, for each node, the edge through which that node is influenced. Under
mild assumptions, we develop online learning algorithms that achieve cumulative expected regrets
of order $O(1/\sqrt{T})$, where $T$ is the total number of rounds. These are the first regret guarantees
for node-level feedback models of influence maximization of any kind. Furthermore, with mild assumptions,
this result also improves the average regret of $\mathcal{O}(\sqrt{\ln T / T})$ for the edge-level
feedback model in \cite{wen2017online}, thus providing a new performance benchmark. 