Haptic exploration is a key skill for both robots and humans to discriminate and handle unknown objects
or to recognize familiar objects. Its active nature is evident in humans who from early on reliably
acquire sophisticated sensory-motor capabilities for active exploratory touch and directed
manual exploration that associates surfaces and object properties with their spatial locations.
This is in stark contrast to robotics. In this field, the relative lack of good real-world interaction
models - along with very restricted sensors and a scarcity of suitable training data to leverage
machine learning methods - has so far rendered haptic exploration a largely underdeveloped skill.
In robot vision however, deep learning approaches and an abundance of available training data have
triggered huge advances. In the present work, we connect recent advances in recurrent models of
visual attention with previous insights about the organisation of human haptic search behavior,
exploratory procedures and haptic glances for a novel architecture that learns a generative model
of haptic exploration in a simulated three-dimensional environment. The proposed algorithm simultaneously
optimizes main perception-action loop components: feature extraction, integration of features
over time, and the control strategy, while continuously acquiring data online. The resulting haptic
meta-controller for the rigid $16 \times 16$ tactile sensor array moving in a physics-driven simulation
environment, called the Haptic Attention Model, performs a sequence of haptic glances, and outputs
corresponding force measurements. The resulting method has been successfully tested with four
different objects. It achieved results close to $100 \%$ while performing object contour exploration
that has been optimized for its own sensor morphology. 