Conversational agents (CAs) play an important role in human computer interaction. Creating believable
movements for CAs is challenging, since the movements have to be meaningful and natural, reflecting
the coupling between gestures and speech. Studies in the past have mainly relied on rule-based or
data-driven approaches. Rule-based methods focus on creating meaningful behaviors conveying
the underlying message, but the gestures cannot be easily synchronized with speech. Data-driven
approaches, especially speech-driven models, can capture the relationship between speech and
gestures. However, they create behaviors disregarding the meaning of the message. This study proposes
to bridge the gap between these two approaches overcoming their limitations. The approach builds
a dynamic Bayesian network (DBN), where a discrete variable is added to constrain the behaviors
on the underlying constraint. The study implements and evaluates the approach with two constraints:
discourse functions and prototypical behaviors. By constraining on the discourse functions (e.g.,
questions), the model learns the characteristic behaviors associated with a given discourse class
learning the rules from the data. By constraining on prototypical behaviors (e.g., head nods),
the approach can be embedded in a rule-based system as a behavior realizer creating trajectories
that are timely synchronized with speech. The study proposes a DBN structure and a training approach
that (1) models the cause-effect relationship between the constraint and the gestures, (2) initializes
the state configuration models increasing the range of the generated behaviors, and (3) captures
the differences in the behaviors across constraints by enforcing sparse transitions between shared
and exclusive states per constraint. Objective and subjective evaluations demonstrate the benefits
of the proposed approach over an unconstrained model. 