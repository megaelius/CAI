Motivation: NLP continues improving substantially through auto-regressive and auto-encoding
Language Models. These LMs require expensive computing resources for self-supervised or un-supervised
learning from huge unlabelled text corpora. The information learned is transferred through so-called
embeddings to downstream prediction tasks. Bioinformatics provide vast gold-mines of structured
and sequentially ordered text data leading to extraordinarily successful protein sequence LMs
that promise new frontiers for generative and predictive tasks at low inference cost. Here, we addressed
two questions: (1) To which extent can HPC up-scale protein LMs to larger databases and larger models?
(2) To which extent can LMs extract features from single proteins to get closer to the performance
of methods using evolutionary information? Methodology: Here, we trained two auto-regressive
language models (Transformer-XL and XLNet) and two auto-encoder models (BERT and Albert) using
80 billion amino acids from 200 million protein sequences (UniRef100) and 393 billion amino acids
from 2.1 billion protein sequences (BFD). The LMs were trained on the Summit supercomputer, using
5616 GPUs and one TPU Pod, using V3-512 cores. Results: The results of training these LMs on proteins
was assessed by predicting secondary structure in three- and eight-states (Q3=75-83, Q8=63-72),
localization for 10 cellular compartments (Q10=74) and whether a protein is membrane-bound or
water-soluble (Q2=89). Dimensionality reduction revealed that the LM-embeddings from unlabelled
data (only protein sequences) captured important biophysical properties of the protein alphabet,
namely the amino acids, and their well orchestrated interplay in governing the shape of proteins.
In the analogy of NLP, this implied having learned some of the grammar of the language of life realized
in protein sequences. 