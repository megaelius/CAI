In many situations, sample data is obtained from a noisy or imperfect source. In order to address
such corruptions, this paper introduces the concept of a sampling corrector. Such algorithms use
structure that the distribution is purported to have, in order to allow one to make "on-the-fly"
corrections to samples drawn from probability distributions. These algorithms then act as filters
between the noisy data and the end user. We show connections between sampling correctors, distribution
learning algorithms, and distribution property testing algorithms. We show that these connections
can be utilized to expand the applicability of known distribution learning and property testing
algorithms as well as to achieve improved algorithms for those tasks. As a first step, we show how
to design sampling correctors using proper learning algorithms. We then focus on the question of
whether algorithms for sampling correctors can be more efficient in terms of sample complexity
than learning algorithms for the analogous families of distributions. When correcting monotonicity,
we show that this is indeed the case when also granted query access to the cumulative distribution
function. We also obtain sampling correctors for monotonicity without this stronger type of access,
provided that the distribution be originally very close to monotone (namely, at a distance $O(1/\log^2
n)$). In addition to that, we consider a restricted error model that aims at capturing "missing data"
corruptions. In this model, we show that distributions that are close to monotone have sampling
correctors that are significantly more efficient than achievable by the learning approach. We
also consider the question of whether an additional source of independent random bits is required
by sampling correctors to implement the correction process. 