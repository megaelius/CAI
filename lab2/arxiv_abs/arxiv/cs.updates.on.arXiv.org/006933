Person re-identification (Re-ID) aims to match identities across non-overlapping camera views.
Researchers have proposed many supervised Re-ID models which require quantities of cross-view
pairwise labelled data. This limits their scalabilities to many applications where a large amount
of data from multiple disjoint camera views is available but unlabelled. Although some unsupervised
Re-ID models have been proposed to address the scalability problem, they often suffer from the view-specific
bias problem which is caused by dramatic variances across different camera views, e.g., different
illumination, viewpoints and occlusion. The dramatic variances induce specific feature distortions
in different camera views, which can be very disturbing in finding cross-view discriminative information
for Re-ID in the unsupervised scenarios, since no label information is available to help alleviate
the bias. We propose to explicitly address this problem by learning an unsupervised asymmetric
distance metric based on cross-view clustering. The asymmetric distance metric allows specific
feature transformations for each camera view to tackle the specific feature distortions. We then
design a novel unsupervised loss function to embed the asymmetric metric into a deep neural network,
and therefore develop a novel unsupervised deep framework named the DEep Clustering-based Asymmetric
MEtric Learning (DECAMEL). In such a way, DECAMEL jointly learns the feature representation and
the unsupervised asymmetric metric. DECAMEL learns a compact cross-view cluster structure of
Re-ID data, and thus help alleviate the view-specific bias and facilitate mining the potential
cross-view discriminative information for unsupervised Re-ID. Extensive experiments on seven
benchmark datasets whose sizes span several orders show the effectiveness of our framework. 