Malnutrition impacts quality of life and places annually-recurring burden on the health care system.
Half of older adults are at risk for malnutrition in long-term care (LTC). Monitoring and measuring
nutritional intake is paramount yet involves time-consuming and subjective visual assessment,
limiting current methods' reliability. The opportunity for automatic image-based estimation
exists. Some progress outside LTC has been made (e.g., calories consumed, food classification),
however, these methods have not been implemented in LTC, potentially due to a lack of ability to independently
evaluate automatic segmentation methods within the intake estimation pipeline. Here, we propose
and evaluate a novel fully-automatic semantic segmentation method for pixel-level classification
of food on a plate using a deep convolutional neural network (DCNN). The macroarchitecture of the
DCNN is a multi-scale encoder-decoder food network (EDFN) architecture comprising a residual
encoder microarchitecture, a pyramid scene parsing decoder microarchitecture, and a specialized
per-pixel food/no-food classification layer. The network was trained and validated on the pre-labelled
UNIMIB 2016 food dataset (1027 tray images, 73 categories), and tested on our novel LTC plate dataset
(390 plate images, 9 categories). Our fully-automatic segmentation method attained similar intersection
over union to the semi-automatic graph cuts (91.2% vs. 93.7%). Advantages of our proposed system
include: testing on a novel dataset, decoupled error analysis, no user-initiated annotations,
with similar segmentation accuracy and enhanced reliability in terms of types of segmentation
errors. This may address several short-comings currently limiting utility of automated food intake
tracking in time-constrained LTC and hospital settings. 