Automatic understanding of human affect using visual signals is of great importance in everyday
human-machine interactions. Appraising human emotional states, behaviors and reactions displayed
in real-world settings, can be accomplished using latent continuous dimensions (e.g., the circumplex
model of affect). Valence (i.e., how positive or negative is an emotion) and arousal (i.e., power
of the activation of the emotion) constitute the most popular and effective affect representations.
Nevertheless, the majority of collected datasets this far, although containing naturalistic
emotional states, have been captured in highly controlled recording conditions. In this paper,
we introduce the Aff-Wild benchmark for training and evaluating affect recognition algorithms.
We also report on the results of the First Affect-in-the-wild Challenge (Aff-Wild Challenge) that
was recently organized on the Aff-Wild database, and was the first ever challenge on the estimation
of valence and arousal in-the-wild. Furthermore, we design and extensively train an end-to-end
deep neural architecture which performs prediction of continuous emotion dimensions based on
visual cues. The proposed deep learning architecture, AffWildNet, includes convolutional and
recurrent neural network (CNN-RNN) layers, exploiting the invariant properties of convolutional
features, while also modeling temporal dynamics that arise in human behavior via the recurrent
layers. The AffWildNet produced state-of-the-art results on the Aff-Wild Challenge. We then exploit
the AffWild database for learning features, which can be used as priors for achieving best performances
both for dimensional, as well as categorical emotion recognition, using the RECOLA, AFEW-VA and
EmotiW 2017 datasets, compared to all other methods designed for the same goal. 