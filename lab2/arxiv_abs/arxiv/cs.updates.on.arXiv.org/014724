A unique member of the power transformation family is known as the Box-Cox transformation. The latter
can be seen as a mathematical operation that leads to finding the optimum lambda ({\lambda}) value
that maximizes the log-likelihood function to transform a data to a normal distribution and to reduce
heteroscedasticity. In data analytics, a normality assumption underlies a variety of statistical
test models. This technique, however, is best known in statistical analysis to handle one-dimensional
data. Herein, this paper revolves around the utility of such a tool as a pre-processing step to transform
two-dimensional data, namely, digital images and to study its effect. Moreover, to reduce time
complexity, it suffices to estimate the parameter lambda in real-time for large two-dimensional
matrices by merely considering their probability density function as a statistical inference
of the underlying data distribution. We compare the effect of this light-weight Box-Cox transformation
with well-established state-of-the-art low light image enhancement techniques. We also demonstrate
the effectiveness of our approach through several test-bed data sets for generic improvement of
visual appearance of images and for ameliorating the performance of a colour pattern classification
algorithm as an example application. Results with and without the proposed approach, are compared
using the state-of-the art transfer/deep learning which are discussed in the Appendix. To the best
of our knowledge, this is the first time that the Box-Cox transformation is extended to digital images
by exploiting histogram transformation. 