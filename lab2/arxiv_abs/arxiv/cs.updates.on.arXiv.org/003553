Mobile network that millions of people use every day is one of the most complex systems in real world.
Optimization of mobile network to meet exploding customer demand and reduce CAPEX/OPEX poses greater
challenges than in prior works. Actually, learning to solve complex problems in real world to benefit
everyone and make the world better has long been ultimate goal of AI. However, application of deep
reinforcement learning (DRL) to complex problems in real world still remains unsolved, due to imperfect
information, data scarcity and complex rules in real world, potential negative impact to real world,
etc. To bridge this reality gap, we propose a sim-to-real framework to direct transfer learning
from simulation to real world without any training in real world. First, we distill temporal-spatial
relationships between cells and mobile users to scalable 3D image-like tensor to best characterize
partially observed mobile network. Second, inspired by AlphaGo, we introduce a novel self-play
mechanism to empower DRL agents to gradually improve intelligence by competing for best record
on multiple tasks, just like athletes compete for world record in decathlon. Third, a decentralized
DRL method is proposed to coordinate multi-agents to compete and cooperate as a team to maximize
global reward and minimize potential negative impact. Using 7693 unseen test tasks over 160 unseen
mobile networks in another simulator as well as 6 field trials on 4 commercial mobile networks in
real world, we demonstrate the capability of this sim-to-real framework to direct transfer the
learning not only from one simulator to another simulator, but also from simulation to real world.
This is the first time that a DRL agent successfully transfers its learning directly from simulation
to very complex real world problems with imperfect information, complex rules, huge state/action
space, and multi-agent interactions. 