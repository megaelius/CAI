This paper is a survey and an analysis of different ways of using deep learning to generate musical
content. We propose a methodology based on five dimensions: Objective - What musical content is
to be generated? Examples are: melody, polyphony, accompaniment or counterpoint. - For what destination
and for what use? To be performed by a human(s) (in the case of a musical score), or by a machine (in the
case of an audio file). Representation - What are the concepts to be manipulated? Examples are: waveform,
spectrogram, note, chord, meter and beat. - What format is to be used? Examples are: MIDI, piano roll
or text. - How will the representation be encoded? Examples are: scalar, one-hot or many-hot. Architecture
- What type(s) of deep neural network is (are) to be used? Examples are: feedforward network, recurrent
network, autoencoder or generative adversarial networks. Challenges - What are the limitations
and open challenges? Examples are: variability, interactivity and creativity. Strategy - How
do we model and control the process of generation? Examples are: single-step feedforward, iterative
feedforward, sampling or input manipulation. For each dimension, we conduct a comparative analysis
of various models and techniques and propose some tentative multidimensional typology which is
bottom-up, based on the analysis of many existing deep-learning based systems for music generation
selected from the relevant literature. These systems are described and used to exemplify the various
choices of objective, representation, architecture, challenges and strategies. The last part
of the paper includes some discussion and some prospects. This is a simplified version (weak DRM)
of the book: Briot, J.-P., Hadjeres, G. and Pachet, F.-D. (2019) Deep Learning Techniques for Music
Generation, Computational Synthesis and Creative Systems, Springer. 