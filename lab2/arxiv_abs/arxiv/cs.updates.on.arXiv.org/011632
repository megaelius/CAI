Certainty around the regulatory environment is crucial to enable responsible AI innovation and
foster the social acceptance of these powerful new technologies. One notable source of uncertainty
is, however, that the existing legal liability system is inapt to assign responsibility where a
potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations
of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability
of how courts would handle such cases makes the risks involved in the investment and use of AI incalculable,
creating an environment that is not conducive to innovation and may deprive society of some of the
benefits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory
best-practices and establish a system of AI guarantee schemes. We envisage the system to form part
of the broader market-structuring regulatory framework, with the primary function to provide
a readily available, clear, and transparent funding mechanism to compensate claims that are either
extremely hard or impossible to realize via conventional litigation. We propose it to be at least
partially industry-funded, with funding arrangements depending on whether it would pursue other
potential policy goals. We aim to engage in a high-level, comparative conceptual debate around
the suitability of the foreseeability concept to limit legal liability rather than confronting
the intricacies of the case law of specific jurisdictions. Recognizing the importance of the latter
task, we leave this to further research in support of the legal system's incremental adaptation
to the novel challenges of present and future AI technologies. 