Spike-based communication between biological neurons is sparse and unreliable. This enables
the brain to process visual information from the eyes efficiently. Taking inspiration from biology,
artificial spiking neural networks coupled with silicon retinas attempt to model these computations.
Recent findings in machine learning allowed the derivation of a family of powerful synaptic plasticity
rules approximating backpropagation for spiking networks. Are these rules capable of processing
real-world visual sensory data? In this paper, we evaluate the performance of Event-Driven Random
Back-Propagation (eRBP) at learning representations from event streams provided by a Dynamic
Vision Sensor (DVS). First, we show that eRBP matches state-of-the-art performance on the DvsGesture
dataset with the addition of a simple covert attention mechanism. By remapping visual receptive
fields relatively to the center of the motion, this attention mechanism provides translation invariance
at low computational cost compared to convolutions. Second, we successfully integrate eRBP in
a real robotic setup, where a robotic arm grasps objects according to detected visual affordances.
In this setup, visual information is actively sensed by a DVS mounted on a robotic head performing
microsaccadic eye movements. We show that our method classifies affordances within 100ms after
microsaccade onset, which is comparable to human performance reported in behavioral study. Our
results suggest that advances in neuromorphic technology and plasticity rules enable the development
of autonomous robots operating at high speed and low energy consumption. 