Zeroth-order (gradient-free) method is a class of powerful optimization tool for many machine
learning problems because it only needs function values (not gradient) in the optimization. In
particular, zeroth-order method is very suitable for many complex problems such as black-box attacks
and bandit feedback, whose explicit gradients are difficult or infeasible to obtain. Recently,
although many zeroth-order methods have been developed, these approaches still exist two main
drawbacks: 1) high function query complexity; 2) not being well suitable for solving the problems
with complex penalties and constraints. To address these challenging drawbacks, in this paper,
we propose a novel fast zeroth-order stochastic alternating direction method of multipliers (ADMM)
method (\emph{i.e.}, ZO-SPIDER-ADMM) with lower function query complexity for solving nonconvex
problems with multiple nonsmooth penalties. Moreover, we prove that our ZO-SPIDER-ADMM has the
optimal function query complexity of $O(dn + dn^{\frac{1}{2}}\epsilon^{-1})$ for finding an
$\epsilon$-approximate local solution, where $n$ and $d$ denote the sample size and dimension
of data, respectively. In particular, the ZO-SPIDER-ADMM improves the existing best nonconvex
zeroth-order ADMM methods by a factor of $O(d^{\frac{1}{3}}n^{\frac{1}{6}})$. Moreover, we
propose a fast online ZO-SPIDER-ADMM (\emph{i.e.,} ZOO-SPIDER-ADMM). Our theoretical analysis
shows that the ZOO-SPIDER-ADMM has the function query complexity of $O(d\epsilon^{-\frac{3}{2}})$,
which improves the existing best result by a factor of $O(\epsilon^{-\frac{1}{2}})$. Finally,
we utilize a task of structured adversarial attack on black-box deep neural networks to demonstrate
the efficiency of our algorithms. 