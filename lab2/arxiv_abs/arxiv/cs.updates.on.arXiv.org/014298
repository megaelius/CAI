High-fidelity clothing reconstruction is the key to achieving photorealism in a wide range of applications
including human digitization, virtual try-on, etc. Recent advances in learning-based approaches
have accomplished unprecedented accuracy in recovering unclothed human shape and pose from single
images, thanks to the availability of powerful statistical models, e.g. SMPL, learned from a large
number of body scans. In contrast, modeling and recovering clothed human and 3D garments remains
notoriously difficult, mostly due to the lack of large-scale clothing models available for the
research community. We propose to fill this gap by introducing Deep Fashion3D, the largest collection
to date of 3D garment models, with the goal of establishing a novel benchmark and dataset for the evaluation
of image-based garment reconstruction systems. Deep Fashion3D contains 2078 models reconstructed
from real garments, which covers 10 different categories and 563 garment instances. It provides
rich annotations including 3D feature lines, 3D body pose and the corresponded multi-view real
images. In addition, each garment is randomly posed to enhance the variety of real clothing deformations.
To demonstrate the advantage of Deep Fashion3D, we propose a novel baseline approach for single-view
garment reconstruction, which leverages the merits of both mesh and implicit representations.
A novel adaptable template is proposed to enable the learning of all types of clothing in a single
network. Extensive experiments have been conducted on the proposed dataset to verify its significance
and usefulness. We will make Deep Fashion3D publicly available upon publication. 