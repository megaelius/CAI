A widely embraced approach to mitigate the dynamic degradation in low-inertia power systems is
to mimic generation response using grid-connected inverters to restore the grid's stiffness.
In this paper, we seek to challenge this approach and advocate for a principled design based on a systematic
analysis of the performance trade-offs of inverter-based frequency control. With this aim, we
perform a qualitative and quantitative study comparing the effect of conventional control strategies
--droop control (DC) and virtual inertia (VI)-- on several performance metrics induced by $\mathcal
L_2$ and $\mathcal L_\infty$ signal norms. By extending a recently proposed modal decomposition
method, we capture the effect of step and stochastic power disturbances, and frequency measurement
noise, on the overall transient and steady-state behavior of the system. Our analysis unveils several
limitations of these solutions, such as the inability of DC to improve dynamic frequency response
without increasing steady-state control effort, or the large frequency variance that VI introduces
in the presence of measurement noise. We further propose a novel dynam-i-c Droop controller (iDroop)
that overcomes the limitations of DC and VI. More precisely, we show that iDroop can be tuned to achieve
high noise rejection, fast system-wide synchronization, or frequency overshoot (Nadir) elimination
without affecting the steady-state control effort share, and propose a tuning recommendation
that strikes a balance among these objectives. Extensive numerical experimentation shows that
the proposed tuning is effective even when our proportionality assumptions are not valid, and that
the particular tuning used for Nadir elimination strikes a good trade-off among various performance
metrics. 