Small data challenges have emerged in many learning problems, since the success of deep neural networks
often relies on the availability of a huge amount of labeled data that is expensive to collect. To
address it, many efforts have been made on training complex models with small data in an unsupervised
and semi-supervised fashion. In this paper, we will review the recent progresses on these two major
categories of methods. A wide spectrum of small data models will be categorized in a big picture,
where we will show how they interplay with each other to motivate explorations of new ideas. We will
review the criteria of learning the transformation equivariant, disentangled, self-supervised
and semi-supervised representations, which underpin the foundations of recent developments.
Many instantiations of unsupervised and semi-supervised generative models have been developed
on the basis of these criteria, greatly expanding the territory of existing autoencoders, generative
adversarial nets (GANs) and other deep networks by exploring the distribution of unlabeled data
for more powerful representations. While we focus on the unsupervised and semi-supervised methods,
we will also provide a broader review of other emerging topics, from unsupervised and semi-supervised
domain adaptation to the fundamental roles of transformation equivariance and invariance in training
a wide spectrum of deep networks. It is impossible for us to write an exclusive encyclopedia to include
all related works. Instead, we aim at exploring the main ideas, principles and methods in this area
to reveal where we are heading on the journey towards addressing the small data challenges in this
big data era. 