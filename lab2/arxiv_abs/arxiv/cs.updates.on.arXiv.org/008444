Purpose: A profound education of novice surgeons is crucial to ensure that surgical interventions
are effective and safe. One important aspect is the teaching of technical skills for minimally invasive
or robot-assisted procedures. This includes the objective and preferably automatic assessment
of surgical skill. Recent studies presented good results for automatic, objective skill evaluation
by collecting and analyzing motion data such as trajectories of surgical instruments. However,
obtaining the motion data generally requires additional equipment for instrument tracking or
the availability of a robotic surgery system to capture kinematic data. In contrast, we investigate
a method for automatic, objective skill assessment that requires video data only. This has the advantage
that video can be collected effortlessly during minimally invasive and robot-assisted training
scenarios. Methods: Our method builds on recent advances in deep learning-based video classification.
Specifically, we propose to use an inflated 3D ConvNet to classify snippets, i.e., stacks of a few
consecutive frames, extracted from surgical video. The network is extended into a Temporal Segment
Network during training. Results: We evaluate the method on the publicly available JIGSAWS dataset,
which consists of recordings of basic robot-assisted surgery tasks performed on a dry lab bench-top
model. Our approach achieves high skill classification accuracies ranging from 95.1% to 100.0%.
Conclusions: Our results demonstrate the feasibility of deep learning-based assessment of technical
skill from surgical video. Notably, the 3D ConvNet is able to learn meaningful patterns directly
from the data, alleviating the need for manual feature engineering. Further evaluation will require
more annotated data for training and testing. 