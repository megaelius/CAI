Recently, artificial neural networks (ANNs) in conjunction with stochastic gradient descent
optimization methods have been employed to approximately compute solutions of possibly rather
high-dimensional partial differential equations (PDEs). Very recently, there have also been
a number of rigorous mathematical results in the scientific literature which examine the approximation
capabilities of such deep learning based approximation algorithms for PDEs. These mathematical
results from the scientific literature prove in part that algorithms based on ANNs are capable of
overcoming the curse of dimensionality in the numerical approximation of high-dimensional PDEs.
In these mathematical results from the scientific literature usually the error between the solution
of the PDE and the approximating ANN is measured in the $L^p$-sense with respect to some $p \in [1,\infty)$
and some probability measure. In many applications it is, however, also important to control the
error in a uniform $L^\infty$-sense. The key contribution of the main result of this article is to
develop the techniques to obtain error estimates between solutions of PDEs and approximating ANNs
in the uniform $L^\infty$-sense. In particular, we prove that the number of parameters of an ANN
to uniformly approximate the classical solution of the heat equation in a region $ [a,b]^d $ for a
fixed time point $ T \in (0,\infty) $ grows at most polynomially in the dimension $ d \in \mathbb{N}
$ and the reciprocal of the approximation precision $ \varepsilon > 0 $. This shows that ANNs can overcome
the curse of dimensionality in the numerical approximation of the heat equation when the error is
measured in the uniform $L^\infty$-norm. 