Despite much success, deep learning generally does not perform well with small labeled training
sets. In these scenarios, data augmentation has shown much promise in alleviating the need for more
labeled data, but it so far has mostly been applied in supervised settings and achieved limited gains.
In this work, we propose to apply data augmentation to unlabeled data in a semi-supervised learning
setting. Our method, named Unsupervised Data Augmentation or UDA, encourages the model predictions
to be consistent between an unlabeled example and an augmented unlabeled example. Unlike previous
methods that use random noise such as Gaussian noise or dropout noise, UDA has a small twist in that
it makes use of harder and more realistic noise generated by state-of-the-art data augmentation
methods. This small twist leads to substantial improvements on six language tasks and three vision
tasks even when the labeled set is extremely small. For example, on the IMDb text classification
dataset, with only 20 labeled examples, UDA achieves an error rate of 4.20, outperforming the state-of-the-art
model trained on 25,000 labeled examples. On standard semi-supervised learning benchmarks CIFAR-10
and SVHN, UDA outperforms all previous approaches and achieves an error rate of 2.7% on CIFAR-10
with only 4,000 examples and an error rate of 2.85% on SVHN with only 250 examples, nearly matching
the performance of models trained on the full sets which are one or two orders of magnitude larger.
UDA also works well on large-scale datasets such as ImageNet. When trained with 10% of the labeled
set, UDA improves the top-1/top-5 accuracy from 55.1/77.3% to 68.7/88.5%. For the full ImageNet
with 1.3M extra unlabeled data, UDA further pushes the performance from 78.3/94.4% to 79.0/94.5%.
