Wireless powered mobile-edge computing (MEC) has recently emerged as a promising paradigm to enhance
the data processing capability of low-power networks, such as wireless sensor networks and internet
of things (IoT). In this paper, we consider a wireless powered MEC network that adopts a binary offloading
policy, so that each computation task of wireless devices (WDs) is either executed locally or fully
offloaded to an MEC server. Our goal is to acquire an online algorithm that optimally adapts task
offloading decisions and wireless resource allocations to the time-varying wireless channel
conditions. This requires quickly solving hard combinatorial optimization problems within the
channel coherence time, which is hardly achievable with conventional numerical optimization
methods. To tackle this problem, we propose a Deep Reinforcement learning-based Online Offloading
(DROO) framework that implements a deep neural network as a scalable solution that learns the binary
offloading decisions from the experience. It eliminates the need of solving combinatorial optimization
problems, and thus greatly reduces the computational complexity especially in large-size networks.
To further reduce the complexity, we propose an adaptive procedure that automatically adjusts
the parameters of the DROO algorithm on the fly. Numerical results show that the proposed algorithm
can achieve near-optimal performance while significantly decreasing the computation time by
more than an order of magnitude compared with existing optimization methods. For example, the CPU
execution latency of DROO is less than $0.1$ second in a $30$-user network, making real-time and
optimal offloading truly viable even in a fast fading environment. 