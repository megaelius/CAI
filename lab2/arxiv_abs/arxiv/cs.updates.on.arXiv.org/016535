Recognizing an object's category and pose lies at the heart of visual understanding. Recent works
suggest that deep neural networks (DNNs) often fail to generalize to category-pose combinations
not seen during training. However, it is unclear when and how such generalization may be possible.
Does the number of combinations seen during training impact generalization? Is it better to learn
category and pose in separate networks, or in a single shared network? Furthermore, what are the
neural mechanisms that drive the network's generalization? In this paper, we answer these questions
by analyzing state-of-the-art DNNs trained to recognize both object category and pose (position,
scale, and 3D viewpoint) with quantitative control over the number of category-pose combinations
seen during training. We also investigate the emergence of two types of specialized neurons that
can explain generalization to unseen combinations---neurons selective to category and invariant
to pose, and vice versa. We perform experiments on MNIST extended with position or scale, the iLab
dataset with vehicles at different viewpoints, and a challenging new dataset for car model recognition
and viewpoint estimation that we introduce in this paper, the Biased-Cars dataset. Our results
demonstrate that as the number of combinations seen during training increases, networks generalize
better to unseen category-pose combinations, facilitated by an increase in the selectivity and
invariance of individual neurons. We find that learning category and pose in separate networks
compared to a shared one leads to an increase in such selectivity and invariance, as separate networks
are not forced to preserve information about both category and pose. This enables separate networks
to significantly outperform shared ones at predicting unseen category-pose combinations. 