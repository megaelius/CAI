Graph-based semi-supervised learning has been shown to be one of the most effective approaches
for classification tasks from a wide range of domains, such as image classification and text classification,
as they can exploit the connectivity patterns between labeled and unlabeled samples to improve
learning performance. In this work, we advance this effective learning paradigm towards a scenario
where labeled data are severely limited. More specifically, we address the problem of graph-based
semi-supervised learning in the presence of severely limited labeled samples, and propose a new
framework, called {\em Shoestring}, that improves the learning performance through semantic
transfer from these very few labeled samples to large numbers of unlabeled samples. In particular,
our framework learns a metric space in which classification can be performed by computing the similarity
to centroid embedding of each class. {\em Shoestring} is trained in an end-to-end fashion to learn
to leverage the semantic knowledge of limited labeled samples as well as their connectivity patterns
with large numbers of unlabeled samples simultaneously. By combining {\em Shoestring} with graph
convolutional networks, label propagation and their recent label-efficient variations (IGCN
and GLP), we are able to achieve state-of-the-art node classification performance in the presence
of very few labeled samples. In addition, we demonstrate the effectiveness of our framework on image
classification tasks in the few-shot learning regime, with significant gains on miniImageNet
($2.57\%\sim3.59\%$) and tieredImageNet ($1.05\%\sim2.70\%$). 