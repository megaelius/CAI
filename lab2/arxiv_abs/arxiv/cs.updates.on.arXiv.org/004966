We consider the problem of learning a linear subspace from data corrupted by outliers. Classical
approaches are typically designed for the case in which the subspace dimension is small relative
to the ambient dimension. Our approach works with a dual representation of the subspace and hence
aims to find its orthogonal complement; as such, it is particularly suitable for subspaces whose
dimension is close to the ambient dimension (subspaces of high relative dimension). We pose the
problem of computing normal vectors to the inlier subspace as a non-convex $\ell_1$ minimization
problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem. We provide
theoretical guarantees under which every global solution to DPCP is a vector in the orthogonal complement
of the inlier subspace. Moreover, we relax the non-convex DPCP problem to a recursion of linear programs
whose solutions are shown to converge in a finite number of steps to a vector orthogonal to the subspace.
In particular, when the inlier subspace is a hyperplane, the solutions to the recursion of linear
programs converge to the global minimum of the non-convex DPCP problem in a finite number of steps.
We also propose algorithms based on alternating minimization and iteratively re-weighted least
squares, which are suitable for dealing with large-scale data. Experiments on synthetic data show
that the proposed methods are able to handle more outliers and higher relative dimensions than current
state-of-the-art methods, while experiments in the context of the three-view geometry problem
in computer vision suggest that the proposed methods can be a useful or even superior alternative
to traditional RANSAC-based approaches for computer vision and other applications. 