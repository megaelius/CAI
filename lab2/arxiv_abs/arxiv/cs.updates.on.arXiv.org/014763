Recent studies show that deep neural networks are vulnerable to adversarial examples which can
be generated via certain types of transformations. Being robust to a desired family of adversarial
attacks is then equivalent to being invariant to a family of transformations. Learning invariant
representations then naturally emerges as an important goal to achieve which we explore in this
paper within specific application contexts. Specifically, we propose a cyclically-trained adversarial
network to learn a mapping from image space to latent representation space and back such that the
latent representation is invariant to a specified factor of variation (e.g., identity). The learned
mapping assures that the synthesized image is not only realistic, but has the same values for unspecified
factors (e.g., pose and illumination) as the original image and a desired value of the specified
factor. Unlike disentangled representation learning, which requires two latent spaces, one for
specified and another for unspecified factors, invariant representation learning needs only
one such space. We encourage invariance to a specified factor by applying adversarial training
using a variational autoencoder in the image space as opposed to the latent space. We strengthen
this invariance by introducing a cyclic training process (forward and backward cycle). We also
propose a new method to evaluate conditional generative networks. It compares how well different
factors of variation can be predicted from the synthesized, as opposed to real, images. In quantitative
terms, our approach attains state-of-the-art performance in experiments spanning three datasets
with factors such as identity, pose, illumination or style. Our method produces sharp, high-quality
synthetic images with little visible artefacts compared to previous approaches. 