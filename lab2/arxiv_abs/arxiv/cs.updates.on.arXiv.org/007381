Purpose: A profound education of novice surgeons is crucial to ensure that surgical interventions
are effective and safe. One important aspect is the teaching of technical skills for minimally invasive
or robot-assisted procedures. This includes the objective and preferably automatic assessment
of surgical skill. Recent studies presented good results for automatic, objective skill evaluation
by collecting and analyzing motion data such as trajectories of surgical instruments. However,
obtaining the motion data generally requires additional equipment for instrument tracking or
the availability of a robotic surgery system to capture kinematic data. In contrast, we investigate
a method for automatic, objective skill assessment that requires video data only. This has the advantage
that video can be collected effortlessly during minimally invasive and robot-assisted training
scenarios. Methods: Our method builds on recent advances in deep learning-based video classification.
Specifically, we propose to use an inflated 3D ConvNet to classify snippets of optical flow extracted
from surgical video. The network is extended into a Temporal Segment Network during training. Results:
On the publicly available JIGSAWS dataset, our approach achieves high skill classification accuracies
ranging from 95.1% to 100.0%. Conclusions: Our results demonstrate the feasibility of deep learning-based
assessment of technical skill from surgical video. The 3D ConvNet is able to learn meaningful patterns
directly from the data, alleviating the need for manual feature engineering. Further evaluation
will require more annotated data for training and testing. 