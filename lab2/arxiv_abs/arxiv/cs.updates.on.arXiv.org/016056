Lecture videos are an increasingly important learning resource for higher education. However,
the challenge of quickly finding the content of interest in a lecture video is an important limitation
of this format. This paper introduces visual summarization of lecture video segments to enhance
navigation. A lecture video is divided into segments based on the frame-to-frame similarity of
content. The user navigates the lecture video content by viewing a single frame visual and textual
summary of each segment. The paper presents a novel methodology to generate the visual summary of
a lecture video segment by computing similarities between images extracted from the segment and
employing a graph-based algorithm to identify the subset of most representative images. The results
from this research are integrated into a real-world lecture video management portal called Videopoints.
To collect ground truth for evaluation, a survey was conducted where multiple users manually provided
visual summaries for 40 lecture video segments. The users also stated whether any images were not
selected for the summary because they were similar to other selected images. The graph based algorithm
for identifying summary images achieves 78% precision and 72% F1-measure with frequently selected
images as the ground truth, and 94% precision and 72% F1-measure with the union of all user selected
images as the ground truth. For 98% of algorithm selected visual summary images, at least one user
also selected that image for their summary or considered it similar to another image they selected.
Over 65% of automatically generated summaries were rated as good or very good by the users on a 4-point
scale from poor to very good. Overall, the results establish that the methodology introduced in
this paper produces good quality visual summaries that are practically useful for lecture video
navigation. 