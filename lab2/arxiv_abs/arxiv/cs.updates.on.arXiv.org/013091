State-of-the-art performance and low system complexity has made deep-learning an increasingly
attractive solution for big data analytics. However, limiting assumptions of end-to-end learning
regimes hinder the use of neural networks on large application-grade datasets. This work addresses
the assumption that output class-labels are defined for all classes in the domain. The amount of
data collected by modern-day sensors span over an incomprehensible range of potential classes.
Therefore, we propose a new learning regime where only some, but not all, classes of the training
data are of interest to the classification system. The semi-supervised learning scenario in big
data requires the assumption of a partial class mismatch between labelled and unlabelled training
data. With classification systems required to classify source classes indicated by labelled samples
while separating novel classes indicated by unlabelled samples, we find ourselves in an open-set
case (vs closed set with only source classes). However, introducing samples from novel classes
into the training set indicates a more relaxed open-set case. As such, our proposed regime of \textit{quasi-open
set semi-supervised learning} is introduced. We propose a suitable method to train under quasi-open
set semi-supervised learning that makes use of Wasserstein generative adversarial networks (WGANs).
A trained classification certainty estimation within the discriminator (or critic) network is
used to enable a reject option for the classifier. By placing a threshold on this certainty estimation,
the reject option accepts classifications of source classes and rejects novel classes. Big data
end-to-end training is promoted by developing models that recognize input samples do not necessarily
belong to output labels. We believe this essential for big data analytics, and urge more work under
quasi-open set semi-supervised learning. 