We describe a framework of hybrid cognition by formulating a hybrid cognitive agent that performs
hierarchical active inference across a human and a machine part. We suggest that, in addition to
enhancing human cognitive functions with an intelligent and adaptive interface, integrated cognitive
processing could accelerate emergent properties within artificial intelligence. To establish
this, a machine learning part learns to integrate into human cognition by explaining away multi-modal
sensory measurements from the environment and physiology simultaneously with the brain signal.
With ongoing training, the amount of predictable brain signal increases. This lends the agent the
ability to self-supervise on increasingly high levels of cognitive processing in order to further
minimize surprise in predicting the brain signal. Furthermore, with increasing level of integration,
the access to sensory information about environment and physiology is substituted with access
to their representation in the brain. While integrating into a joint embodiment of human and machine,
human action and perception are treated as the machine's own. The framework can be implemented with
invasive as well as non-invasive sensors for environment, body and brain interfacing. Online and
offline training with different machine learning approaches are thinkable. Building on previous
research on shared representation learning, we suggest a first implementation leading towards
hybrid active inference with non-invasive brain interfacing and state of the art probabilistic
deep learning methods. We further discuss how implementation might have effect on the meta-cognitive
abilities of the described agent and suggest that with adequate implementation the machine part
can continue to execute and build upon the learned cognitive processes autonomously. 