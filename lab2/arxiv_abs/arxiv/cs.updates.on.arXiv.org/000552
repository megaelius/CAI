Software defect prediction plays a crucial role in estimating the most defect-prone components
of software, and a large number of studies have been pursued to improve prediction accuracy within
a project or across projects. However, the rules for making an appropriate decision between within-
and cross-project defect prediction when historical data available is insufficient remain unclear.
The objective of this work is to validate the feasibility of the predictor built with simplified
metric set for software defect prediction in different scenarios, and to investigate practical
guidelines for the choice of training data, classifier and metric subset of a given project. First
of all, based on six typical classifiers, we constructed three types of predictors by means of the
size of software metric set in three scenarios. Then, we validated the acceptable performance of
the predictor based on Top-k metrics in terms of statistical methods. Finally, we attempted to minimize
the Top-k metric subset by removing redundant metrics, and tested the stability of such a minimum
metric subset with one-way ANOVA test. The experimental results indicate that (1) the choice of
training data should depend on the specific requirement of prediction accuracy, (2) the predictor
built with simplified metric set works well and is very useful in case limited resources are supplied,
(3) simple classifier (e.g., Naive Bayes) also tends to perform well when using simplified metric
set for defect prediction, and (4) minimum metric subset, in several cases, can be identified to
facilitate the procedure of general defect prediction with loss of prediction precision in practice.
The guideline for choosing a suitable simplified metric set in different scenarios is presented
in Table 10. 