Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government
have turned to machine learning with hopes of operationalizing predictions to drive automated
decisions. Unfortunately, many social desiderata concerning consequential decisions, such
as justice or fairness, have no natural formulation within a purely predictive framework. In efforts
to mitigate these problems, researchers have proposed a variety of metrics for quantifying deviations
from various statistical parities that we might expect to observe in a fair world and offered a variety
of algorithms in attempts to satisfy subsets of these parities or to trade off the degree to which
they are satisfied against utility. In this paper, we connect this approach to \emph{fair machine
learning} to the literature on ideal and non-ideal methodological approaches in political philosophy.
The ideal approach requires positing the principles according to which a just world would operate.
In the most straightforward application of ideal theory, one supports a proposed policy by arguing
that it closes a discrepancy between the real and the perfectly just world. However, by failing to
account for the mechanisms by which our non-ideal world arose, the responsibilities of various
decision-makers, and the impacts of proposed policies, naive applications of ideal thinking can
lead to misguided interventions. In this paper, we demonstrate a connection between the fair machine
learning literature and the ideal approach in political philosophy, and argue that the increasingly
apparent shortcomings of proposed fair machine learning algorithms reflect broader troubles
faced by the ideal approach. We conclude with a critical discussion of the harms of misguided solutions,
a reinterpretation of impossibility results, and directions for future research. 