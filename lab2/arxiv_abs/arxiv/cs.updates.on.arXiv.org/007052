We continue the study of computational limitations in learning robust classifiers, following
the recent work of Bubeck, Lee, Price and Razenshteyn. First, we demonstrate classification tasks
where computationally efficient robust classifiers do not exist, even when computationally unbounded
robust classifiers do. We rely on the hardness of decoding problems with preprocessing on codes
and lattices. Second, we show classification tasks where efficient robust classifiers exist,
but they are computationally hard to learn. Bubeck et al. showed examples of such tasks in the small-perturbation
regime where the robust classifier can recover from a constant number of perturbed bits. Indeed,
as we observe, the question of whether a large-perturbation robust classifier for their task exists
is related to important open questions in computational number theory. We show two such classification
tasks in the large-perturbation regime: the first relies on the existence of one-way functions,
a minimal assumption in cryptography; and the second on the hardness of the learning parity with
noise problem. For the second task, not only does a non-robust classifier exist, but also an efficient
algorithm that generates fresh new labeled samples given access to polynomially many training
examples (termed as generation by Kearns et. al. (1994)). Third, we show that any such task implies
the existence of cryptographic primitives such as one-way functions or even forms of public-key
encryption. This leads us to a win-win scenario: either we can quickly learn an efficient robust
classifier (assuming one exists), or we can construct new instances of popular and useful cryptographic
primitives. 