Walter Huang, a 38-year-old Apple Inc. engineer, died on March 23, 2018, after his Tesla Model X crashed
into a highway barrier in Mountain View, California. Tesla immediately disavowed responsibility
for the accident. "The fundamental premise of both moral and legal liability is a broken promise,
and there was none here: [Mr. Huang] was well aware that the Autopilot was not perfect [and the] only
way for this accident to have occurred is if Mr. Huang was not paying attention to the road, despite
the car providing multiple warnings to do so." This is the standard response from Tesla and Uber,
the manufacturers of the automated vehicles involved in the six fatal accidents to date: the automated
vehicle isn't perfect, the driver knew it wasn't perfect, and if only the driver had been paying attention
and heeded the vehicle's warnings, the accident would never have occurred. However, as researchers
focused on human-automation interaction in aviation and military operations, we cannot help but
wonder if there really are no broken promises and no legal liabilities. Science has a critical role
in determining legal liability, and courts appropriately rely on scientists and engineers to determine
whether an accident, or harm, was foreseeable. Specifically, a designer could be found liable if,
at the time of the accident, scientists knew there was a systematic relationship between the accident
and the designer's untaken precaution. Nearly 70 years of research provides an undeniable answer:
It is insufficient, inappropriate, and dangerous to automate everything you can and leave the rest
to the human. There is a systematic relationship between the design of automated vehicles and the
types of accidents that are occurring now and will inevitably continue to occur in the future. These
accidents were not unforeseeable and the drivers were not exclusively to blame. 