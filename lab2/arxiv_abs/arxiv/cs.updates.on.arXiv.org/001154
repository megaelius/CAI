Tuning curves characterizing the response selectivities of biological neurons often exhibit
large degrees of irregularity and diversity across neurons. Theoretical network models that feature
heterogeneous cell populations or random connectivity also generate diverse tuning behaviors
across cells. However, a general framework for fitting such models to experimentally measured
tuning curves is lacking. We address this problem by proposing to view mechanistic network models
as generative models whose parameters can be optimized to fit the distribution of experimentally
measured tuning curves. A major obstacle for fitting such models is that, in most cases of interest,
an explicit form for the likelihood function, i.e. the distribution of generated tuning curves,
is not available. Recent advances in machine learning provide ways for fitting generative models
without the need to evaluate the likelihood and its gradient. Generative Adversarial Networks
(GAN) provide one such framework, and have been successful in traditional machine learning tasks.
We apply this approach in two separate experiments, showing how GANs can be used to fit commonly used
mechanistic models in theoretical neuroscience to datasets of measured tuning curves. This approach
avoids the computationally expensive step of inferring latent variables, e.g. the biophysical
parameters of individual cells or the particular realization of the full synaptic connectivity
matrix, and directly learns model parameters which characterize the statistics of connectivity
or single-cell properties. Another strength of this approach is that it fits the entire, joint distribution
of experimental tuning curves, instead of matching a few summary statistics picked by the user.
This framework opens the door to fitting theoretically motivated dynamical network models directly
to simultaneously or non-simultaneously recorded neural responses. 