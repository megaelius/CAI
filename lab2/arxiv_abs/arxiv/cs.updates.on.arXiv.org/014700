The big problem for neural network models which are trained to count instances is that whenever test
range goes high training range generalization error increases i.e. they are not good generalizers
outside training range. Consider the case of automating cell counting process where more dense
images with higher cell counts are commonly encountered as compared to images used in training data.
By making better predictions for higher ranges of cell count we are aiming to create better generalization
systems for cell counting. With architecture proposal of neural arithmetic logic units (NALU)
for arithmetic operations, task of counting has become feasible for higher numeric ranges which
were not included in training data with better accuracy. As a part of our study we used these units
and different other activation functions for learning cell counting task with two different architectures
namely Fully Convolutional Regression Network and U-Net. These numerically biased units are added
in the form of residual concatenated layers to original architectures and a comparative experimental
study is done with these newly proposed changes . This comparative study is described in terms of
optimizing regression loss problem from these models trained with extensive data augmentation
techniques. We were able to achieve better results in our experiments of cell counting tasks with
introduction of these numerically biased units to already existing architectures in the form of
residual layer concatenation connections. Our results confirm that above stated numerically
biased units does help models to learn numeric quantities for better generalization results. 