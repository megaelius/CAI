Progress in generative modelling, especially generative adversarial networks, have made it possible
to efficiently synthesize and alter media at scale. Malicious individuals now rely on these machine-generated
media, or deepfakes, to manipulate social discourse. In order to ensure media authenticity, existing
research is focused on deepfake detection. Yet, the very nature of frameworks used for generative
modeling suggests that progress towards detecting deepfakes will enable more realistic deepfake
generation. Therefore, it comes at no surprise that developers of generative models are under the
scrutiny of stakeholders dealing with misinformation campaigns. As such, there is a clear need
to develop tools that ensure the transparent use of generative modeling, while minimizing the harm
caused by malicious applications. We propose a framework to provide developers of generative models
with plausible deniability. We introduce two techniques to provide evidence that a model developer
did not produce media that they are being accused of. The first optimizes over the source of entropy
of each generative model to probabilistically attribute a deepfake to one of the models. The second
involves cryptography to maintain a tamper-proof and publicly-broadcasted record of all legitimate
uses of the model. We evaluate our approaches on the seminal example of face synthesis, demonstrating
that our first approach achieves 97.62% attribution accuracy, and is less sensitive to perturbations
and adversarial examples. In cases where a machine learning approach is unable to provide plausible
deniability, we find that involving cryptography as done in our second approach is required. We
also discuss the ethical implications of our work, and highlight that a more meaningful legislative
framework is required for a more transparent and ethical use of generative modeling. 