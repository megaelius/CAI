Many methods have been developed to help people find the video contents they want efficiently. However,
there are still some unsolved problems in this area. For example, given a query video and a reference
video, how to accurately localize a segment in the reference video such that the segment semantically
corresponds to the query video? We define a distinctively new task, namely \textbf{video re-localization},
to address this scenario. Video re-localization is an important emerging technology implicating
many applications, such as fast seeking in videos, video copy detection, video surveillance, etc.
Meanwhile, it is also a challenging research task because the visual appearance of a semantic concept
in videos can have large variations. The first hurdle to clear for the video re-localization task
is the lack of existing datasets. It is labor expensive to collect pairs of videos with semantic coherence
or correspondence and label the corresponding segments. We first exploit and reorganize the videos
in ActivityNet to form a new dataset for video re-localization research, which consists of about
10,000 videos of diverse visual appearances associated with localized boundary information.
Subsequently, we propose an innovative cross gated bilinear matching model such that every time-step
in the reference video is matched against the attentively weighted query video. Consequently,
the prediction of the starting and ending time is formulated as a classification problem based on
the matching results. Extensive experimental results show that the proposed method outperforms
the competing methods. Our code is available at: https://github.com/fengyang0317/video_reloc.
