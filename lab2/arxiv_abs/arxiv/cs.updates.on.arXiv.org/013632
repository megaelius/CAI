Partial Label Learning (PLL) aims to train a classifier when each training instance is associated
with a set of candidate labels, among which only one is correct but is not accessible during the training
phase. The common strategy dealing with such ambiguous labeling information is to disambiguate
the candidate label sets. Nonetheless, existing methods ignore the disambiguation difficulty
of instances and adopt the single-trend training mechanism. The former would lead to the vulnerability
of models to the false positive labels and the latter may arouse error accumulation problem. To remedy
these two drawbacks, this paper proposes a novel approach termed "Network Cooperation with Progressive
Disambiguation" (NCPD) for PLL. Specifically, we devise a progressive disambiguation strategy
of which the disambiguation operations are performed on simple instances firstly and then gradually
on more complicated ones. Therefore, the negative impacts brought by the false positive labels
of complicated instances can be effectively mitigated as the disambiguation ability of the model
has been strengthened via learning from the simple instances. Moreover, by employing artificial
neural networks as the backbone, we utilize a network cooperation mechanism which trains two networks
collaboratively by letting them interact with each other. As two networks have different disambiguation
ability, such interaction is beneficial for both networks to reduce their respective disambiguation
errors, and thus is much better than the existing algorithms with single-trend training process.
Extensive experimental results on various benchmark and practical datasets demonstrate the superiority
of our NCPD to other state-of-the-art PLL methods. 