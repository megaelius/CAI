With an aim to increase the capture range and accelerate the performance of state-of-the-art inter-subject
and subject-to-template 3D registration, we propose deep learning-based methods that are trained
to find the 3D position of arbitrarily oriented subjects or anatomy based on slices or volumes of
medical images. For this, we propose regression CNNs that learn to predict the angle-axis representation
of 3D rotations and translations using image features. We use and compare mean square error and geodesic
loss for training regression CNNs in two different scenarios: 3D pose estimation from slices and
3D to 3D registration. As an exemplary application, we applied the proposed methods to register
arbitrarily oriented reconstructed images of fetuses scanned in-utero at a wide gestational age
range to a standard atlas space. Our results show that in such registration applications that are
amendable to learning, the proposed deep learning methods with geodesic loss minimization can
achieve accurate results with a wide capture range in real-time (<100ms). We tested the generalization
capability of the trained CNNs on an expanded age range and on images of newborn subjects with similar
and different MR image contrasts. We trained our models on T2-weighted fetal brain MRI scans and
used them to predict the 3D position of newborn brains based on T1-weighted MRI scans. We showed that
trained models generalized well for the new domain when we performed image contrast transfer through
a conditional generative adversarial network. This indicates that the domain of application of
the trained deep regression CNNs can be further expanded to image modalities and contrasts other
than those used in training. A combination of our proposed methods with optimization-based registration
algorithms can dramatically enhance the performance of automatic imaging devices and image processing
methods of the future. 