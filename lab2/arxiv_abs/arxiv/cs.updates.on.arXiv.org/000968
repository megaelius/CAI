In this paper we formulate the framework of recovering a hidden orthonormal basis given access to
a certain "Basis Encoding Function". We describe the class of Basis Encoding Functions (BEF), such
that their local maxima on the unit sphere are in one-to-one correspondence with the basis elements.
This description relies on a certain "hidden convexity" property of these functions. A number of
theoretical and practical problems of recent interest can be interpreted as recovering a hidden
basis from potentially noisy observations. Specifically, we show how our simple and general framework
applies to Independent Component Analysis (ICA), tensor decompositions, spectral clustering
and Gaussian mixture learning. We describe a new algorithm, "gradient iteration", for provable
recovery of the hidden basis. We provide a complete theoretical analysis of Gradient Iteration
both for the exact case as well as for the case when the observed function is a perturbation of the "true"
underlying BEF. In both cases we show convergence and complexity bounds polynomial in dimension
and other relevant parameters, such as perturbation size. Our perturbation results can be considered
as a very general non-linear version of the classical Davis-Kahan theorem for eigenvectors of perturbations
of symmetric matrices. In addition we show that in the exact case the algorithm converges superlinearly
and give conditions relating the degree of convergence to properties of the Basis Encoding Function.
Our algorithm can be viewed as a generalization of the classical power iteration method for eigenanalysis
of symmetric matrices as well as a generalization of power iterations for tensors. Moreover, the
Gradient Iteration algorithm can be easily and efficiently implemented in practice. 