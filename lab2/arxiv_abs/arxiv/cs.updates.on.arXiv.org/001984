We consider generalized linear models (GLMs) where an unknown $n$-dimensional signal vector is
observed through the application of a random matrix and a non-linear (possibly probabilistic)
componentwise output function. We consider the models in the high-dimensional limit, where the
observation consists of m points, and $m/n\to\alpha$ where $\alpha$ stays finite in the limit $m,n\to\infty$.
This situation is ubiquitous in applications ranging from supervised machine learning to signal
processing. A substantial amount of theoretical work analyzed the model-case when the observation
matrix has i.i.d. elements and the components of the ground-truth signal are taken independently
from some known distribution. While statistical physics provided number of explicit conjectures
for special cases of this model, results existing for non-linear output functions were so far non-rigorous.
At the same time GLMs with non-linear output functions are used as a basic building block of powerful
multilayer feedforward neural networks. Therefore rigorously establishing the formulas conjectured
for the mutual information is a key open problem that we solve in this paper. We also provide an explicit
asymptotic formula for the optimal generalization error, and confirm the prediction of phase transitions
in GLMs. Analyzing the resulting formulas for several non-linear output functions, including
the rectified linear unit or modulus functions, we obtain quantitative descriptions of information-theoretic
limitations of high-dimensional inference. Our proof technique relies on a new version of the interpolation
method with an adaptive interpolation path and is of independent interest. Furthermore we show
that a polynomial-time algorithm referred to as generalized approximate message-passing reaches
the optimal generalization error for a large set of parameters. 