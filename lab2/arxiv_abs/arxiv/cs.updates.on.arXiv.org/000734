Semantic composition is the task of understanding the meaning of text by composing the meanings
of the individual words in the text. Semantic decomposition is the task of understanding the meaning
of an individual word by decomposing it into various aspects (factors, constituents, components)
that are latent in the meaning of the word. We take a distributional approach to semantics, in which
a word is represented by a context vector. Much recent work has considered the problem of recognizing
compositions and decompositions, but we tackle the more difficult generation problem. For simplicity,
we focus on noun-modifier bigrams and noun unigrams. A test for semantic composition is, given context
vectors for the noun and modifier in a noun-modifier bigram ("red salmon"), generate a noun unigram
that is synonymous with the given bigram ("sockeye"). A test for semantic decomposition is, given
a context vector for a noun unigram ("snifter"), generate a noun-modifier bigram that is synonymous
with the given unigram ("brandy glass"). With a vocabulary of about 73,000 unigrams from WordNet,
there are 73,000 candidate unigram compositions for a bigram and 5,300,000,000 (73,000 squared)
candidate bigram decompositions for a unigram. We generate ranked lists of potential solutions
in two passes. A fast unsupervised learning algorithm generates an initial list of candidates and
then a slower supervised learning algorithm refines the list. We evaluate the candidate solutions
by comparing them to WordNet synonym sets. For decomposition (unigram to bigram), the top 100 most
highly ranked bigrams include a WordNet synonym of the given unigram 50.7% of the time. For composition
(bigram to unigram), the top 100 most highly ranked unigrams include a WordNet synonym of the given
bigram 77.8% of the time. 