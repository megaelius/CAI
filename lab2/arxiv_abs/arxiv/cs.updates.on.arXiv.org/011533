LSTMs promise much to financial time-series analysis, temporal and cross-sectional inference,
but we find they do not deliver in a real-world financial management task. We examine an alternative
called Continual Learning (CL), a memory-augmented approach, which can provide transparent explanations;
which memory did what and when. This work has implications for many financial applications including
to credit, time-varying fairness in decision making and more. We make three important new observations.
Firstly, as well as being more explainable, time-series CL approaches outperform LSTM and a simple
sliding window learner (feed-forward neural net (FFNN)). Secondly, we show that CL based on a sliding
window learner (FFNN) is more effective than CL based on a sequential learner (LSTM). Thirdly, we
examine how real-world, time-series noise impacts several similarity approaches used in CL memory
addressing. We provide these insights using an approach called Continual Learning Augmentation
(CLA) tested on a complex real world problem; emerging market equities investment decision making.
CLA provides a test-bed as it can be based on different types of time-series learner, allowing testing
of LSTM and sliding window (FFNN) learners side by side. CLA is also used to test several distance
approaches used in a memory recall-gate: euclidean distance (ED), dynamic time warping (DTW),
auto-encoder (AE) and a novel hybrid approach, warp-AE. We find CLA out-performs simple LSTM and
FFNN learners and CLA based on a sliding window (CLA-FFNN) out-performs a LSTM (CLA-LSTM) implementation.
While for memory-addressing, ED under-performs DTW and AE but warp-AE shows the best overall performance
in a real-world financial task. 