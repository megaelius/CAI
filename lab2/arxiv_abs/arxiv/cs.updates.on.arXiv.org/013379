Even though convolutional neural networks (CNNs) are driving progress in medical image segmentation,
standard models still have some drawbacks. First, the use of multi-scale approaches, i.e., encoder-decoder
architectures, leads to a redundant use of information, where similar low-level features are extracted
multiple times at multiple scales. Second, long-range feature dependencies are not efficiently
modeled, resulting in non-optimal discriminative feature representations associated with each
semantic class. In this paper we attempt to overcome these limitations with the proposed architecture,
by capturing richer contextual dependencies based on the use of guided self-attention mechanisms.
This approach is able to integrate local features with their corresponding global dependencies,
as well as highlight interdependent channel maps in an adaptive manner. Further, the additional
loss between different modules guides the attention mechanisms to neglect irrelevant information
and focus on more discriminant regions of the image by emphasizing relevant feature associations.
We evaluate the proposed model in the context of semantic segmentation on three different datasets:
abdominal organs, cardiovascular structures and brain tumors. A series of ablation experiments
support the importance of these attention modules in the proposed architecture. In addition, compared
to other state-of-the-art segmentation networks our model yields better segmentation performance,
increasing the accuracy of the predictions while reducing the standard deviation. This demonstrates
the efficiency of our approach to generate precise and reliable automatic segmentations of medical
images. Our code is made publicly available at https://github.com/sinAshish/Multi-Scale-Attention
