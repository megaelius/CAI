Deep Neural Networks (DNNs) have been widely deployed for many Machine Learning applications.
Recently, CapsuleNets have overtaken traditional DNNs, because of their improved generalization
ability due to the multi-dimensional capsules, in contrast to the single-dimensional neurons.
Consequently, CapsuleNets also require extremely intense matrix computations, making it a gigantic
challenge to achieve high performance. In this paper, we propose CapsAcc, the first specialized
CMOS-based hardware architecture to perform CapsuleNets inference with high performance and
energy efficiency. State-of-the-art convolutional DNN accelerators would not work efficiently
for CapsuleNets, as their designs do not account for key operations involved in CapsuleNets, like
squashing and dynamic routing, as well as multi-dimensional matrix processing. Our CapsAcc architecture
targets this problem and achieves significant improvements, when compared to an optimized GPU
implementation. Our architecture exploits the massive parallelism by flexibly feeding the data
to a specialized systolic array according to the operations required in different layers. It also
avoids extensive load and store operations on the on-chip memory, by reusing the data when possible.
We further optimize the routing algorithm to reduce the computations needed at this stage. We synthesized
the complete CapsAcc architecture in a 32nm CMOS technology using Synopsys design tools, and evaluated
it for the MNIST benchmark (as also done by the original CapsuleNet paper) to ensure consistent and
fair comparisons. This work enables highly-efficient CapsuleNets inference on embedded platforms.
