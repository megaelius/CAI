Neuromorphic computing, inspired by the brain, promises extreme efficiency for certain classes
of learning tasks, such as classification and pattern recognition. The performance and power consumption
of neuromorphic computing depends heavily on the choice of the neuron architecture. Digital neurons
(Dig-N) are conventionally known to be accurate and efficient at high speed, while suffering from
high leakage currents from a large number of transistors in a large design. On the other hand, analog/mixed-signal
neurons are prone to noise, variability and mismatch, but can lead to extremely low-power designs.
In this work, we will analyze, compare and contrast existing neuron architectures with a proposed
mixed-signal neuron (MS-N) in terms of performance, power and noise, thereby demonstrating the
applicability of the proposed mixed-signal neuron for achieving extreme energy-efficiency in
neuromorphic computing. The proposed MS-N is implemented in 65 nm CMOS technology and exhibits
> 100X better energy-efficiency across all frequencies over two traditional digital neurons synthesized
in the same technology node. We also demonstrate that the inherent error-resiliency of a fully connected
or even convolutional neural network (CNN) can handle the noise as well as the manufacturing non-idealities
of the MS-N up to certain degrees. Notably, a system-level implementation on MNIST datasets exhibits
a worst-case increase in classification error by 2.1% when the integrated noise power in the bandwidth
is ~ 0.1 uV2, along with +-3{\sigma} amount of variation and mismatch introduced in the transistor
parameters for the proposed neuron with 8-bit precision. 