In this paper, we aim at solving the multi-domain image-to-image translation problem by a single
GAN-based model in an unsupervised manner. In the field of image-to-image translation, most previous
works mainly focus on adopting a generative adversarial network, which contains three parts, i.e.,
encoder, decoder and discriminator. These three parts are trained to give the encoder and the decoder
together as a translator. However, the discriminator that occupies a lot of parameters is abandoned
after the training process, which is wasteful of computation and memory. To handle this problem,
we integrate the discriminator and the encoder of the traditional framework into a single network,
where the decoder in our framework translates the information encoded by the discriminator to the
target image. As a result, our framework only contains two parts, i.e., decoder and discriminator,
which effectively reduces the number of the parameters of the network and achieves more effective
training. Then, we expand the traditional binary-class discriminator to the multi-classes discriminator,
which solves the multi-domain image-to-image translation problem in traditional settings. At
last, we propose the label encoder to transform the label vector to high-dimension representation
automatically rather than designing a one-hot vector manually. We performed extensive experiments
on many image-to-image translation tasks including style transfer, season transfer, face hallucination,
etc. A unified model was trained to translate images sampled from 14 considerable different domains
and the comparisons to several recently-proposed approaches demonstrate the superiority and
novelty of our framework. 