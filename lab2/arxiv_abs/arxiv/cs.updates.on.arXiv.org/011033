We show new connections between adversarial learning and explainability in Deep Neural Networks
(DNNs). One form of explanation of the output of a neural network model in terms of its input features,
is a vector of feature-attributions using the Integrated Gradient (IG) method. Two desirable characteristics
of an attribution-based explanation are: (1) $\textit{sparseness}$: the attributions of irrelevant
or weakly relevant features should be negligible, thus resulting in $\textit{concise}$ explanations
in terms of the significant features, and (2) $\textit{stability}$: it should not vary significantly
within a small local neighborhood of the input. Our first contribution is a theoretical exploration
of these connections for a class of 1-layer networks (which includes models such as logistic regression),
for which we show that (a) adversarial training using an $\ell_\infty$-bounded adversary produces
models with sparse IG vectors, and (b) natural model-training while encouraging stable IG-explanations
(via an extra term in the loss function), is equivalent to adversarial training. Our second contribution
is an empirical verification of phenomenon (a), which we show, somewhat surprisingly, occurs $\textit{not
only in 1-layer networks,}$ $\textit{but also DNNs}$ $\textit{trained on}$ $\textit{standard
image}$ $\textit{datasets:}$ adversarial training with $\ell_\infty$-bounded perturbations,
yields significantly sparser IG vectors, with little degradation in performance on natural test
data, compared to natural training. Moreover, the sparseness of the IG vectors is significantly
better than that achievable via $\ell_1$-regularized natural training. 