In biomedical research, many different types of patient data can be collected, such as various types
of omics data and medical imaging modalities. Applying multi-view learning to these different
sources of information can increase the accuracy of medical classification models compared with
single-view procedures. However, collecting biomedical data can be expensive and/or burdening
for patients, so that it is important to reduce the amount of required data collection. It is therefore
necessary to develop multi-view learning methods which can accurately identify those views that
are most important for prediction. In recent years, several biomedical studies have used an approach
known as multi-view stacking (MVS), where a model is trained on each view separately and the resulting
predictions are combined through stacking. In these studies, MVS has been shown to increase classification
accuracy. However, the MVS framework can also be used for selecting a subset of important views.
To study the view selection potential of MVS, we develop a special case called stacked penalized
logistic regression (StaPLR). Compared with existing view-selection methods, StaPLR can make
use of faster optimization algorithms and is easily parallelized. We show that nonnegativity constraints
on the parameters of the function which combines the views play an important role in preventing unimportant
views from entering the model. We investigate the performance of StaPLR through simulations, and
consider two real data examples. We compare the performance of StaPLR with an existing view selection
method called the group lasso and observe that, in terms of view selection, StaPLR is often more conservative
and has a consistently lower false positive rate. 