Finding overcomplete latent representations of data is important for signal processing, machine
learning and theoretical neuroscience. In an overcomplete representation, the number of latent
features exceeds the data dimensionality, which is useful when the data is undersampled by the measurements
(compressed sensing, information bottlenecks in neural systems) or composed from multiple complete
sets of linear features, each spanning the data space. Independent Components Analysis (ICA) is
a linear technique for learning sparse latent representations, which typically has a lower computational
cost than sparse coding, its nonlinear, recurrent counterpart. While well suited for finding complete
representations, we show that overcompleteness poses a challenge to existing ICA algorithms.
Specifically, it is the coherence control in existing ICA algorithms, necessary to prevent the
formation of duplicate features, which is ill-suited for overcompleteness. We show that in the
overcomplete case several existing ICA algorithms have undesirable maximum-coherence global
minima. Further, by comparing ICA algorithms on synthetic data and natural images to the computationally
more expensive sparse coding solution, we show that the coherence control biases the exploration
of the data manifold, sometimes yielding suboptimal solutions. Finally, we provide a theoretical
analysis of these failures and, based on theory, propose improved ICA algorithms for learning low
coherence, overcomplete dictionaries. All told, this study contributes new insights into and
methods for coherence control for linear ICA, some of which are applicable to many other, potentially
nonlinear, unsupervised learning methods. 