Recognizing human actions from varied views is challenging due to huge appearance variations in
different views. The key to this problem is to learn discriminant view-invariant representations
generalizing well across views. In this paper, we address this problem by learning view-invariant
representations hierarchically using a novel method, referred to as Joint Sparse Representation
and Distribution Adaptation (JSRDA). To obtain robust and informative feature representations,
we first incorporate a sample-affinity matrix into the marginalized stacked denoising Autoencoder
(mSDA) to obtain shared features, which are then combined with the private features. In order to
make the feature representations of videos across views transferable, we then learn a transferable
dictionary pair simultaneously from pairs of videos taken at different views to encourage each
action video across views to have the same sparse representation. However, the distribution difference
across views still exists because a unified subspace where the sparse representations of one action
across views are the same may not exist when the view difference is large. Therefore, we propose a
novel unsupervised distribution adaptation method that learns a set of projections that project
the source and target views data into respective low-dimensional subspaces where the marginal
and conditional distribution differences are reduced simultaneously. Therefore, the finally
learned feature representation is view-invariant and robust for substantial distribution difference
across views even the view difference is large. Experimental results on four multiview datasets
show that our approach outperforms the state-ofthe-art approaches. 