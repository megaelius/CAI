In a typical real-world application of re-id, a watch-list (gallery set) of a handful of target people
(e.g. suspects) to track around a large volume of non-target people are demanded across camera views,
and this is called the open-world person re-id. Different from conventional (closed-world) person
re-id, a large portion of probe samples are not from target people in the open-world setting. And,
it always happens that a non-target person would look similar to a target one and therefore would
seriously challenge a re-id system. In this work, we introduce a deep open-world group-based person
re-id model based on adversarial learning to alleviate the attack problem caused by similar non-target
people. The main idea is learning to attack feature extractor on the target people by using GAN to
generate very target-like images (imposters), and in the meantime the model will make the feature
extractor learn to tolerate the attack by discriminative learning so as to realize group-based
verification. The framework we proposed is called the adversarial open-world person re-identification,
and this is realized by our Adversarial PersonNet (APN) that jointly learns a generator, a person
discriminator, a target discriminator and a feature extractor, where the feature extractor and
target discriminator share the same weights so as to makes the feature extractor learn to tolerate
the attack by imposters for better group-based verification. While open-world person re-id is
challenging, we show for the first time that the adversarial-based approach helps stabilize person
re-id system under imposter attack more effectively. 