This work has been submitted to the IEEE for possible publication. Copyright may be transferred
without notice, after which this version may no longer be accessible. This paper presents a novel
data-driven method for learning deep constrained continuous control policies and dynamical models
of linear systems. By leveraging partial knowledge of system dynamics and constraint enforcing
multi-objective loss functions, the method can learn from small and static datasets, handle time-varying
state and input constraints and enforce the stability properties of the controlled system. We use
a continuous control design example to demonstrate the performance of the method on three distinct
tasks: system identification, control policy learning, and simultaneous system identification
and policy learning. We assess the system identification performance by comparing open-loop simulations
of the true system and the learned models. We demonstrate the performance of the policy learning
methodology in closed-loop simulations using the system model affected by varying levels of parametric
and additive uncertainties. We report superior performance in terms of reference tracking, robustness,
and online computational and memory footprints compared with classical control approaches, namely
LQR and LQI controllers, and with three variants of model predictive control (MPC) formulations
and two traditional MPC solution approaches. We then evaluate the potential of simultaneously
learning the system model and control policy. Our empirical results demonstrate the effectiveness
of our unifying framework for constrained optimal control of linear systems to provide stability
guarantees of the learned dynamics, robustness to uncertainty, and high sampling efficiency.
