Recent advances in adversarial Deep Learning (DL) have opened up a new and largely unexplored surface
for malicious attacks jeopardizing the integrity of autonomous DL systems. This paper introduces
CuRTAIL, a novel end-to-end computing framework to characterize and thwart potential adversarial
attacks and significantly improve the reliability (safety) of a victim DL model. We formalize the
goal of preventing adversarial attacks as an optimization problem to minimize the rarely observed
regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization
problem, a set of complementary but disjoint modular redundancies are trained to validate the legitimacy
of the input samples. The proposed countermeasure is unsupervised, meaning that no adversarial
sample is leveraged to train modular redundancies. This, in turn, ensures the effectiveness of
the defense in the face of generic attacks. We evaluate the robustness of our proposed methodology
against the state-of-the-art adaptive attacks in a white-box setting considering that the adversary
knows everything about the victim model and its defenders. Extensive evaluations for analyzing
MNIST, CIFAR10, and ImageNet data corroborate the effectiveness of CuRTAIL framework against
adversarial samples. The computations in each modular redundancy can be performed independently
of the other redundancy modules. As such, CuRTAIL detection algorithm can be completely parallelized
among multiple hardware settings to achieve maximum throughput. We further provide an open-source
Application Programming Interface (API) to facilitate the adoption of the proposed framework
for various applications. 