Many of the existing machine learning algorithms, both supervised and unsupervised, depend on
the quality of the input characteristics to generate a good model. The amount of these variables
is also important, since performance tends to decline as the input dimensionality increases, hence
the interest in using feature fusion techniques, able to produce feature sets that are more compact
and higher level. A plethora of procedures to fuse original variables for producing new ones has
been developed in the past decades. The most basic ones use linear combinations of the original variables,
such as PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis), while others
find manifold embeddings of lower dimensionality based on non-linear combinations, such as Isomap
or LLE (Linear Locally Embedding) techniques. More recently, autoencoders (AEs) have emerged
as an alternative to manifold learning for conducting nonlinear feature fusion. Dozens of AE models
have been proposed lately, each with its own specific traits. Although many of them can be used to
generate reduced feature sets through the fusion of the original ones, there also AEs designed with
other applications in mind. The goal of this paper is to provide the reader with a broad view of what
an AE is, how they are used for feature fusion, a taxonomy gathering a broad range of models, and how
they relate to other classical techniques. In addition, a set of didactic guidelines on how to choose
the proper AE for a given task is supplied, together with a discussion of the software tools available.
Finally, two case studies illustrate the usage of AEs with datasets of handwritten digits and breast
cancer. 