Tensor decompositions are used in various data mining applications from social network to medical
applications and are extremely useful in discovering latent structures or concepts in the data.
Many real-world applications are dynamic in nature and so are their data. To deal with this dynamic
nature of data, there exist a variety of online tensor decomposition algorithms. A central assumption
in all those algorithms is that the number of latent concepts remains fixed throughout the entire
stream. However, this need not be the case. Every incoming batch in the stream may have a different
number of latent concepts, and the difference in latent concepts from one tensor batch to another
can provide insights into how our findings in a particular application behave and deviate over time.
In this paper, we define "concept" and "concept drift" in the context of streaming tensor decomposition,
as the manifestation of the variability of latent concepts throughout the stream. Furthermore,
we introduce SeekAndDestroy, an algorithm that detects concept drift in streaming tensor decomposition
and is able to produce results robust to that drift. To the best of our knowledge, this is the first
work that investigates concept drift in streaming tensor decomposition. We extensively evaluate
SeekAndDestroy on synthetic datasets, which exhibit a wide variety of realistic drift. Our experiments
demonstrate the effectiveness of SeekAndDestroy, both in the detection of concept drift and in
the alleviation of its effects, producing results with similar quality to decomposing the entire
tensor in one shot. Additionally, in real datasets, SeekAndDestroy outperforms other streaming
baselines, while discovering novel useful components. 