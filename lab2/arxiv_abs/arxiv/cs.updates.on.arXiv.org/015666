This thesis describes work on two applications of probabilistic programming: the learning of probabilistic
program code given specifications, in particular program code of one-dimensional samplers; and
the facilitation of sequential Monte Carlo inference with help of data-driven proposals. The latter
is presented with experimental results on a linear Gaussian model and a non-parametric dependent
Dirichlet process mixture of objects model for object recognition and tracking. In Chapter 1 we
provide a brief introduction to probabilistic programming. In Chapter 2 we present an approach
to automatic discovery of samplers in the form of probabilistic programs. We formulate a Bayesian
approach to this problem by specifying a grammar-based prior over probabilistic program code.
We use an approximate Bayesian computation method to learn the programs, whose executions generate
samples that statistically match observed data or analytical characteristics of distributions
of interest. In our experiments we leverage different probabilistic programming systems to perform
Markov chain Monte Carlo sampling over the space of programs. Experimental results have demonstrated
that, using the proposed methodology, we can learn approximate and even some exact samplers. Finally,
we show that our results are competitive with regard to genetic programming methods. In Chapter
3, we describe a way to facilitate sequential Monte Carlo inference in probabilistic programming
using data-driven proposals. In particular, we develop a distance-based proposal for the non-parametric
dependent Dirichlet process mixture of objects model. We implement this approach in the probabilistic
programming system Anglican, and show that for that model data-driven proposals provide significant
performance improvements. We also explore the possibility of using neural networks to improve
data-driven proposals. 