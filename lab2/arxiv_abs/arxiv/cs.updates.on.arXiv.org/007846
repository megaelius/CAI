Few-shot learning aims to learn classifiers for new classes with only a few training examples per
class. Most existing few-shot learning approaches belong to either metric-based meta-learning
or optimization-based meta-learning category, both of which have achieved successes in the simplified
"$k$-shot $N$-way" image classification settings. Specifically, the optimization-based approaches
train a meta-learner to predict the parameters of the task-specific classifiers. The task-specific
classifiers are required to be homogeneous-structured to ease the parameter prediction, so the
meta-learning approaches could only handle few-shot learning problems where the tasks share a
uniform number of classes. The metric-based approaches learn one task-invariant metric for all
the tasks. Even though the metric-learning approaches allow different numbers of classes, they
require the tasks all coming from a similar domain such that there exists a uniform metric that could
work across tasks. In this work, we propose a hybrid meta-learning model called Meta-Metric-Learner
which combines the merits of both optimization- and metric-based approaches. Our meta-metric-learning
approach consists of two components, a task-specific metric-based learner as a base model, and
a meta-learner that learns and specifies the base model. Thus our model is able to handle flexible
numbers of classes as well as generate more generalized metrics for classification across tasks.
We test our approach in the standard "$k$-shot $N$-way" few-shot learning setting following previous
works and a new realistic few-shot setting with flexible class numbers in both single-source form
and multi-source forms. Experiments show that our approach can obtain superior performance in
all settings. 