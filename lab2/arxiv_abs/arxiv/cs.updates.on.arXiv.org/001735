Biomedical word sense disambiguation (WSD) is an important intermediate task in many natural language
processing applications such as named entity recognition, syntactic parsing, and relation extraction.
In this paper, we employ knowledge-based approaches that also exploit recent advances in neural
word/concept embeddings to improve over the state-of-the-art in biomedical WSD using the MSH WSD
dataset as the test set. Our methods involve weak supervision - we do not use any hand-labeled examples
for WSD to build our prediction models; however, we employ an existing well known named entity recognition
and concept mapping program, MetaMap, to obtain our concept vectors. Over the MSH WSD dataset, our
linear time (in terms of numbers of senses and words in the test instance) method achieves an accuracy
of 92.24% which is an absolute 3% improvement over the best known results obtained via unsupervised
or knowledge-based means. A more expensive approach that we developed relies on a nearest neighbor
framework and achieves an accuracy of 94.34%. Employing dense vector representations learned
from unlabeled free text has been shown to benefit many language processing tasks recently and our
efforts show that biomedical WSD is no exception to this trend. For a complex and rapidly evolving
domain such as biomedicine, building labeled datasets for larger sets of ambiguous terms may be
impractical. Here, we show that weak supervision that leverages recent advances in representation
learning can rival supervised approaches in biomedical WSD. However, external knowledge bases
(here sense inventories) play a key role in the improvements achieved. 