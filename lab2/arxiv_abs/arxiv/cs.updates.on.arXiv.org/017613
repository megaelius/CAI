In recent years, deep learning-based feature representation methods have shown a promising impact
in electroencephalography (EEG)-based brain-computer interface (BCI). Nonetheless, owing
to high intra- and inter-subject variabilities, many studies on decoding EEG were designed in a
subject-specific manner by using calibration samples, with no concern of its practical use, hampered
by time-consuming steps and a large data requirement. To this end, recent studies adopted a transfer
learning strategy, especially domain adaptation techniques. Among those, to our knowledge, an
adversarial learning has shown its potential in BCIs. In the meantime, it is known that adversarial
learning-based domain adaptation methods are prone to negative transfer that disrupts learning
generalized feature representations, applicable to diverse domains, e.g., subjects or sessions
in BCIs. In this paper, we propose a novel framework that learns class-relevant and subject-invariant
feature representations in an information-theoretic manner, without using adversarial learning.
To be specific, we devise two operational components in a deep network that explicitly estimate
mutual information between feature representations; (1) to decompose features in an intermediate
layer into class-relevant and class-irrelevant ones, (2) to enrich class-discriminative feature
representation. On two large EEG datasets, we validated the effectiveness of our proposed framework
by comparing with several comparative methods in performance. Further, we conducted rigorous
analyses by performing an ablation study in regard to the components in our network, explaining
our model's decision on input EEG signals via layer-wise relevance propagation, and visualizing
the distribution of learned features via t-SNE. 