Deep neural network classifiers are vulnerable to small input perturbations carefully generated
by the adversaries. Injecting adversarial inputs during training, known as adversarial training,
can improve robustness against one-step attacks, but not for unknown iterative attacks. To address
this challenge, we propose to utilize embedding space for both classification and low-level (pixel-level)
similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial
images without replacing their corresponding clean images and penalize the distance between the
two embeddings (clean and adversarial). This additional regularization encourages two similar
images (clean and perturbed versions) to produce the same outputs, not necessarily the true labels,
enhancing classifier's robustness against pixel level perturbation. Next, we show iteratively
generated adversarial images easily transfer between networks trained with the same strategy.
Inspired by this observation, we also propose cascade adversarial training, which transfers the
knowledge of the end results of adversarial training. We train a network from scratch by injecting
iteratively generated adversarial images crafted from already defended networks in addition
to one-step adversarial images from the network being trained. Experimental results show that
cascade adversarial training together with our proposed low-level similarity learning efficiently
enhance the robustness against iterative attacks, but at the expense of decreased robustness against
one-step attacks. We show that combining those two techniques can also improve robustness under
the worst case black box attack scenario. 