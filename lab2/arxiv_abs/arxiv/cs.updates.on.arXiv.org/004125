With the advent of sequential matching (of supply and demand) systems (uber, Lyft, Grab for taxis;
ubereats, deliveroo, etc for food; amazon prime, lazada etc. for groceries) across many online
and offline services, individuals (taxi drivers, delivery boys, delivery van drivers, etc.) earn
more by being at the "right" place at the "right" time. We focus on learning techniques for providing
guidance (on right locations to be at right times) to individuals in the presence of other "learning"
individuals. Interactions between indivduals are anonymous, i.e, the outcome of an interaction
(competing for demand) is independent of the identity of the agents and therefore we refer to these
as Anonymous MARL settings. Existing research of relevance is on independent learning using Reinforcement
Learning (RL) or on Multi-Agent Reinforcement Learning (MARL). The number of individuals in aggregation
systems is extremely large and individuals have their own selfish interest (of maximising revenue).
Therefore, traditional MARL approaches are either not scalable or assumptions of common objective
or action coordination are not viable. In this paper, we focus on improving performance of independent
reinforcement learners, specifically the popular Deep Q-Networks (DQN) and Advantage Actor Critic
(A2C) approaches by exploiting anonymity. Specifically, we control non-stationarity introduced
by other agents using entropy of agent density distribution. We demonstrate a significant improvement
in revenue for individuals and for all agents together with our learners on a generic experimental
set up for aggregation systems and a real world taxi dataset. 