Active inference is a first principle account of how autonomous agents operate in dynamic, non-stationary
environments. This problem is also considered in reinforcement learning, but limited work exists
on comparing the two approaches on the same discrete-state environments. In this paper, we provide:
1) an accessible overview of the discrete-state formulation of active inference, highlighting
natural behaviors in active inference that are generally engineered in reinforcement learning;
2) to our knowledge, the first explicit discrete-state comparison between active inference and
reinforcement learning on an OpenAI gym baseline. We begin by providing a condensed overview of
the active inference literature, in particular viewing the various natural behaviors of active
inference agents through the lens of reinforcement learning. We show that by operating in a pure
belief-based setting, active inference agents can carry out epistemic exploration - and account
for uncertainty about their environment - in a Bayes optimal fashion. We make these properties explicit
by showing that the active inference agent's ability to carry out online planning, in a pure-belief
setting, enables it to act optimally, given the non-stationary dynamics of an environment when
compared to both Q-learning and Bayesian model-based reinforcement learning agents. We conclude
by noting that this formalism can be applied to more complex settings; e.g., robotic arm movement,
Atari games, etc., if appropriate generative models can be formulated. In short, we aim to demystify
the behavior of active inference agents by presenting an accessible discrete state-space and time
formulation, and demonstrate these behaviors in a OpenAI gym environment, alongside reinforcement
learning agents. 