Steady-State Visual Evoked Potentials (SSVEPs) are neural oscillations from the parietal and
occipital regions of the brain that are evoked from flickering visual stimuli. SSVEPs are robust
signals measurable in the electroencephalogram (EEG) and are commonly used in brain-computer
interfaces (BCIs). However, methods for high-accuracy decoding of SSVEPs usually require hand-crafted
approaches that leverage domain-specific knowledge of the stimulus signals, such as specific
temporal frequencies in the visual stimuli and their relative spatial arrangement. When this knowledge
is unavailable, such as when SSVEP signals are acquired asynchronously, such approaches tend to
fail. In this paper, we show how a compact convolutional neural network (Compact-CNN), which only
requires raw EEG signals for automatic feature extraction, can be used to decode signals from a 12-class
SSVEP dataset without the need for any domain-specific knowledge or calibration data. We report
across subject mean accuracy of approximately 80% (chance being 8.3%) and show this is substantially
better than current state-of-the-art hand-crafted approaches using canonical correlation analysis
(CCA) and Combined-CCA. Furthermore, we analyze our Compact-CNN to examine the underlying feature
representation, discovering that the deep learner extracts additional phase and amplitude related
features associated with the structure of the dataset. We discuss how our Compact-CNN shows promise
for BCI applications that allow users to freely gaze/attend to any stimulus at any time (e.g., asynchronous
BCI) as well as provides a method for analyzing SSVEP signals in a way that might augment our understanding
about the basic processing in the visual cortex. 