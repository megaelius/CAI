Nowadays, we are immersed in tens of newly-proposed evolutionary and swam-intelligence metaheuristics,
which makes it very difficult to choose a proper one to be applied on a specific optimization problem
at hand. On the other hand, most of these metaheuristics are nothing but slightly modified variants
of the basic metaheuristics. For example, Differential Evolution (DE) or Shuffled Frog Leaping
(SFL) are just Genetic Algorithms (GA) with a specialized operator or an extra local search, respectively.
Therefore, what comes to the mind is whether the behavior of such newly-proposed metaheuristics
can be investigated on the basis of studying the specifications and characteristics of their ancestors.
In this paper, a comprehensive evaluation study on some basic metaheuristics i.e. Genetic Algorithm
(GA), Particle Swarm Optimization (PSO), Artificial Bee Colony (ABC), Teaching-Learning-Based
Optimization (TLBO), and Cuckoo Optimization algorithm (COA) is conducted, which give us a deeper
insight into the performance of them so that we will be able to better estimate the performance and
applicability of all other variations originated from them. A large number of experiments have
been conducted on 20 different combinatorial optimization benchmark functions with different
characteristics, and the results reveal to us some fundamental conclusions besides the following
ranking order among these metaheuristics, {ABC, PSO, TLBO, GA, COA} i.e. ABC and COA are the best
and the worst methods from the performance point of view, respectively. In addition, from the convergence
perspective, PSO and ABC have significant better convergence for unimodal and multimodal functions,
respectively, while GA and COA have premature convergence to local optima in many cases needing
alternative mutation mechanisms to enhance diversification and global search. 