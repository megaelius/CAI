Due to their inherent variabilities,nanomaterial-based sensors are challenging to translate
into real-world applications,where reliability/reproducibility is key.Recently we showed
Bayesian inference can be employed on engineered variability in layered nanomaterial-based optical
transmission filters to determine optical wavelengths with high accuracy/precision.In many
practical applications the sensing cost/speed and long-term reliability can be equal or more important
considerations.Though various machine learning tools are frequently used on sensor/detector
networks to address these,nonetheless their effectiveness on nanomaterial-based sensors has
not been explored.Here we show the best choice of ML algorithm in a cyber-nanomaterial detector
is mainly determined by specific use considerations,e.g.,accuracy, computational cost,speed,
and resilience against drifts/ageing effects.When sufficient data/computing resources are
provided,highest sensing accuracy can be achieved by the kNN and Bayesian inference algorithms,but
but can be computationally expensive for real-time applications.In contrast,artificial neural
networks are computationally expensive to train,but provide the fastest result under testing
conditions and remain reasonably accurate.When data is limited,SVMs perform well even with small
training sets,while other algorithms show considerable reduction in accuracy if data is scarce,hence,setting
a lower limit on the size of required training data.We show by tracking/modeling the long-term drifts
of the detector performance over large (1year) period,it is possible to improve the predictive
accuracy with no need for recalibration.Our research shows for the first time if the ML algorithm
is chosen specific to use-case,low-cost solution-processed cyber-nanomaterial detectors can
be practically implemented under diverse operational requirements,despite their inherent variabilities.
