The use of RGB-D information for salient object detection has been extensively explored in recent
years. However, relatively few efforts have been put towards modeling salient object detection
in real-world human activity scenes with RGBD. In this work, we fill the gap by making the following
contributions to RGB-D salient object detection. (1) We carefully collect a new SIP (salient person)
dataset, which consists of ~1K high-resolution images that cover diverse real-world scenes from
various viewpoints, poses, occlusions, illuminations, and backgrounds. (2) We conduct a large-scale
(and, so far, the most comprehensive) benchmark comparing contemporary methods, which has long
been missing in the field and can serve as a baseline for future research. We systematically summarize
32 popular models and evaluate 18 parts of 32 models on seven datasets containing a total of about
97K images. (3) We propose a simple general architecture, called Deep Depth-Depurator Network
(D3Net). It consists of a depth depurator unit (DDU) and a three-stream feature learning module
(FLM), which performs low-quality depth map filtering and cross-modal feature learning respectively.
These components form a nested structure and are elaborately designed to be learned jointly. D3Net
exceeds the performance of any prior contenders across all five metrics under consideration, thus
serving as a strong model to advance research in this field. We also demonstrate that D3Net can be
used to efficiently extract salient object masks from real scenes, enabling effective background
changing application with a speed of 65fps on a single GPU. All the saliency maps, our new SIP dataset,
the D3Net model, and the evaluation tools are publicly available at https://github.com/DengPingFan/D3NetBenchmark.
