Training machine learning models often requires data from multiple parties. However, in some cases,
data owners cannot share their data due to legal or privacy constraints but would still benefit from
training a model jointly with multiple parties. Federated learning has arisen as an alternative
to allow for the collaborative training of models without the sharing of raw data. However, attacks
in the literature have demonstrated that simply maintaining data locally during training processes
does not provide strong enough privacy guarantees. We need a federated learning system capable
of preventing inference over the messages exchanged between parties during training as well as
the final, trained model, considering potential collusion between parties, and ensuring the resulting
machine learning model has acceptable predictive accuracy. Currently, existing approaches are
either vulnerable to inference or do not scale for a large number of parties, resulting in models
with low accuracy. To close this gap, we present a scalable approach that protects against these
threats while producing models with high accuracy. Our approach provides formal data privacy guarantees
using both differential privacy and secure multiparty computation frameworks. We validate our
system with experimental results on two popular and significantly different machine learning
algorithms: decision trees and convolutional neural networks. To the best of our knowledge, this
presents the first approach to accurately train a neural network in a private, federated fashion.
Our experiments demonstrate that our approach outperforms state of the art solutions in accuracy,
customizability, and scalability. 