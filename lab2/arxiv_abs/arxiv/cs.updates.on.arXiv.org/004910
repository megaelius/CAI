Recent neural networks such as WaveNet and sampleRNN that learn directly from speech waveform samples
have achieved very high-quality synthetic speech in terms of both naturalness and speaker similarity
even in multi-speaker text-to-speech synthesis systems. Such neural networks are being used as
an alternative to vocoders and hence they are often called neural vocoders. The neural vocoder uses
acoustic features as local condition parameters, and these parameters need to be accurately predicted
by another acoustic model. However, it is not yet clear how to train this acoustic model, which is
problematic because the final quality of synthetic speech is significantly affected by the performance
of the acoustic model. Significant degradation happens, especially when predicted acoustic features
have mismatched characteristics compared to natural ones. In order to reduce the mismatched characteristics
between natural and generated acoustic features, we propose frameworks that incorporate either
a conditional generative adversarial network (GAN) or its variant, Wasserstein GAN with gradient
penalty (WGAN-GP), into multi-speaker speech synthesis that uses the WaveNet vocoder. We also
extend the GAN frameworks and use the discretized mixture logistic loss of a well-trained WaveNet
in addition to mean squared error and adversarial losses as parts of objective functions. Experimental
results show that acoustic models trained using the WGAN-GP framework using back-propagated discretized-mixture-of-logistics
(DML) loss achieves the highest subjective evaluation scores in terms of both quality and speaker
similarity. 