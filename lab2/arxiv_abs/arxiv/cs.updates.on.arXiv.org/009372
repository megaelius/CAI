Visualizing the details of different cellular structures is of great importance to elucidate cellular
functions. However, it is challenging to obtain high quality images of different structures directly
due to complex cellular environments. Fluorescence microscopy is a popular technique to label
different structures but has several drawbacks. In particular, labeling is time consuming and
may affect cell morphology, and simultaneous labels are inherently limited. This raises the need
of building computational models to learn relationships between unlabeled and labeled fluorescence
images, and to infer fluorescent labels of other unlabeled fluorescence images. We propose to develop
a novel deep model for fluorescence image prediction. We first propose a novel network layer, known
as the global transformer layer, that fuses global information from inputs effectively. The proposed
global transformer layer can generate outputs with arbitrary dimensions, and can be employed for
all the regular, down-sampling, and up-sampling operators. We then incorporate our proposed global
transformer layers and dense blocks to build an U-Net like network. We believe such a design can promote
feature reusing between layers. In addition, we propose a multi-scale input strategy to encourage
networks to capture features at different scales. We conduct evaluations across various label-free
prediction tasks to demonstrate the effectiveness of our approach. Both quantitative and qualitative
results show that our method outperforms the state-of-the-art approach significantly. It is also
shown that our proposed global transformer layer is useful to improve the fluorescence image prediction
results. 