Fluorescence microscopy plays a vital role in understanding the subcellular structures of living
cells. However, it requires considerable effort in sample preparation related to chemical fixation,
staining, cost, and time. To reduce those factors, we present a virtual fluorescence staining method
based on deep neural networks (VirFluoNet) to transform fluorescence images of molecular labels
into other molecular fluorescence labels in the same field-of-view. To achieve this goal, we develop
and train a conditional generative adversarial network (cGAN) to perform digital fluorescence
imaging demonstrated on human osteosarcoma U2OS cell fluorescence images captured under Cell
Painting staining protocol. A detailed comparative analysis is also conducted on the performance
of the cGAN network between predicting fluorescence channels based on phase contrast or based on
another fluorescence channel using human breast cancer MDA-MB-231 cell line as a test case. In addition,
we implement a deep learning model to perform autofocusing on another human U2OS fluorescence dataset
as a preprocessing step to defocus an out-focus channel in U2OS dataset. A quantitative index of
image prediction error is introduced based on signal pixel-wise spatial and intensity differences
with ground truth to evaluate the performance of prediction to high-complex and throughput fluorescence.
This index provides a rational way to perform image segmentation on error signals and to understand
the likelihood of mis-interpreting biology from the predicted image. In total, these findings
contribute to the utility of deep learning image regression for fluorescence microscopy datasets
of biological cells, balanced against savings of cost, time, and experimental effort. Furthermore,
the approach introduced here holds promise for modeling the internal relationships between organelles
and biomolecules within living cells. 