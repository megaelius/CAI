Generative Adversarial Networks have shown impressive results for the task of object translation,
including face-to-face translation. A key component behind the success of recent approaches is
the self-consistency loss, which encourages a network to recover the original input image when
the output generated for a desired attribute is itself passed through the same network, but with
the target attribute inverted. While the self-consistency loss yields photo-realistic results,
it can be shown that the input and target domains, supposed to be close, differ substantially. This
is empirically found by observing that a network recovers the input image even if attributes other
than the inversion of the original goal are set as target. This stops one combining networks for different
tasks, or using a network to do progressive forward passes. In this paper, we show empirical evidence
of this effect, and propose a new loss to bridge the gap between the distributions of the input and
target domains. This "triple consistency loss", aims to minimise the distance between the outputs
generated by the network for different routes to the target, independent of any intermediate steps.
To show this is effective, we incorporate the triple consistency loss into the training of a new landmark-guided
face to face synthesis, where, contrary to previous works, the generated images can simultaneously
undergo a large transformation in both expression and pose. To the best of our knowledge, we are the
first to tackle the problem of mismatching distributions in self-domain synthesis, and to propose
"in-the-wild" landmark-guided synthesis. Code will be available at https://github.com/ESanchezLozano/GANnotation
