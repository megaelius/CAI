Discriminative localization is essential for fine-grained image classification task, which
devotes to recognizing hundreds of subcategories in the same basic-level category. Reflecting
on discriminative regions of objects, key differences among different subcategories are subtle
and local. Existing methods generally adopt a two-stage learning framework: The first stage is
to localize the discriminative regions of objects, and the second is to encode the discriminative
features for training classifiers. However, these methods generally have two limitations: (1)
Separation of the two-stage learning is time-consuming. (2) Dependence on object and parts annotations
for discriminative localization learning leads to heavily labor-consuming labeling. It is highly
challenging to address these two important limitations simultaneously. Existing methods only
focus on one of them. Therefore, this paper proposes the discriminative localization approach
via saliency-guided Faster R-CNN to address the above two limitations at the same time, and our main
novelties and advantages are: (1) End-to-end network based on Faster R-CNN is designed to simultaneously
localize discriminative regions and encode discriminative features, which accelerates classification
speed. (2) Saliency-guided localization learning is proposed to localize the discriminative
region automatically, avoiding labor-consuming labeling. Both are jointly employed to simultaneously
accelerate classification speed and eliminate dependence on object and parts annotations. Comparing
with the state-of-the-art methods on the widely-used CUB-200-2011 dataset, our approach achieves
both the best classification accuracy and efficiency. 