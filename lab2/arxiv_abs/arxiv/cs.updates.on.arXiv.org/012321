With the improvements in speech recognition and voice generation technologies over the last years,
a lot of companies have sought to develop conversation understanding systems that run on mobile
phones or smart home devices through natural language interfaces. Conversational assistants,
such as Google Assistant and Microsoft Cortana, can help users to complete various types of tasks.
This requires an accurate understanding of the user's information need as the conversation evolves
into multiple turns. Finding relevant context in a conversation's history is challenging because
of the complexity of natural language and the evolution of a user's information need. In this work,
we present an extensive analysis of language, relevance, dependency of user utterances in a multi-turn
information-seeking conversation. To this aim, we have annotated relevant utterances in the conversations
released by the TREC CaST 2019 track. The annotation labels determine which of the previous utterances
in a conversation can be used to improve the current one. Furthermore, we propose a neural utterance
relevance model based on BERT fine-tuning, outperforming competitive baselines. We study and
compare the performance of multiple retrieval models, utilizing different strategies to incorporate
the user's context. The experimental results on both classification and retrieval tasks show that
our proposed approach can effectively identify and incorporate the conversation context. We show
that processing the current utterance using the predicted relevant utterance leads to a 38% relative
improvement in terms of nDCG@20. Finally, to foster research in this area, we have released the dataset
of the annotations. 