Deep learning (DL) systems are increasingly deployed in security-critical domains including
self-driving cars and malware detection, where the correctness and predictability of a system's
behavior for corner-case inputs are of great importance. However, systematic testing of large-scale
DL systems with thousands of neurons and millions of parameters for all possible corner-cases is
a hard problem. Existing DL testing depends heavily on manually labeled data and therefore often
fails to expose different erroneous behaviors for rare inputs. We present DeepXplore, the first
whitebox framework for systematically testing real-world DL systems. We address two problems:
(1) generating inputs that trigger different parts of a DL system's logic and (2) identifying incorrect
behaviors of DL systems without manual effort. First, we introduce neuron coverage for estimating
the parts of DL system exercised by a set of test inputs. Next, we leverage multiple DL systems with
similar functionality as cross-referencing oracles and thus avoid manual checking for erroneous
behaviors. We demonstrate how finding inputs triggering differential behaviors while achieving
high neuron coverage for DL algorithms can be represented as a joint optimization problem and solved
efficiently using gradient-based optimization techniques. DeepXplore finds thousands of incorrect
corner-case behaviors in state-of-the-art DL models trained on five popular datasets. For all
tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior
within one second while running on a commodity laptop. The inputs generated by DeepXplore achieved
33.2% higher neuron coverage on average than existing testing methods. We further show that the
test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve
classification accuracy or identify polluted training data. 