Speech-driven facial animation is useful for a variety of applications such as telepresence, chatbots,
etc. The necessary attributes of having a realistic face animation are 1) audio-visual synchronization
(2) identity preservation of the target individual (3) plausible mouth movements (4) presence
of natural eye blinks. The existing methods mostly address the audio-visual lip synchronization,
and few recent works have addressed the synthesis of natural eye blinks for overall video realism.
In this paper, we propose a method for identity-preserving realistic facial animation from speech.
We first generate person-independent facial landmarks from audio using DeepSpeech features for
invariance to different voices, accents, etc. To add realism, we impose eye blinks on facial landmarks
using unsupervised learning and retargets the person-independent landmarks to person-specific
landmarks to preserve the identity-related facial structure which helps in the generation of plausible
mouth shapes of the target identity. Finally, we use LSGAN to generate the facial texture from person-specific
facial landmarks, using an attention mechanism that helps to preserve identity-related texture.
An extensive comparison of our proposed method with the current state-of-the-art methods demonstrates
a significant improvement in terms of lip synchronization accuracy, image reconstruction quality,
sharpness, and identity-preservation. A user study also reveals improved realism of our animation
results over the state-of-the-art methods. To the best of our knowledge, this is the first work in
speech-driven 2D facial animation that simultaneously addresses all the above-mentioned attributes
of a realistic speech-driven face animation. 