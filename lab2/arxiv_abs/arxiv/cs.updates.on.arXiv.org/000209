In recent years, a great many methods of learning from multi-view data by considering the diversity
of different views have been proposed. These views may be obtained from multiple sources or different
feature subsets. In trying to organize and highlight similarities and differences between the
variety of multi-view learning approaches, we review a number of representative multi-view learning
algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple
kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately
to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms
exploit kernels that naturally correspond to different views and combine kernels either linearly
or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain
a latent subspace shared by multiple views by assuming that the input views are generated from this
latent subspace. Though there is significant variance in the approaches to integrating multiple
views to improve learning performance, they mainly exploit either the consensus principle or the
complementary principle to ensure the success of multi-view learning. Since accessing multiple
views is the fundament of multi-view learning, with the exception of study on learning a model from
multiple views, it is also valuable to study how to construct multiple views and how to evaluate these
views. Overall, by exploring the consistency and complementary properties of different views,
multi-view learning is rendered more effective, more promising, and has better generalization
ability than single-view learning. 