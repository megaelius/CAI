This paper offers a novel mathematical approach, the modified Fractional-order Steepest Descent
Method (FSDM) for training BackPropagation Neural Networks (BPNNs); this differs from the majority
of the previous approaches and as such. A promising mathematical method, fractional calculus,
has the potential to assume a prominent role in the applications of neural networks and cybernetics
because of its inherent strengths such as long-term memory, nonlocality, and weak singularity.
Therefore, to improve the optimization performance of classic first-order BPNNs, in this paper
we study whether it could be possible to modified FSDM and generalize classic first-order BPNNs
to modified FSDM based Fractional-order Backpropagation Neural Networks (FBPNNs). Motivated
by this inspiration, this paper proposes a state-of-the-art application of fractional calculus
to implement a modified FSDM based FBPNN whose reverse incremental search is in the negative directions
of the approximate fractional-order partial derivatives of the square error. At first, the theoretical
concept of a modified FSDM based FBPNN is described mathematically. Then, the mathematical proof
of the fractional-order global optimal convergence, an assumption of the structure, and the fractional-order
multi-scale global optimization of a modified FSDM based FBPNN are analysed in detail. Finally,
we perform comparative experiments and compare a modified FSDM based FBPNN with a classic first-order
BPNN, i.e., an example function approximation, fractional-order multi-scale global optimization,
and two comparative performances with real data. The more efficient optimal searching capability
of the fractional-order multi-scale global optimization of a modified FSDM based FBPNN to determine
the global optimal solution is the major advantage being superior to a classic first-order BPNN.
