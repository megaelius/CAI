Extracting information related to weather and visual conditions at a given time and space is indispensable
for scene awareness, which strongly impacts our behaviours, from simply walking in a city to riding
a bike, driving a car, or autonomous drive-assistance. Despite the significance of this subject,
it is still not been fully addressed by the machine intelligence relying on deep learning and computer
vision to detect the multi-labels of weather and visual conditions with a unified method that can
be easily used for practice. What has been achieved to-date is rather sectorial models that address
limited number of labels that do not cover the wide spectrum of weather and visual conditions. Nonetheless,
weather and visual conditions are often addressed individually. In this paper, we introduce a novel
framework to automatically extract this information from street-level images relying on deep
learning and computer vision using a unified method without any pre-defined constraints in the
processed images. A pipeline of four deep Convolutional Neural Network (CNN) models, so-called
the WeatherNet, is trained, relying on residual learning using ResNet50 architecture, to extract
various weather and visual conditions such as Dawn/dusk, day and night for time detection, and glare
for lighting conditions, and clear, rainy, snowy, and foggy for weather conditions. The WeatherNet
shows strong performance in extracting this information from user-defined images or video streams
that can be used not limited to: autonomous vehicles and drive-assistance systems, tracking behaviours,
safety-related research, or even for better understanding cities through images for policy-makers.
