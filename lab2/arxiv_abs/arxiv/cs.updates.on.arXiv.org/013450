We study the design of rating systems that incentivize (more) efficient social learning among self-interested
agents. Agents arrive sequentially and are presented with a set of possible actions, each of which
yields a positive reward with an unknown probability. A disclosure policy sends messages about
the rewards of previously-chosen actions to arriving agents. These messages can alter agents'
incentives towards exploration, taking potentially sub-optimal actions for the sake of learning
more about their rewards. Prior work achieves much progress with disclosure policies that merely
recommend an action to each user, but relies heavily on standard, yet very strong rationality assumptions.
We study a particular class of disclosure policies that use messages, called unbiased subhistories,
consisting of the actions and rewards from a subsequence of past agents. Each subsequence is chosen
ahead of time, according to a predetermined partial order on the rounds. We posit a flexible model
of frequentist agent response, which we argue is plausible for this class of "order-based" disclosure
policies. We measure the success of a policy by its regret, i.e., the difference, over all rounds,
between the expected reward of the best action and the reward induced by the policy. A disclosure
policy that reveals full history in each round risks inducing herding behavior among the agents,
and typically has regret linear in the time horizon $T$. Our main result is an order-based disclosure
policy that obtains regret $\tilde{O}(\sqrt{T})$. This regret is known to be optimal in the worst
case over reward distributions, even absent incentives. We also exhibit simpler order-based policies
with higher, but still sublinear, regret. These policies can be interpreted as dividing a sublinear
number of agents into constant-sized focus groups, whose histories are then revealed to future
agents. 