Machine learning has emerged as the dominant tool for implementing complex cognitive tasks that
require supervised, unsupervised, and reinforcement learning. While the resulting machines
have demonstrated in some cases even super-human performance, their energy consumption has often
proved to be prohibitive in the absence of costly super-computers. Most state-of-the-art machine
learning solutions are based on memory-less models of neurons. This is unlike the neurons in the
human brain, which encode and process information using temporal information in spike events.
The different computing principles underlying biological neurons and how they combine together
to efficiently process information is believed to be a key factor behind their superior efficiency
compared to current machine learning systems. Inspired by the time-encoding mechanism used by
the brain, third generation spiking neural networks (SNNs) are being studied for building a new
class of information processing engines. Modern computing systems based on the von Neumann architecture,
however, are ill-suited for efficiently implementing SNNs, since their performance is limited
by the need to constantly shuttle data between physically separated logic and memory units. Hence,
novel computational architectures that address the von Neumann bottleneck are necessary in order
to build systems that can implement SNNs with low energy budgets. In this paper, we review some of
the architectural and system level design aspects involved in developing a new class of brain-inspired
information processing engines that mimic the time-based information encoding and processing
aspects of the brain. 