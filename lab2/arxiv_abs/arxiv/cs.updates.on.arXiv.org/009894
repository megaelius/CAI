For several decades, electroencephalography (EEG) has featured as one of the most commonly used
tools in emotional state recognition via monitoring of distinctive brain activities. An array
of datasets have been generated with the use of diverse emotion-eliciting stimuli and the resulting
brainwave responses conventionally captured with high-end EEG devices. However, the applicability
of these devices is to some extent limited by practical constraints and may prove difficult to be
deployed in highly mobile context omnipresent in everyday happenings. In this study, we evaluate
the potential of OpenBCI to bridge this gap by first comparing its performance to research grade
EEG system, employing the same algorithms that were applied on benchmark datasets. Moreover, for
the purpose of emotion classification, we propose a novel method to facilitate the selection of
audio-visual stimuli of high/low valence and arousal. Our setup entailed recruiting 200 healthy
volunteers of varying years of age to identify the top 60 affective video clips from a total of 120
candidates through standardized self assessment, genre tags, and unsupervised machine learning.
Additional 43 participants were enrolled to watch the pre-selected clips during which emotional
EEG brainwaves and peripheral physiological signals were collected. These recordings were analyzed
and extracted features fed into a classification model to predict whether the elicited signals
were associated with a high or low level of valence and arousal. As it turned out, our prediction accuracies
were decidedly comparable to those of previous studies that utilized more costly EEG amplifiers
for data acquisition. 