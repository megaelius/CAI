For many people suffering from motor disabilities, assistive devices controlled with only brain
activity are the only way to interact with their environment. Natural tasks often require different
kinds of interactions, involving different controllers the user should be able to select in a self-paced
way. We developed a Brain-Computer Interface (BCI) allowing users to switch between four control
modes in a self-paced way in real-time. Since the system is devised to be used in domestic environments
in a user-friendly way, we selected non-invasive electroencephalographic (EEG) signals and convolutional
neural networks (CNNs), known for their ability to find the optimal features in classification
tasks. We tested our system using the Cybathlon BCI computer game, which embodies all the challenges
inherent to real-time control. Our preliminary results show that an efficient architecture (SmallNet),
with only one convolutional layer, can classify 4 mental activities chosen by the user. The BCI system
is run and validated online. It is kept up-to-date through the use of newly collected signals along
playing, reaching an online accuracy of 47.6% where most approaches only report results obtained
offline. We found that models trained with data collected online better predicted the behaviour
of the system in real-time. This suggests that similar (CNN based) offline classifying methods
found in the literature might experience a drop in performance when applied online. Compared to
our previous decoder of physiological signals relying on blinks, we increased by a factor 2 the amount
of states among which the user can transit, bringing the opportunity for finer control of specific
subtasks composing natural grasping in a self-paced way. Our results are comparable to those shown
at the Cybathlon's BCI Race but further improvements on accuracy are required. 