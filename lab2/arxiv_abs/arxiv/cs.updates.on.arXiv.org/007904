We initiate the study of numerical linear algebra in the sliding window model, where only the most
recent $W$ updates in the data stream form the underlying set. Although most existing work in the
sliding window model uses the smooth histogram framework, most interesting linear-algebraic
problems are not smooth; we show that the spectral norm, vector induced matrix norms, generalized
regression, and low-rank approximation are not amenable for the smooth histogram framework. To
overcome this challenge, we first give a deterministic algorithm that achieves spectral approximation
in the sliding window model that can be viewed as a generalization of smooth histograms, using the
Loewner ordering of PSD matrices. We then give algorithms for both spectral approximation and low-rank
approximation that are space-optimal up to polylogarithmic factors. Our algorithms are based
on a new notion of "reverse online" leverage scores that account for both how unique and how recent
a row is, while preserving sparsity so that both our algorithms run in input sparsity runtime, up
to lower order factors. We show that our techniques have applications to linear-algebraic problems
in other settings. Specifically, we show that our analysis immediately implies an algorithm for
low-rank approximation in the online setting that is space-optimal up to logarithmic factors,
as well as nearly input sparsity time. We show our deterministic spectral approximation algorithm
can be used to handle $\ell_1$ spectral approximation in the sliding window model under a certain
assumption on the bit complexity of the entries. Finally, we show that our downsampling framework
can be applied to the problem of approximate matrix multiplication and provide upper and lower bounds
that are tight up to $\log\log W$ factors. 