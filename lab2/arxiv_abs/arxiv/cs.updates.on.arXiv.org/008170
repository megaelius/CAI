As we increasingly delegate decision-making to algorithms, whether directly or indirectly, important
questions emerge in circumstances where those decisions have direct consequences for individual
rights and personal opportunities, as well as for the collective good. A key problem for policymakers
is that the social implications of these new methods can only be grasped if there is an adequate comprehension
of their general technical underpinnings. The discussion here focuses primarily on the case of
enforcement decisions in the criminal justice system, but draws on similar situations emerging
from other algorithms utilised in controlling access to opportunities, to explain how machine
learning works and, as a result, how decisions are made by modern intelligent algorithms or 'classifiers'.
It examines the key aspects of the performance of classifiers, including how classifiers learn,
the fact that they operate on the basis of correlation rather than causation, and that the term 'bias'
in machine learning has a different meaning to common usage. An example of a real world 'classifier',
the Harm Assessment Risk Tool (HART), is examined, through identification of its technical features:
the classification method, the training data and the test data, the features and the labels, validation
and performance measures. Four normative benchmarks are then considered by reference to HART:
(a) prediction accuracy (b) fairness and equality before the law (c) transparency and accountability
(d) informational privacy and freedom of expression, in order to demonstrate how its technical
features have important normative dimensions that bear directly on the extent to which the system
can be regarded as a viable and legitimate support for, or even alternative to, existing human decision-makers.
