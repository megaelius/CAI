Predicting depth from a single image is an attractive research topic since it provides one more dimension
of information to enable machines to better perceive the world. Recently, deep learning has emerged
as an effective approach to monocular depth estimation. As obtaining labeled data is costly, there
is a recent trend to move from supervised learning to unsupervised learning to obtain monocular
depth. However, most unsupervised learning methods capable of achieving high depth prediction
accuracy will require a deep network architecture which will be too heavy and complex to run on embedded
devices with limited storage and memory spaces. To address this issue, we propose a new powerful
network with a recurrent module to achieve the capability of a deep network while at the same time
maintaining an extremely lightweight size for real-time high performance unsupervised monocular
depth prediction from video sequences. Besides, a novel efficient upsample block is proposed to
fuse the features from the associated encoder layer and recover the spatial size of features with
the small number of model parameters. We validate the effectiveness of our approach via extensive
experiments on the KITTI dataset. Our new model can run at a speed of about 110 frames per second (fps)
on a single GPU, 37 fps on a single CPU, and 2 fps on a Raspberry Pi 3. Moreover, it achieves higher depth
accuracy with nearly 33 times fewer model parameters than state-of-the-art models. To the best
of our knowledge, this work is the first extremely lightweight neural network trained on monocular
video sequences for real-time unsupervised monocular depth estimation, which opens up the possibility
of implementing deep learning-based real-time unsupervised monocular depth prediction on low-cost
embedded devices. 