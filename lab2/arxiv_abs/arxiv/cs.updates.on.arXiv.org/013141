In recent years, deep learning based visual tracking methods have obtained great success owing
to the powerful feature representation ability of Convolutional Neural Networks (CNNs). Among
these methods, classification-based tracking methods exhibit excellent performance while their
speeds are heavily limited by the expensive computation for massive proposal feature extraction.
In contrast, matching-based tracking methods (such as Siamese networks) possess remarkable speed
superiority. However, the absence of online updating renders these methods unadaptable to significant
object appearance variations. In this paper, we propose a novel real-time visual tracking method,
which adopts an object-adaptive LSTM network to effectively capture the video sequential dependencies
and adaptively learn the object appearance variations. For high computational efficiency, we
also present a fast proposal selection strategy, which utilizes the matching-based tracking method
to pre-estimate dense proposals and selects high-quality ones to feed to the LSTM network for classification.
This strategy efficiently filters out some irrelevant proposals and avoids the redundant computation
for feature extraction, which enables our method to operate faster than conventional classification-based
tracking methods. In addition, to handle the problems of sample inadequacy and class imbalance
during online tracking, we adopt a data augmentation technique based on the Generative Adversarial
Network (GAN) to facilitate the training of the LSTM network. Extensive experiments on four visual
tracking benchmarks demonstrate the state-of-the-art performance of our method in terms of both
tracking accuracy and speed, which exhibits great potentials of recurrent structures for visual
tracking. 