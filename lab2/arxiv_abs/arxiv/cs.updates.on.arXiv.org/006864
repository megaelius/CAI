Characterizing human values is a topic deeply interwoven with the sciences, humanities, art, and
many other human endeavors. In recent years, a number of thinkers have argued that accelerating
trends in computer science, cognitive science, and related disciplines foreshadow the creation
of intelligent machines which meet and ultimately surpass the cognitive abilities of human beings,
thereby entangling an understanding of human values with future technological development. Contemporary
research accomplishments suggest sophisticated AI systems becoming widespread and responsible
for managing many aspects of the modern world, from preemptively planning users' travel schedules
and logistics, to fully autonomous vehicles, to domestic robots assisting in daily living. The
extrapolation of these trends has been most forcefully described in the context of a hypothetical
"intelligence explosion," in which the capabilities of an intelligent software agent would rapidly
increase due to the presence of feedback loops unavailable to biological organisms. The possibility
of superintelligent agents, or simply the widespread deployment of sophisticated, autonomous
AI systems, highlights an important theoretical problem: the need to separate the cognitive and
rational capacities of an agent from the fundamental goal structure, or value system, which constrains
and guides the agent's actions. The "value alignment problem" is to specify a goal structure for
autonomous agents compatible with human values. In this brief article, we suggest that recent ideas
from affective neuroscience and related disciplines aimed at characterizing neurological and
behavioral universals in the mammalian class provide important conceptual foundations relevant
to describing human values. We argue that the notion of "mammalian value systems" points to a potential
avenue for fundamental research in AI safety and AI ethics. 