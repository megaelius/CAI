Single image deraining is an important yet challenging issue due to the complex and diverse rain
structures in real scenes. Currently, the state-of-the-art performance on this task is achieved
by deep learning (DL)-based methods that mainly benefit from abundant pre-collected paired rainy-clean
samples either manually synthesized or semi-automatically generated under human supervision.
This tends to bring a large labor for data collection and more importantly, such manner neglects
to elaborately explore the intrinsic generative mechanism of rain streaks which should be related
to the most insightful understanding of the task. Against this issue, we investigate the generative
process of rainy image and construct a full Bayesian generative model for generating rains from
automatically extracted latent variables that represent physical structural factors for depicting
rains, like direction, scale, and thickness. To solve this model, we propose an algorithm where
the posteriors of latent variables are parameterized as CNNs and all the involved parameters can
be inferred under a concise variational inference framework in a data-driven manner. Especially,
the rain layer is modeled as an implicit distribution, parameterized as a generator, which avoids
subjective prior assumptions on rains as in traditional model-based methods. More practically,
from the learned generator, rain patches can be automatically generated and utilized to simulate
diverse training pairs so as to enrich and augment the existing benchmark datasets. Comprehensive
experiments substantiate that the proposed model has fine capability of generating plausible
samples that not only helps significantly improve the deraining performance of current DL-based
single image derainers, but also largely loosens the requirement of large training sample pre-collection
for the task. 