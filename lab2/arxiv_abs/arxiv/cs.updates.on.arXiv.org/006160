Generative models using neural network have opened a door to large-scale studies for various application
domains, especially for studies that suffer from lack of real samples to obtain statistically robust
inference. Typically, these generative models would train on existing data to learn the underlying
distribution of the measurements (e.g., images) in latent spaces conditioned on covariates (e.g.,
image labels), and generate independent samples that are identically distributed in the latent
space. Such models may work for cross-sectional studies, however, they are not suitable to generate
data for longitudinal studies that focus on "progressive" behavior in a sequence of data. In practice,
this is a quite common case in various neuroimaging studies whose goal is to characterize a trajectory
of pathologies of a specific disease even from early stages. This may be too ambitious especially
when the sample size is small (e.g., up to a few hundreds). Motivated from the setup above, we seek
to develop a conditional generative model for longitudinal data generation by designing an invertable
neural network. Inspired by recurrent nature of longitudinal data, we propose a novel neural network
that incorporates recurrent subnetwork and context gating to include smooth transition in a sequence
of generated data. Our model is validated on a video sequence dataset and a longitudinal AD dataset
with various experimental settings for qualitative and quantitative evaluations of the generated
samples. The results with the AD dataset captures AD specific group differences with sufficiently
generated longitudinal samples that are consistent with existing literature, which implies a
great potential to be applicable to other disease studies. 