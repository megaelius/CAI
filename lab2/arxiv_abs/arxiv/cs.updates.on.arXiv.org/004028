Timbre spaces have been used in music perception to study the relationships between instruments
based on dissimilarity ratings. However, these spaces do not generalize, need to be reconstructed
for each novel example and are not continuous, preventing audio synthesis. In parallel, generative
models have aimed to provide methods for synthesizing novel timbres. However, these systems do
not provide an explicit control structure, nor do they provide an understanding of their inner workings
and are not related to any perceptually relevant information. Here, we show that Variational Auto-Encoders
(VAE) can alleviate these limitations by constructing generative timbre spaces. To do so, we adapt
VAEs to create a generative latent space, while using perceptual ratings from timbre studies to
regularize the organization of this space. The resulting space allows to analyze novel instruments,
while being able to synthesize audio from any point of this space. We introduce a specific regularization
allowing to directly enforce given similarity ratings onto these spaces. We compare the resulting
space to existing timbre spaces and show that they provide almost similar distance relationships.
We evaluate several spectral transforms and show that the Non-Stationary Gabor Transform (NSGT)
provides the highest correlation to timbre spaces and the best quality of synthesis. We show that
these spaces can generalize to novel instruments and can generate any path between instruments
to understand their timbre relationships. As these spaces are continuous, we study how the traditional
acoustic descriptors behave along the latent dimensions. We show that descriptors have an overall
non-linear topology, but follow a locally smooth evolution. Based on this, we introduce a method
for descriptor-based synthesis and show that we can control the descriptors of an instrument while
keeping its timbre structure. 