Evaluating, explaining, and visualizing high-level concepts in generative models, such as variational
autoencoders (VAEs), is challenging in part due to a lack of known prediction classes that are required
to generate saliency maps in supervised learning. While saliency maps may help identify relevant
features (e.g., pixels) in the input for classification tasks of deep neural networks, similar
frameworks are understudied in unsupervised learning. Therefore, we introduce a new method of
obtaining saliency maps for latent representations of known or novel high-level concepts, often
called concept vectors in generative models. Concept scores, analogous to class scores in classification
tasks, are defined as dot products between concept vectors and encoded input data, which can be readily
used to compute the gradients. The resulting concept saliency maps are shown to highlight input
features deemed important for high-level concepts. Our method is applied to the VAE's latent space
of CelebA dataset in which known attributes such as "smiles" and "hats" are used to elucidate relevant
facial features. Furthermore, our application to spatial transcriptomic (ST) data of a mouse olfactory
bulb demonstrates the potential of latent representations of morphological layers and molecular
features in advancing our understanding of complex biological systems. By extending the popular
method of saliency maps to generative models, the proposed concept saliency maps help improve interpretability
of latent variable models in deep learning. Codes to reproduce and to implement concept saliency
maps: https://github.com/lenbrocki/concept-saliency-maps 