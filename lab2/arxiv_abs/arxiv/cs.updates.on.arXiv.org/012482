With the emergence of a spectrum of high-end mobile devices, many applications that formerly required
desktop-level computation capability are being transferred to these devices. However, executing
the inference of Deep Neural Networks (DNNs) is still challenging considering high computation
and storage demands, specifically, if real-time performance with high accuracy is needed. Weight
pruning of DNNs is proposed, but existing schemes represent two extremes in the design space: non-structured
pruning is fine-grained, accurate, but not hardware friendly; structured pruning is coarse-grained,
hardware-efficient, but with higher accuracy loss. In this paper, we introduce a new dimension,
fine-grained pruning patterns inside the coarse-grained structures, revealing a previously
unknown point in design space. With the higher accuracy enabled by fine-grained pruning patterns,
the unique insight is to use the compiler to re-gain and guarantee high hardware efficiency. In other
words, our method achieves the best of both worlds, and is desirable across theory/algorithm, compiler,
and hardware levels. The proposed PatDNN is an end-to-end framework to efficiently execute DNN
on mobile devices with the help of a novel model compression technique (pattern-based pruning based
on extended ADMM solution framework) and a set of thorough architecture-aware compiler- and code
generation-based optimizations (filter kernel reordering, compressed weight storage, register
load redundancy elimination, and parameter auto-tuning). Evaluation results demonstrate that
PatDNN outperforms three state-of-the-art end-to-end DNN frameworks, TensorFlow Lite, TVM,
and Alibaba Mobile Neural Network with speedup up to 44.5x, 11.4x, and 7.1x, respectively, with
no accuracy compromise. Real-time inference of representative large-scale DNNs (e.g., VGG-16,
ResNet-50) can be achieved using mobile devices. 