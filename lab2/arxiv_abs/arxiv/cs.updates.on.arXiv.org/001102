Sleep stage classification constitutes an important preliminary exam in the diagnosis of sleep
disorders and is traditionally performed by a sleep expert who assigns to each 30s of signal a sleep
stage, based on the visual inspection of signals such as electroencephalograms (EEG), electrooculograms
(EOG), electrocardiograms (ECG) and electromyograms (EMG). In this paper, we introduce the first
end-to-end deep learning approach that performs automatic temporal sleep stage classification
from multivariate and multimodal Polysomnography (PSG) signals. We build a general deep architecture
which can extract information from EEG, EOG and EMG channels and pools the learnt representations
into a final softmax classifier. The architecture is light enough to be distributed in time in order
to learn from the temporal context of each sample, namely previous and following data segments.
Our model, which is unique in its ability to learn a feature representation from multiple modalities,
is compared to alternative automatic approaches based on convolutional networks or decisions
trees. Results obtained on 61 publicly available PSG records with up to 20 EEG channels demonstrate
that our network architecture yields state-of-the-art performance. Our study reveals a number
of insights on the spatio-temporal distribution of the signal of interest: a good trade-off for
optimal classification performance measured with balanced accuracy is to use 6 EEG with some EOG
and EMG channels. Also exploiting one minute of data before and after each data segment to be classified
offers the strongest improvement when a limited number of channels is available. Our approach aims
to improve a key step in the study of sleep disorders. As sleep experts, our system exploits the multivariate
and multimodal character of PSG signals to deliver state-of-the-art classification performance
at a very low complexity cost. 