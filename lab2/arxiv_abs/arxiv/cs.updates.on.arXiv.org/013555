While making a tremendous impact in various fields, deep neural networks usually require large
amounts of labeled data for training which are expensive to collect in many applications, especially
in the medical domain. Unlabeled data, on the other hand, is much more abundant. Semi-supervised
learning techniques, such as co-training, could provide a powerful tool to leverage unlabeled
data. In this paper, we propose a novel framework, uncertainty-aware multi-view co-training (UMCT),
to address semi-supervised learning on 3D data, such as volumetric data from medical imaging. In
our work, co-training is achieved by exploiting multi-viewpoint consistency of 3D data. We generate
different views by rotating or permuting the 3D data and utilize asymmetrical 3D kernels to encourage
diversified features in different sub-networks. In addition, we propose an uncertainty-weighted
label fusion mechanism to estimate the reliability of each view's prediction with Bayesian deep
learning. As one view requires the supervision from other views in co-training, our self-adaptive
approach computes a confidence score for the prediction of each unlabeled sample in order to assign
a reliable pseudo label. Thus, our approach can take advantage of unlabeled data during training.
We show the effectiveness of our proposed semi-supervised method on several public datasets from
medical image segmentation tasks (NIH pancreas & LiTS liver tumor dataset). Meanwhile, a fully-supervised
method based on our approach achieved state-of-the-art performances on both the LiTS liver tumor
segmentation and the Medical Segmentation Decathlon (MSD) challenge, demonstrating the robustness
and value of our framework, even when fully supervised training is feasible. 