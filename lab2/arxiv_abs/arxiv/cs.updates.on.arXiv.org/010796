The 3D-zoom operation is the positive translation of the camera in the Z-axis, perpendicular to
the image plane. In contrast, the optical zoom changes the focal length and the digital zoom is used
to enlarge a certain region of an image to the original image size. In this paper, we are the first to
formulate an unsupervised 3D-zoom learning problem where images with an arbitrary zoom factor
can be generated from a given single image. An unsupervised framework is convenient, as it is a challenging
task to obtain a 3D-zoom dataset of natural scenes due to the need for special equipment to ensure
camera movement is restricted to the Z-axis. In addition, the objects in the scenes should not move
when being captured, which hinders the construction of a large dataset of outdoor scenes. We present
a novel unsupervised framework to learn how to generate arbitrarily 3D-zoomed versions of a single
image, not requiring a 3D-zoom ground truth, called the Deep 3D-Zoom Net. The Deep 3D-Zoom Net incorporates
the following features: (i) transfer learning from a pre-trained disparity estimation network
via a back re-projection reconstruction loss; (ii) a fully convolutional network architecture
that models depth-image-based rendering (DIBR), taking into account high-frequency details
without the need for estimating the intermediate disparity; and (iii) incorporating a discriminator
network that acts as a no-reference penalty for unnaturally rendered areas. Even though there is
no baseline to fairly compare our results, our method outperforms previous novel view synthesis
research in terms of realistic appearance on large camera baselines. We performed extensive experiments
to verify the effectiveness of our method on the KITTI and Cityscapes datasets. 