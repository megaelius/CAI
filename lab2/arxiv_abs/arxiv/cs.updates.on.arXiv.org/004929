Data-driven discovery of "hidden physics" -- i.e., machine learning of differential equation
models underlying observed data -- has recently been approached by embedding the discovery problem
into a Gaussian Process regression of spatial data, treating and discovering unknown equation
parameters as hyperparameters of a modified "physics informed" Gaussian Process kernel. This
kernel includes the parametrized differential operators applied to a prior covariance kernel.
We extend this framework to linear space-fractional differential equations. The methodology
is compatible with a wide variety of fractional operators in $\mathbb{R}^d$ and stationary covariance
kernels, including the Matern class, and can optimize the Matern parameter during training. We
provide a user-friendly and feasible way to perform fractional derivatives of kernels, via a unified
set of d-dimensional Fourier integral formulas amenable to generalized Gauss-Laguerre quadrature.
The implementation of fractional derivatives has several benefits. First, it allows for discovering
fractional-order PDEs for systems characterized by heavy tails or anomalous diffusion, bypassing
the analytical difficulty of fractional calculus. Data sets exhibiting such features are of increasing
prevalence in physical and financial domains. Second, a single fractional-order archetype allows
for a derivative of arbitrary order to be learned, with the order itself being a parameter in the regression.
This is advantageous even when used for discovering integer-order equations; the user is not required
to assume a "dictionary" of derivatives of various orders, and directly controls the parsimony
of the models being discovered. We illustrate on several examples, including fractional-order
interpolation of advection-diffusion and modeling relative stock performance in the S&P 500 with
alpha-stable motion via a fractional diffusion equation. 