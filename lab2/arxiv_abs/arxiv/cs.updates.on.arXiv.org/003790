The pervasiveness of Internet of Things results in vast volumes of personal data generated by smart
devices of users (data producers) such as smart phones, wearables and other embedded sensors. It
is a common requirement, especially for Big Data analytics systems, to transfer these large in scale
and distributed data to centralized computational systems for analysis. Nevertheless, third
parties that run and manage these systems (data consumers) do not always guarantee users' privacy.
Their primary interest is to improve utility that is usually a metric related to the performance,
costs and the quality of service. There are several techniques that mask user-generated data to
ensure privacy, e.g. differential privacy. Setting up a process for masking data, referred to in
this paper as a `privacy setting', decreases on the one hand the utility of data analytics, while,
on the other hand, increases privacy. This paper studies parameterizations of privacy-settings
that regulate the trade-off between maximum utility, minimum privacy and minimum utility, maximum
privacy, where utility refers to the accuracy in the approximations of aggregation functions.
Privacy settings can be universally applied as system-wide parameterizations and policies (homogeneous
data sharing). Nonetheless they can also be applied autonomously by each user or decided under the
influence of (monetary) incentives (heterogeneous data sharing). This latter diversity in data
sharing by informational self-determination plays a key role on the privacy-utility trajectories
as shown in this paper both theoretically and empirically. A generic and novel computational framework
is introduced for measuring privacy-utility trade-offs and their optimization. The framework
computes a broad spectrum of such trade-offs that form privacy-utility trajectories under homogeneous
and heterogeneous data sharing. 