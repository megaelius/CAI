In this paper we propose and investigate a novel nonlinear unit, called an $L_p$ unit, for deep neural
networks. The proposed $L_p$ unit receives signal from several projections of a subset of units
in the layer below and computes the normalized $L_p$ norm. We notice two interesting interpretations
of the $L_p$ unit. First, the proposed unit can be understood as a generalization of a number of conventional
pooling operators such as average, root-mean-square and max pooling widely used in, for instance,
convolutional neural networks (CNN), HMAX models and neocognitrons. Furthermore, the $L_p$ unit
is, to a certain degree, similar to the recently proposed maxout unit (Goodfellow et al., 2013) which
achieved the state-of-the-art object recognition results on a number of benchmark datasets. Secondly,
we provide a geometrical interpretation of the activation function based on which we argue that
the $L_p$ unit is more efficient at representing complex, nonlinear separating boundaries. Each
$L_p$ unit defines a superelliptic boundary, with its exact shape defined by the order $p$. We claim
that this makes it possible to model arbitrarily shaped, curved boundaries more efficiently by
combining a few $L_p$ units of different orders. This insight justifies the need of using or learning
different orders for each unit in the model. We empirically evaluate the proposed $L_p$ units on
a number of datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$ units achieves
the state-of-the-art results on a number of benchmark datasets. Furthermore, we evaluate the proposed
$L_p$ unit on the recently proposed deep recurrent neural networks (RNN). 