Multilayer (or deep) networks are powerful probabilistic models based on multiple stages of a linear
transform followed by a non-linear (possibly random) function. In general, the linear transforms
are defined by matrices and the non-linear functions are defined by information channels. These
models have gained great popularity due to their ability to characterize complex probabilistic
relationships arising in a wide variety of inference problems. The contribution of this paper is
a new method for analyzing the fundamental limits of statistical inference in settings where the
model is known. The validity of our method can be established in a number of settings and is conjectured
to hold more generally. A key assumption made throughout is that the matrices are drawn randomly
from orthogonally invariant distributions. Our method yields explicit formulas for 1) the mutual
information; 2) the minimum mean-squared error (MMSE); 3) the existence and locations of certain
phase-transitions with respect to the problem parameters; and 4) the stationary points for the
state evolution of approximate message passing algorithms. When applied to the special case of
models with multivariate Gaussian channels our method is rigorous and has close connections to
free probability theory for random matrices. When applied to the general case of non-Gaussian channels,
our method provides a simple alternative to the replica method from statistical physics. A key observation
is that the combined effects of the individual components in the model (namely the matrices and the
channels) are additive when viewed in a certain transform domain. 