Designing of touchless user interface is gaining popularity in various contexts. Using such interfaces,
users can interact with electronic devices even when the hands are dirty or non-conductive. Also,
user with partial physical disability can interact with electronic devices using such systems.
Research in this direction has got major boost because of the emergence of low-cost sensors such
as Leap Motion, Kinect or RealSense devices. In this paper, we propose a Leap Motion controller-based
methodology to facilitate rendering of 2D and 3D shapes on display devices. The proposed method
tracks finger movements while users perform natural gestures within the field of view of the sensor.
In the next phase, trajectories are analyzed to extract extended Npen++ features in 3D. These features
represent finger movements during the gestures and they are fed to unidirectional left-to-right
Hidden Markov Model (HMM) for training. A one-to-one mapping between gestures and shapes is proposed.
Finally, shapes corresponding to these gestures are rendered over the display using MuPad interface.
We have created a dataset of 5400 samples recorded by 10 volunteers. Our dataset contains 18 geometric
and 18 non-geometric shapes such as "circle", "rectangle", "flower", "cone", "sphere" etc. The
proposed methodology achieves an accuracy of 92.87% when evaluated using 5-fold cross validation
method. Our experiments revel that the extended 3D features perform better than existing 3D features
in the context of shape representation and classification. The method can be used for developing
useful HCI applications for smart display devices. 