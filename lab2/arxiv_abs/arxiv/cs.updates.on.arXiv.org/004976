We study a setting of reinforcement learning, where the state transition is a convex combination
of a stochastic continuous function and a deterministic function. Such a setting include as a special
case the stochastic state transition setting, namely the setting of deterministic policy gradient
(DPG). We firstly give a simple example to illustrate that the deterministic policy gradient may
not exist under deterministic state transitions, and introduce a theoretical technique to prove
the existence of the policy gradient in this generalized setting. Using this technique, we prove
that the deterministic policy gradient indeed exists for a certain set of discount factors, and
further prove two conditions that guarantee the existence for all discount factors. We then derive
a closed form of the policy gradient whenever exists. Interestingly, the form of the policy gradient
in such setting is equivalent to that in DPG. Furthermore, to overcome the challenge of high sample
complexity of DPG in this setting, we propose the Generalized Deterministic Policy Gradient (GDPG)
algorithm. The main innovation of the algorithm is to optimize a weighted objective of the original
Markov decision process (MDP) and an augmented MDP that simplifies the original one, and serves
as its lower bound. To solve the augmented MDP, we make use of the model-based methods which enable
fast convergence. We finally conduct extensive experiments comparing GDPG with state-of-the-art
methods on several standard benchmarks. Results demonstrate that GDPG substantially outperforms
other baselines in terms of both convergence and long-term rewards. 