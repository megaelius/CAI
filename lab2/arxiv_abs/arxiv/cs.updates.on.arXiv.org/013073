The role of robots in society keeps expanding, bringing with it the necessity of interacting and
communicating with humans. In order to keep such interaction intuitive, we provide automatic wayfinding
based on verbal navigational instructions. Our first contribution is the creation of a large-scale
dataset with verbal navigation instructions. To this end, we have developed an interactive visual
navigation environment based on Google Street View; we further design an annotation method to highlight
mined anchor landmarks and local directions between them in order to help annotators formulate
typical, human references to those. The annotation task was crowdsourced on the AMT platform, to
construct a new Talk2Nav dataset with $10,714$ routes. Our second contribution is a new learning
method. Inspired by spatial cognition research on the mental conceptualization of navigational
instructions, we introduce a soft dual attention mechanism defined over the segmented language
instructions to jointly extract two partial instructions -- one for matching the next upcoming
visual landmark and the other for matching the local directions to the next landmark. On the similar
lines, we also introduce spatial memory scheme to encode the local directional transitions. Our
work takes advantage of the advance in two lines of research: mental formalization of verbal navigational
instructions and training neural network agents for automatic way finding. Extensive experiments
show that our method significantly outperforms previous navigation methods. For demo video, dataset
and code, please refer to our project page: https://www.trace.ethz.ch/publications/2019/talk2nav/index.html
