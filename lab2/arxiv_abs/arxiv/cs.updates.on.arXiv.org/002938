Sign recognition is an integral part of autonomous cars. Any misclassification of traffic signs
can potentially lead to a multitude of disastrous consequences, ranging from a life-threatening
accident to a large-scale interruption of transportation services relying on autonomous cars.
In this paper, we propose and examine realistic security attacks against sign recognition systems
for Deceiving Autonomous caRs with Toxic Signs (we call the proposed attacks DARTS). Leveraging
the concept of adversarial examples, we modify innocuous signs/advertisements in the environment
in such a way that they seem normal to human observers but are interpreted as the adversary's desired
traffic sign by autonomous cars. Further, we pursue a fundamentally different perspective to attacking
autonomous cars, motivated by the observation that the driver and vehicle-mounted camera see the
environment from different angles (the camera commonly sees the road with a higher angle, e.g.,
from top of the car). We propose a novel attack against vehicular sign recognition systems: we create
signs that change as they are viewed from different angles, and thus, can be interpreted differently
by the driver and sign recognition. We extensively evaluate the proposed attacks under various
conditions: different distances, lighting conditions, and camera angles. We first examine our
attacks virtually, i.e., we check if the digital images of toxic signs can deceive the sign recognition
system. Further, we investigate the effectiveness of attacks in real-world settings: we print
toxic signs, install them in the environment, capture videos using a vehicle-mounted camera, and
process them using our sign recognition pipeline. 