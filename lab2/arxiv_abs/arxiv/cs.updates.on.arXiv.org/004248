Recent years have seen many algorithmic advances in the area of submodular optimization: (SO) $\min/\max~f(S):
S \in \mathcal{F}$, where $\mathcal{F}$ is a given family of feasible sets over a ground set $V$ and
$f:2^V \rightarrow \mathbb{R}$ is submodular. This progress has been coupled with a wealth of new
applications for these models. Our focus is on a more general class of multivariate submodular optimization
(MVSO): $\min/\max~f (S_1,\ldots,S_k): S_1 \uplus S_2 \uplus \cdots \uplus S_k \in \mathcal{F}$.
Here we use $\uplus$ to denote disjoint union and hence this model is attractive where resources
are being allocated across $k$ agents, who share a "joint" multivariate nonnegative objective
$f(S_1,\ldots,S_k)$ that captures some type of submodularity (i.e. diminishing returns) property.
In this paper we explore the extent to which the approximability of the multivariate problems are
linked to their single-agent versions, referred to informally as the multivariate gap. For maximization
we show that monotone (resp. nonmonotone) (MVSO) admits an $\alpha (1-1/e)$ (resp. $\alpha \cdot
0.385$) approximation whenever monotone (resp. nonmonotone) (SO) admits an $\alpha$-approximation
over the multilinear formulation; thus substantially expanding the family of tractable models.
Moreover, the $\alpha (1-1/e)$-approximation for monotone objectives is tight. We discuss several
families (such as matroids and $p$-systems) that have an (optimal) multivariate gap of 1. For minimization
we show that if (SO) admits a $\beta$-approximation over modular functions, then (MVSO) admits
a $\frac{\beta \cdot n}{1+(n-1)(1-c)}$-approximation where $c\in [0,1]$ denotes the curvature
of $f$. We show that this approximation is essentially tight even for $\mathcal{F}=\{V\}$. Finally,
(MVSO) admits an $\alpha k$-approximation whenever (SO) admits an $\alpha$-approximation over
the convex formulation. 