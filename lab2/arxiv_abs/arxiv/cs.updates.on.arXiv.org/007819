Purpose The course of surgical procedures is often unpredictable, making it difficult to estimate
the duration of procedures beforehand. A context-aware method that analyses the workflow of an
intervention online and automatically predicts the remaining duration would alleviate these
problems. As basis for such an estimate, information regarding the current state of the intervention
is required. Methods Today, the operating room contains a diverse range of sensors. During laparoscopic
interventions, the endoscopic video stream is an ideal source of such information. Extracting
quantitative information from the video is challenging though, due to its high dimensionality.
Other surgical devices (e.g. insufflator, lights, etc.) provide data streams which are, in contrast
to the video stream, more compact and easier to quantify. Though whether such streams offer sufficient
information for estimating the duration of surgery is uncertain. Here, we propose and compare methods,
based on convolutional neural networks, for continuously predicting the duration of laparoscopic
interventions based on unlabeled data, such as from endoscopic images and surgical device streams.
Results The methods are evaluated on 80 laparoscopic interventions of various types, for which
surgical device data and the endoscopic video are available. Here the combined method performs
best with an overall average error of 37% and an average halftime error of 28%. Conclusion In this
paper, we present, to our knowledge, the first approach for online procedure duration prediction
using unlabeled endoscopic video data and surgical device data in a laparoscopic setting. We also
show that a method incorporating both vision and device data performs better than methods based
only on vision, while methods only based on tool usage and surgical device data perform poorly, showing
the importance of the visual channel. 