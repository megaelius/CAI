The amount of data for processing and categorization grows at an ever increasing rate. At the same
time the demand for collaboration and transparency in organizations, government and businesses,
drives the release of data from internal repositories to the public or 3rd party domain. This in turn
increase the potential of sharing sensitive information. The leak of sensitive information can
potentially be very costly, both financially for organizations, but also for individuals. In this
work we address the important problem of sensitive information detection. Specially we focus on
detection in unstructured text documents. We show that simplistic, brittle rule sets for detecting
sensitive information only find a small fraction of the actual sensitive information. Furthermore
we show that previous state-of-the-art approaches have been implicitly tailored to such simplistic
scenarios and thus fail to detect actual sensitive content. We develop a novel family of sensitive
information detection approaches which only assumes access to labeled examples, rather than unrealistic
assumptions such as access to a set of generating rules or descriptive topical seed words. Our approaches
are inspired by the current state-of-the-art for paraphrase detection and we adapt deep learning
approaches over recursive neural networks to the problem of sensitive information detection.
We show that our context-based approaches significantly outperforms the family of previous state-of-the-art
approaches for sensitive information detection, so-called keyword-based approaches, on real-world
data and with human labeled examples of sensitive and non-sensitive documents. 