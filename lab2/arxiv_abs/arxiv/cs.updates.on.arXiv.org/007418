Typical attempts to improve the capability of visual place recognition techniques include the
use of multi-sensor fusion and integration of information over time from image sequences. These
approaches can improve performance but have disadvantages including the need for multiple physical
sensors and calibration processes, both for multiple sensors and for tuning the image matching
sequence length. In this paper we address these shortcomings with a novel "multi-sensor" fusion
approach applied to multiple image processing methods for a single visual image stream, combined
with a dynamic sequence matching length technique and an automatic processing method weighting
scheme. In contrast to conventional single method approaches, our approach reduces the performance
requirements of a single image processing methodology, instead requiring that within the suite
of image processing methods, at least one performs well in any particular environment. In comparison
to static sequence length techniques, the dynamic sequence matching technique enables reduced
localization latencies through analysis of recognition quality metrics when re-entering familiar
locations. We evaluate our approach on multiple challenging benchmark datasets, achieving superior
performance to two state-of-the-art visual place recognition systems across environmental changes
including winter to summer, afternoon to morning and night to day. Across the four benchmark datasets
our proposed approach achieves an average F1 score of 0.96, compared to 0.78 for NetVLAD and 0.49
for SeqSLAM. We provide source code for the multi-fusion method and present analysis explaining
how superior performance is achieved despite the multiple, disparate, image processing methods
all being applied to a single source of imagery, rather than to multiple separate sensors. 