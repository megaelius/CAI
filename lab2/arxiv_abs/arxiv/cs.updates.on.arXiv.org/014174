One major impediment to the wider use of deep learning for clinical decision making is the difficulty
of assigning a level of confidence to model predictions. Currently, deep Bayesian neural networks
and sparse Gaussian processes are the main two scalable uncertainty estimation methods. However,
deep Bayesian neural network suffers from lack of expressiveness, and more expressive models such
as deep kernel learning, which is an extension of sparse Gaussian process, captures only the uncertainty
from the higher level latent space. Therefore, the deep learning model under it lacks interpretability
and ignores uncertainty from the raw data. In this paper, we merge features of the deep Bayesian learning
framework with deep kernel learning to leverage the strengths of both methods for more comprehensive
uncertainty estimation. Through a series of experiments on predicting the first incidence of heart
failure, diabetes and depression applied to large-scale electronic medical records, we demonstrate
that our method is better at capturing uncertainty than both Gaussian processes and deep Bayesian
neural networks in terms of indicating data insufficiency and distinguishing true positive and
false positive predictions, with a comparable generalisation performance. Furthermore, by assessing
the accuracy and area under the receiver operating characteristic curve over the predictive probability,
we show that our method is less susceptible to making overconfident predictions, especially for
the minority class in imbalanced datasets. Finally, we demonstrate how uncertainty information
derived by the model can inform risk factor analysis towards model interpretability. 