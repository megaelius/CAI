Machine learning based Single Image Intrinsic Decomposition (SIID) methods decompose a captured
scene into its albedo and shading images by using the knowledge of a large set of known and realistic
ground truth decompositions. Collecting and annotating such a dataset is an approach that cannot
scale to sufficient variety and realism. We free ourselves from this limitation by training on unannotated
images. Our method leverages the observation that two images of the same scene but with different
lighting provide useful information on their intrinsic properties: by definition, albedo is invariant
to lighting conditions, and cross-combining the estimated albedo of a first image with the estimated
shading of a second one should lead back to the second one's input image. We transcribe this relationship
into a siamese training scheme for a deep convolutional neural network that decomposes a single
image into albedo and shading. The siamese setting allows us to introduce a new loss function including
such cross-combinations, and to train solely on (time-lapse) images, discarding the need for any
ground truth annotations. As a result, our method has the good properties of i) taking advantage
of the time-varying information of image sequences in the (pre-computed) training step, ii) not
requiring ground truth data to train on, and iii) being able to decompose single images of unseen
scenes at runtime. To demonstrate and evaluate our work, we additionally propose a new rendered
dataset containing illumination-varying scenes and a set of quantitative metrics to evaluate
SIID algorithms. Despite its unsupervised nature, our results compete with state of the art methods,
including supervised and non data-driven methods. 