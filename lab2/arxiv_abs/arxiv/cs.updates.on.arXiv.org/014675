Local learning methods are a popular class of machine learning algorithms. The basic idea for the
entire cadre is to choose some non-local model family, to train many of them on small sections of neighboring
data, and then to `stitch' the resulting models together in some way. Due to the limits of constraining
a training dataset to a small neighborhood, research on locally-learned models has largely been
restricted to simple model families. Also, since simple model families have no complex structure
by design, this has limited use of the individual local models to predictive tasks. We hypothesize
that, using a sufficiently complex local model family, various properties of the individual local
models, such as their learned parameters, can be used as features for further learning. This dissertation
improves upon the current state of research and works toward establishing this hypothesis by investigating
algorithms for localization of more complex model families and by studying their applications
beyond predictions as a feature extraction mechanism. We summarize this generic technique of using
local models as a feature extraction step with the term ``local model feature transformations.''
In this document, we extend the local modeling paradigm to Gaussian processes, orthogonal quadric
models and word embedding models, and extend the existing theory for localized linear classifiers.
We then demonstrate applications of local model feature transformations to epileptic event classification
from EEG readings, activity monitoring via chest accelerometry, 3D surface reconstruction, 3D
point cloud segmentation, handwritten digit classification and event detection from Twitter
feeds. 