Surgical data science is a new research field that aims to observe all aspects and factors of the patient
treatment process in order to provide the right assistance to the right person at the right time.
Due to the breakthrough successes of deep learning-based solutions for automatic image annotation,
the availability of reference annotations for algorithm training is becoming a major bottleneck
in the field. The purpose of this paper was to investigate the concept of self-supervised learning
to address this issue. Our approach is guided by the hypothesis that unlabeled video data can be used
to learn a representation of the target domain that boosts the performance of state-of-the-art
machine learning algorithms when used for pre-training. Essentially, this method involves an
auxiliary task that requires training with unlabeled endoscopic video data from the target domain
to initialize a convolutional neural network (CNN) for the target task. In this paper, we propose
to undertake a re-colorization of medical images with generative adversarial network (GAN)-based
architecture as an auxiliary task. A variant of the method involves a second pre-training step based
on labeled data for the target task from a related domain. We have validated both variants using medical
instrument segmentation as the target task. The proposed approach can be used to radically reduce
the manual annotation effort involved in training CNNs. Compared to the baseline approach of generating
annotated data from scratch, our method decreases exploratively the number of labeled images by
up to 60% without sacrificing performance. Our method also outperforms alternative methods for
CNN pre-training, such as pre-training on publicly available non-medical (COCO) or medical data
(MICCAI endoscopic vision challenge 2017) using the target task (in this instance: segmentation).
