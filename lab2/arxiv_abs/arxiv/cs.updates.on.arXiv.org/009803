Deep reinforcement learning has over the past few years shown great potential in learning near-optimal
control in complex simulated environments with little visible information. Rainbow (Q-Learning)
and PPO (Policy Optimisation) have shown outstanding performance in a variety of tasks, including
Atari 2600, MuJoCo, and Roboschool test suite. While these algorithms are fundamentally different,
both suffer from high variance, low sample efficiency, and hyperparameter sensitivity that in
practice, make these algorithms a no-go for critical operations in the industry. On the other hand,
model-based reinforcement learning focuses on learning the transition dynamics between states
in an environment. If these environment dynamics are adequately learned, a model-based approach
is perhaps the most sample efficient method for learning agents to act in an environment optimally.
The traits of model-based reinforcement are ideal for real-world environments where sampling
is slow and for mission-critical operations. In the warehouse industry, there is an increasing
motivation to minimise time and to maximise production. Currently, autonomous agents act suboptimally
using handcrafted policies for significant portions of the state-space. In this paper, we present
The Dreaming Variational Autoencoder v2 (DVAE-2), a model-based reinforcement learning algorithm
that increases sample efficiency, hence enable algorithms with low sample efficiency function
better in real-world environments. We introduce Deep Warehouse, a simulated environment for industry-near
testing of autonomous agents in grid-based warehouses. Finally, we illustrate that DVAE-2 improves
the sample efficiency for the Deep Warehouse compared to model-free methods. 