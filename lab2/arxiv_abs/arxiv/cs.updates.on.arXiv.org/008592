Continual learning is a critical ability of continually acquiring and transferring knowledge
without catastrophically forgetting previously learned knowledge. However, enabling continual
learning for AI remains a long-standing challenge. In this work, we propose a novel method, Prototype
Reminding, that efficiently embeds and recalls previously learnt knowledge to tackle catastrophic
forgetting issue. In particular, we consider continual learning in classification tasks. For
each classification task, our method learns a metric space containing a set of prototypes where
embedding of the samples from the same class cluster around prototypes and class-representative
prototypes are separated apart. To alleviate catastrophic forgetting, our method preserves the
embedding function from the samples to the previous metric space, through our proposed prototype
reminding from previous tasks. Specifically, the reminding process is implemented by replaying
a small number of samples from previous tasks and correspondingly matching their embedding to their
nearest class-representative prototypes. Compared with recent continual learning methods,
our contributions are fourfold: first, our method achieves the best memory retention capability
while adapting quickly to new tasks. Second, our method uses metric learning for classification,
and does not require adding in new neurons given new object classes. Third, our method is more memory
efficient since only class-representative prototypes need to be recalled. Fourth, our method
suggests a promising solution for few-shot continual learning. Without tampering with the performance
on initial tasks, our method learns novel concepts given a few training examples of each class in
new tasks. 