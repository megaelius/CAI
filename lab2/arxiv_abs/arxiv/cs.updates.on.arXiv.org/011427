Today, more and more, it is necessary that most applications and documents developed in previous
or current technologies to be accessible online on cloud-based infrastructures. That is why the
migration of legacy systems including their hosts of documents to new technologies and online infrastructures,
using modern Artificial Intelligence techniques, is absolutely necessary. With the advancement
of Artificial Intelligence and Deep Learning with its multitude of applications, a new area of research
is emerging - that of automated systems development and maintenance. The underlying work objective
that led to this paper aims to research and develop truly intelligent systems able to analyze user
interfaces from various sources and generate real and usable inferences ranging from architecture
analysis to actual code generation. One key element of such systems is that of artificial scene detection
and analysis based on deep learning computer vision systems. Computer vision models and particularly
deep directed acyclic graphs based on convolutional modules are generally constructed and trained
based on natural images datasets. Due to this fact, the models will develop during the training process
natural image feature detectors apart from the base graph modules that will learn basic primitive
features. In the current paper, we will present the base principles of a deep neural pipeline for
computer vision applied to artificial scenes (scenes generated by user interfaces or similar).
Finally, we will present the conclusions based on experimental development and benchmarking against
state-of-the-art transfer-learning implemented deep vision models. 