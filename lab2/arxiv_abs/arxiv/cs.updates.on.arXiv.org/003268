The practical impact of deep learning on complex supervised learning problems has been significant,
so much so that almost every Artificial Intelligence problem, or at least a portion thereof, has
been somehow recast as a deep learning problem. The applications appeal is significant, but this
appeal is increasingly challenged by what some call the challenge of explainability, or more generally
the more traditional challenge of debuggability: if the outcomes of a deep learning process produce
unexpected results (e.g., less than expected performance of a classifier), then there is little
available in the way of theories or tools to help investigate the potential causes of such unexpected
behavior, especially when this behavior could impact people's lives. We describe a preliminary
framework to help address this issue, which we call "deep visual explanation" (DVE). "Deep," because
it is the development and performance of deep neural network models that we want to understand. "Visual,"
because we believe that the most rapid insight into a complex multi-dimensional model is provided
by appropriate visualization techniques, and "Explanation," because in the spectrum from instrumentation
by inserting print statements to the abductive inference of explanatory hypotheses, we believe
that the key to understanding deep learning relies on the identification and exposure of hypotheses
about the performance behavior of a learned deep model. In the exposition of our preliminary framework,
we use relatively straightforward image classification examples and a variety of choices on initial
configuration of a deep model building scenario. By careful but not complicated instrumentation,
we expose classification outcomes of deep models using visualization, and also show initial results
for one potential application of interpretability. 