Consider the problem of learning, from non-experimental data, the causal (Markov equivalence)
structure of the true, unknown causal Bayesian network (CBN) on a given, fixed set of (categorical)
variables. This learning problem is known to be so hard that there is no learning algorithm that converges
to the truth for all possible CBNs (on the given set of variables). So the convergence property has
to be sacrificed for some CBNs---but for which? In response, the standard practice has been to design
and employ learning algorithms that secure the convergence property for at least all the CBNs that
satisfy the famous faithfulness condition, which implies sacrificing the convergence property
for some CBNs that violate the faithfulness condition (Spirtes et al. 2000). This standard design
practice can be justified by assuming---that is, accepting on faith---that the true, unknown CBN
satisfies the faithfulness condition. But the real question is this: Is it possible to explain,
without assuming the faithfulness condition or any of its weaker variants, why it is mandatory rather
than optional to follow the standard design practice? This paper aims to answer the above question
in the affirmative. We first define an array of modes of convergence to the truth as desiderata that
might or might not be achieved by a causal learning algorithm. Those modes of convergence concern
(i) how pervasive the domain of convergence is on the space of all possible CBNs and (ii) how uniformly
the convergence happens. Then we prove a result to the following effect: for any learning algorithm
that tackles the causal learning problem in question, if it achieves the best achievable mode of
convergence (considered in this paper), then it must follow the standard design practice of converging
to the truth for at least all CBNs that satisfy the faithfulness condition---it is a requirement,
not an option. 