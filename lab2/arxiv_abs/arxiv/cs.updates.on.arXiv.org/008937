Autonomous vehicles (AVs) are key for the intelligent mobility of the future. A crucial component
of an AV is the artificial intelligence (AI) able to drive towards a desired destination. Today,
there are different paradigms addressing the development of AI drivers. On the one hand, we find
modular pipelines, which divide the driving task into sub-tasks such as perception (object detection,
semantic segmentation, depth estimation, tracking) and maneuver control (local path planing
and control). On the other hand, we find end-to-end driving approaches that try to learn a direct
mapping from input raw sensor data to vehicle control signals (the steering angle). The later are
relatively less studied, but are gaining popularity since they are less demanding in terms of sensor
data annotation. This paper focuses on end-to-end autonomous driving. So far, most proposals relying
on this paradigm assume RGB images as input sensor data. However, AVs will not be equipped only with
cameras, but also with active sensors providing accurate depth information (traditional LiDARs,
or new solid state ones). Accordingly, this paper analyses if RGB and depth data, RGBD data, can actually
act as complementary information in a multimodal end-to-end driving approach, producing a better
AI driver. Using the CARLA simulator functionalities, its standard benchmark, and conditional
imitation learning (CIL), we will show how, indeed, RGBD gives rise to more successful end-to-end
AI drivers. We will compare the use of RGBD information by means of early, mid and late fusion schemes,
both in multisensory and single-sensor (monocular depth estimation) settings. 