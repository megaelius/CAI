In-memory (transactional) data stores are recognized as a first-class data management technology
for cloud platforms, thanks to their ability to match the elasticity requirements imposed by the
pay-as-you-go cost model. On the other hand, defining the well-suited amount of cache servers to
be deployed, and the degree of in-memory replication of slices of data, in order to optimize reliability/availability
and performance tradeoffs, is far from being a trivial task. Yet, it is an essential aspect of the
provisioning process of cloud platforms, given that it has an impact on how well cloud resources
are actually exploited. To cope with the issue of determining optimized configurations of cloud
in-memory data stores, in this article we present a flexible simulation framework offering skeleton
simulation models that can be easily specialized in order to capture the dynamics of diverse data
grid systems, such as those related to the specific protocol used to provide data consistency and/or
transactional guarantees. Besides its flexibility, another peculiar aspect of the framework
lies in that it integrates simulation and machine-learning (black-box) techniques, the latter
being essentially used to capture the dynamics of the data-exchange layer (e.g. the message passing
layer) across the cache servers. This is a relevant aspect when considering that the actual data-transport/networking
infrastructure on top of which the data grid is deployed might be unknown, hence being not feasible
to be modeled via white-box (namely purely simulative) approaches. We also provide an extended
experimental study aimed at validating instances of simulation models supported by our framework
against execution dynamics of real data grid systems deployed on top of either private or public
cloud infrastructures. 