Learning-to-rank (LTR) has become a key technology in E-commerce applications. Most existing
LTR approaches follow a supervised learning paradigm, relying on labeled data collected from a
specific online system. However, the online performance of the models may be inconsistent with
that observed in offline evaluation. This inconsistency is serious: even if we successfully reproduce
the offline performance of a newly proposed model, it may perform poorly when deployed in online
systems. Interacting with online systems is an accurate albeit costly way of evaluation as the process
may hurt user experience. To avoid the inaccuracy of offline evaluation and the cost of the online
interaction-based evaluation, we propose an evaluator-generator framework. Firstly, we train
an evaluator model as an approximation of online systems with offline data. Secondly, we learn a
generator with supervision from the evaluator, which can approximate the maximal online performance
through reinforcement learning. Through extensive experience, we show that the classic data-based
metrics on the validation set can be inconsistent with online performance, and can even be misleading.
We also demonstrate that the proposed evaluator score is significantly more robust than common
ranking metrics: classic metrics do not match the actual performance in both an offline simulated
environment and a real online system while our evaluator score matches them well. Finally, we show
that our method achieves a significant improvement of ($\textgreater2\%$) over the current industrial-level
pair-wise model in terms of both Conversion Rate (CR) and Gross Merchandise Volume (GMV) in online
A/B tests on AliExpress Search. Roughly, it contributes more than 300 millions dollars to GMV per
year in the common daily selling. 