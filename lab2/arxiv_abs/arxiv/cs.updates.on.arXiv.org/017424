Person re-identification has seen significant advancement in recent years. However, the ability
of learned models to generalize to unknown target domains still remains limited. One possible reason
for this is the lack of large-scale and diverse source training data, since manually labeling such
a dataset is very expensive and privacy sensitive. To address this, we propose to automatically
synthesize a large-scale person re-identification dataset following a set-up similar to real
surveillance but with virtual environments, and then use the synthesized person images to train
a generalizable person re-identification model. Specifically, we design a method to generate
a large number of random UV texture maps and use them to create different 3D clothing models. Then,
an automatic code is developed to randomly generate various different 3D characters with diverse
clothes, races and attributes. Next, we simulate a number of different virtual environments using
Unity3D, with customized camera networks similar to real surveillance systems, and import multiple
3D characters at the same time, with various movements and interactions along different paths through
the camera networks. As a result, we obtain a virtual dataset, called RandPerson, with 1,801,816
person images of 8,000 identities. By training person re-identification models on these synthesized
person images, we demonstrate, for the first time, that models trained on virtual data can generalize
well to unseen target images, surpassing the models trained on various real-world datasets, including
CUHK03, Market-1501, DukeMTMC-reID, and almost MSMT17. The RandPerson dataset is available at
https://github.com/VideoObjectSearch/RandPerson. 