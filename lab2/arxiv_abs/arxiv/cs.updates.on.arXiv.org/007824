The search for an application of near-term quantum devices is widespread. Quantum Machine Learning
is touted as a potential utilisation of such devices, particularly those which are out of the reach
of the simulation capabilities of classical computers. In this work, we propose a generative Quantum
Machine Learning Model, called the Ising Born Machine (IBM), which we show cannot, in the worst case,
and up to suitable notions of error, be simulated efficiently by a classical device. We also show
this holds for all the circuit families encountered during training. In particular, we explore
quantum circuit learning using non-universal circuits derived from Ising Model Hamiltonians,
which are implementable on near term quantum devices. We propose two novel training methods for
the IBM by utilising the Stein Discrepancy and the Sinkhorn Divergence cost functions. We show numerically,
both using a simulator within Rigetti's Forest platform and on the Aspen-1 16Q chip, that the cost
functions we suggest outperform the more commonly used Maximum Mean Discrepancy (MMD) for differentiable
training. We also propose an improvement to the MMD by proposing a novel utilisation of quantum kernels
which we demonstrate provides improvements over its classical counterpart. We discuss the potential
of these methods to learn `hard' quantum distributions, a feat which would demonstrate the advantage
of quantum over classical computers, and provide the first formal definitions for what we call `Quantum
Learning Supremacy'. Finally, we propose a novel view on the area of quantum circuit compilation
by using the IBM to `mimic' target quantum circuits using classical output data only. 