Gated Recurrent Unit (GRU) is a recently-developed variation of the long short-term memory (LSTM)
unit, both of which are types of recurrent neural network (RNN). Through empirical evidence, both
models have been proven to be effective in a wide variety of machine learning tasks such as natural
language processing (Wen et al., 2015), speech recognition (Chorowski et al., 2015), and text classification
(Yang et al., 2016). Conventionally, like most neural networks, both of the aforementioned RNN
variants employ the Softmax function as its final output layer for its prediction, and the cross-entropy
function for computing its loss. In this paper, we present an amendment to this norm by introducing
linear support vector machine (SVM) as the replacement for Softmax in the final output layer of a
GRU model. Furthermore, the cross-entropy function shall be replaced with a margin-based function.
While there have been similar studies (Alalshekmubarak & Smith, 2013; Tang, 2013), this proposal
is primarily intended for binary classification on intrusion detection using the 2013 network
traffic data from the honeypot systems of Kyoto University. Results show that the GRU-SVM model
performs relatively higher than the conventional GRU-Softmax model. The proposed model reached
a training accuracy of ~81.54% and a testing accuracy of ~84.15%, while the latter was able to reach
a training accuracy of ~63.07% and a testing accuracy of ~70.75%. In addition, the juxtaposition
of these two final output layers indicate that the SVM would outperform Softmax in prediction time
- a theoretical implication which was supported by the actual training and testing time in the study.
