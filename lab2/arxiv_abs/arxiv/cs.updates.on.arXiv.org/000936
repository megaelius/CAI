Feature selection involes identifying the most relevant subset of input features, with a view to
improving generalization of predictive models by reducing overfitting. Directly searching for
the most relevant combination of attributes is NP-hard. Variable selection is of critical importance
in many applications, such as micro-array data analysis, where selecting a small number of discriminative
features is crucial to developing useful models of disease mechanisms, as well as for prioritizing
targets for drug discovery. The recently proposed Minimal Complexity Machine (MCM) provides a
way to learn a hyperplane classifier by minimizing an exact (\boldmath{$\Theta$}) bound on its
VC dimension. It is well known that a lower VC dimension contributes to good generalization. For
a linear hyperplane classifier in the input space, the VC dimension is upper bounded by the number
of features; hence, a linear classifier with a small VC dimension is parsimonious in the set of features
it employs. In this paper, we use the linear MCM to learn a classifier in which a large number of weights
are zero; features with non-zero weights are the ones that are chosen. Selected features are used
to learn a kernel SVM classifier. On a number of benchmark datasets, the features chosen by the linear
MCM yield comparable or better test set accuracy than when methods such as ReliefF and FCBF are used
for the task. The linear MCM typically chooses one-tenth the number of attributes chosen by the other
methods; on some very high dimensional datasets, the MCM chooses about $0.6\%$ of the features;
in comparison, ReliefF and FCBF choose 70 to 140 times more features, thus demonstrating that minimizing
the VC dimension may provide a new, and very effective route for feature selection and for learning
sparse representations. 