Deep neural networks are ubiquitous due to the ease of developing models and their influence on other
domains. At the heart of this progress is convolutional neural networks (CNNs) that are capable
of learning representations or features given a set of data. Making sense of such complex models
(i.e., millions of parameters and hundreds of layers) remains challenging for developers as well
as the end-users. This is partially due to the lack of tools or interfaces capable of providing interpretability
and transparency. A growing body of literature, for example, class activation map (CAM), focuses
on making sense of what a model learns from the data or why it behaves poorly in a given task. This paper
builds on previous ideas to cope with the increasing demand for interpretable, robust, and transparent
models. Our approach provides a simpler and intuitive (or familiar) way of generating CAM. The proposed
Eigen-CAM computes and visualizes the principle components of the learned features/representations
from the convolutional layers. Empirical studies were performed to compare the Eigen-CAM with
the state-of-the-art methods (such as Grad-CAM, Grad-CAM++, CNN-fixations) by evaluating on
benchmark datasets such as weakly-supervised localization and localizing objects in the presence
of adversarial noise. Eigen-CAM was found to be robust against classification errors made by fully
connected layers in CNNs, does not rely on the backpropagation of gradients, class relevance score,
maximum activation locations, or any other form of weighting features. In addition, it works with
all CNN models without the need to modify layers or retrain models. Empirical results show up to 12%
improvement over the best method among the methods compared on weakly supervised object localization.
