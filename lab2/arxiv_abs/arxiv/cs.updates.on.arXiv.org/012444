A fundamental goal in deep learning is the characterization of trainability and generalization
of neural networks as a function of their architecture and hyperparameters. In this paper, we discuss
these challenging issues in the context of wide neural networks at large depths where we will see
that the situation simplifies considerably. To do this, we leverage recent advances that have separately
shown: (1) that in the wide network limit, random networks before training are Gaussian Processes
governed by a kernel known as the Neural Network Gaussian Process (NNGP) kernel, (2) that at large
depths the spectrum of the NNGP kernel simplifies considerably and becomes "weakly data-dependent"
and (3) that gradient descent training of wide neural networks is described by a kernel called the
Neural Tangent Kernel (NTK) that is related to the NNGP. Here we show that in the large depth limit
the spectrum of the NTK simplifies in much the same way as that of the NNGP kernel. By analyzing this
spectrum, we arrive at a precise characterization of trainability and a necessary condition for
generalization across a range of architectures including Fully Connected Networks (FCNs) and
Convolutional Neural Networks (CNNs). In particular, we find that there are large regions of hyperparameter
space where networks can only memorize the training set in the sense they reach perfect training
accuracy but completely fail to generalize outside the training set, in contrast with several recent
results. By comparing CNNs with- and without-global average pooling, we show that CNNs without
average pooling have very nearly identical learning dynamics to FCNs while CNNs with pooling contain
a correction that alters its generalization performance. We perform a thorough empirical investigation
of these theoretical results and finding excellent agreement on real datasets. 