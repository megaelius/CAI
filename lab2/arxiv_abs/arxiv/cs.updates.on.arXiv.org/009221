This paper presents a national case study-based analysis of the numerous dimensions to cybersecurity
education and how they are implemented and accredited; from understanding the interaction of hardware
and software, moving from theory to practice (and vice versa), to human factors, policy and politics
(as well as other important facets). A multitude of model curricula and recommendations have been
presented and discussed in international fora in recent years, with varying levels of impact on
education, policy and practice. This paper address three key questions: i) What is taught and what
should be taught for cybersecurity to general computer science students; ii) Should cybersecurity
be taught stand-alone or in an integrated manner to general computer science students; and iii)
Can accreditation by national professional, statutory and regulatory bodies enhance the provision
of cybersecurity within a body's jurisdiction? Evaluating how cybersecurity is taught in all aspects
of computer science is clearly a task of considerable size, one that is beyond the scope of this paper.
Instead a case study-based research approach, primarily focusing on the UK, has been adopted to
evaluate the evidence of the teaching of cybersecurity within general computer science to university-level
students. Thus, in the context of widespread international computer science/engineering curriculum
reform, what does this need to embed cybersecurity mean more generally for institutions and educators,
and how can we teach this subject more effectively? Through this UK case study, and by contrasting
with the US, we demonstrate the positive effect that national accreditation requirements can have,
and give some recommendations both for future research and curriculum developments. 