This paper studies audio-visual deep saliency prediction. It introduces a conceptually simple
and effective Deep Audio-Visual Embedding for dynamic saliency prediction dubbed ``DAVE" in conjunction
with our efforts towards building an Audio-Visual Eye-tracking corpus named ``AVE". Despite existing
a strong relation between auditory and visual cues for guiding gaze during perception, video saliency
models only consider visual cues and neglect the auditory information that is ubiquitous in dynamic
scenes. Here, we investigate the applicability of audio cues in conjunction with visual ones in
predicting saliency maps using deep neural networks. To this end, the proposed model is intentionally
designed to be simple. Two baseline models are developed on the same architecture which consists
of an encoder-decoder. The encoder projects the input into a feature space followed by a decoder
that infers saliency. We conduct an extensive analysis on different modalities and various aspects
of multi-model dynamic saliency prediction. Our results suggest that (1) audio is a strong contributing
cue for saliency prediction, (2) salient visible sound-source is the natural cause of the superiority
of our Audio-Visual model, (3) richer feature representations for the input space leads to more
powerful predictions even in absence of more sophisticated saliency decoders, and (4) Audio-Visual
model improves over 53.54\% of the frames predicted by the best Visual model (our baseline). Our
endeavour demonstrates that audio is an important cue that boosts dynamic video saliency prediction
and helps models to approach human performance. The code is available at https://github.com/hrtavakoli/DAVE
