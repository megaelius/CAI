In recent years, sequence-to-sequence (seq2seq) models are used in a variety of tasks from machine
translation, headline generation, text summarization, speech to text, to image caption generation.
The underlying framework of all these models are usually a deep neural network which contains an
encoder and decoder. The encoder processes the input data and a decoder receives the output of the
encoder and generates the final output. Although simply using an encoder/decoder model would,
most of the time, produce better result than traditional methods on the above-mentioned tasks,
researchers proposed additional improvements over these sequence to sequence models, like using
an attention-based model over the input, pointer-generation models, and self-attention models.
However, all these seq2seq models suffer from two common problems: 1) exposure bias and 2) inconsistency
between train/test measurement. Recently a completely fresh point of view emerged in solving these
two problems in seq2seq models by using methods in Reinforcement Learning (RL). In these new researches,
we try to look at the seq2seq problems from the RL point of view and we try to come up with a formulation
that could combine the power of RL methods in decision-making and sequence to sequence models in
remembering long memories. In this paper, we will summarize some of the most recent frameworks that
combines concepts from RL world to the deep neural network area and explain how these two areas could
benefit from each other in solving complex seq2seq tasks. In the end, we will provide insights on
some of the problems of the current existing models and how we can improve them with better RL models.
We also provide the source code for implementing most of the models that will be discussed in this
paper on the complex task of abstractive text summarization. 