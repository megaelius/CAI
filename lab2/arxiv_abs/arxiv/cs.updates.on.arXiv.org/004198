Major advancements in building general-purpose and customized hardware have been one of the key
enablers of versatility and pervasiveness of machine learning models such as deep neural networks.
To sustain this ubiquitous deployment of machine learning models and cope with their computational
and storage complexity, several solutions such as low-precision representation of model parameters
using fixed-point representation and deploying approximate arithmetic operations have been
employed. Studying the potency of such solutions in different applications requires integrating
them into existing machine learning frameworks for high-level simulations as well as implementing
them in hardware to analyze their effects on power/energy dissipation, throughput, and chip area.
Lop is a library for design space exploration that bridges the gap between machine learning and efficient
hardware realization. It comprises a Python module, which can be integrated with some of the existing
machine learning frameworks and implements various customizable data representations including
fixed-point and floating-point as well as approximate arithmetic operations.Furthermore, it
includes a highly-parameterized Scala module, which allows synthesizing hardware based on the
said data representations and arithmetic operations. Lop allows researchers and designers to
quickly compare quality of their models using various data representations and arithmetic operations
in Python and contrast the hardware cost of viable representations by synthesizing them on their
target platforms (e.g., FPGA or ASIC). To the best of our knowledge, Lop is the first library that
allows both software simulation and hardware realization using customized data representations
and approximate computing techniques. 