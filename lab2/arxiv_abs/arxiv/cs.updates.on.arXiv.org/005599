Temporal models based on recurrent neural networks have proven to be quite powerful in a wide variety
of applications, including language modeling and speech processing. However, to train these models,
one relies on back-propagation through time, which entails unfolding the network over many time
steps, making the process of conducting credit assignment considerably more challenging. Furthermore,
the nature of back-propagation itself does not permit the use of non-differentiable activation
functions and is inherently sequential, making parallelization of the underlying training process
very difficult. In this work, we propose the Parallel Temporal Neural Coding Network, a biologically
inspired model trained by the local learning algorithm known as Local Representation Alignment,
that aims to resolve the difficulties and problems that plague recurrent networks trained by back-propagation
through time. Most notably, this architecture requires neither unrolling nor the derivatives
of its internal activation functions. We compare our model and learning procedure to other online
back-propagation-through-time alternatives (which also tend to be computationally expensive),
including real-time recurrent learning, echo state networks, and unbiased online recurrent optimization,
and show that it outperforms them on sequence modeling benchmarks such as Bouncing MNIST, a new benchmark
we call Bouncing NotMNIST, and Penn Treebank. Notably, our approach can, in some instances, even
outperform full back-propagation through time itself as well as variants such as sparse attentive
back-tracking. Furthermore, we present promising experimental results that demonstrate our
model's ability to conduct zero-shot adaptation. 