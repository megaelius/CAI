The Langevin Markov chain algorithms are widely deployed methods to sample from distributions
in challenging high-dimensional and non-convex statistics and machine learning applications.
Despite this, current bounds for the Langevin algorithms are slower than those of competing algorithms
in many important situations, for instance when sampling from weakly log-concave distributions,
or when sampling or optimizing non-convex log-densities. In this paper, we obtain improved bounds
in many of these situations, showing that the Metropolis-adjusted Langevin algorithm (MALA) is
faster than the best bounds for its competitor algorithms when the target distribution satisfies
weak third- and fourth- order regularity properties associated with the input data. In many settings,
our regularity conditions are weaker than the usual Euclidean operator norm regularity properties,
allowing us to show faster bounds for a much larger class of distributions than would be possible
with the usual Euclidean operator norm approach, including in statistics and machine learning
applications where the data satisfy a certain incoherence condition. In particular, we show that
using our regularity conditions one can obtain faster bounds for applications which include sampling
problems in Bayesian logistic regression with weakly convex priors, and the nonconvex optimization
problem of learning linear classifiers with zero-one loss functions. Our main technical contribution
in this paper is our analysis of the Metropolis acceptance probability of MALA in terms of its "energy-conservation
error," and our bound for this error in terms of third- and fourth- order regularity conditions.
Our combination of this higher-order analysis of the energy conservation error with the conductance
method is key to obtaining bounds which have a sub-linear dependence on the dimension $d$ in the non-strongly
logconcave setting. 