Abbreviations often have several distinct meanings, often making their use in text ambiguous.
Expanding them to their intended meaning in context is important for Machine Reading tasks such
as document search, recommendation and question answering. Existing approaches mostly rely on
manually labeled examples of abbreviations and their correct long-forms. Such data sets are costly
to create and result in trained models with limited applicability and flexibility. Importantly,
most current methods must be subjected to a full empirical evaluation in order to understand their
limitations, which is cumbersome in practice. In this paper, we present an entirely unsupervised
abbreviation disambiguation method (called UAD) that picks up abbreviation definitions from
unstructured text. Creating distinct tokens per meaning, we learn context representations as
word vectors. We demonstrate how to further boost abbreviation disambiguation performance by
obtaining better context representations using additional unstructured text. Our method is the
first abbreviation disambiguation approach with a transparent model that allows performance
analysis without requiring full-scale evaluation, making it highly relevant for real-world deployments.
In our thorough empirical evaluation, UAD achieves high performance on large real-world data sets
from different domains and outperforms both baseline and state-of-the-art methods. UAD scales
well and supports thousands of abbreviations with multiple different meanings within a single
model. In order to spur more research into abbreviation disambiguation, we publish a new data set,
that we also use in our experiments. 