Big data refers to large and complex data sets that, under existing approaches, exceed the capacity
and capability of current compute platforms, systems software, analytical tools and human understanding.
Numerous lessons on the scalability of big data can already be found in asymptotic analysis of algorithms
and from the high-performance computing (HPC) and applications communities. However, scale is
only one aspect of current big data trends; fundamentally, current and emerging problems in big
data are a result of unprecedented complexity--in the structure of the data and how to analyze it,
in dealing with unreliability and redundancy, in addressing the human factors of comprehending
complex data sets, in formulating meaningful analyses, and in managing the dense, power-hungry
data centers that house big data. The computer science solution to complexity is finding the right
abstractions, those that hide as much triviality as possible while revealing the essence of the
problem that is being addressed. The "big data challenge" has disrupted computer science by stressing
to the very limits the familiar abstractions which define the relevant subfields in data analysis,
data management and the underlying parallel systems. As a result, not enough of these challenges
are revealed by isolating abstractions in a traditional software stack or standard algorithmic
and analytical techniques, and attempts to address complexity either oversimplify or require
low-level management of details. The authors believe that the abstractions for big data need to
be rethought, and this reorganization needs to evolve and be sustained through continued cross-disciplinary
collaboration. 