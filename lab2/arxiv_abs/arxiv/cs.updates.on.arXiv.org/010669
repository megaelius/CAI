Pedestrian detection has achieved significant progress with the availability of existing benchmark
datasets. However, there is a gap in the diversity and density between real world requirements and
current pedestrian detection benchmarks: 1) most of existing datasets are taken from a vehicle
driving through the regular traffic scenario, usually leading to insufficient diversity; 2) crowd
scenarios with highly occluded pedestrians are still under represented, resulting in low density.
To narrow this gap and facilitate future pedestrian detection research, we introduce a large and
diverse dataset named WiderPerson for dense pedestrian detection in the wild. This dataset involves
five types of annotations in a wide range of scenarios, no longer limited to the traffic scenario.
There are a total of $13,382$ images with $399,786$ annotations, i.e., $29.87$ annotations per
image, which means this dataset contains dense pedestrians with various kinds of occlusions. Hence,
pedestrians in the proposed dataset are extremely challenging due to large variations in the scenario
and occlusion, which is suitable to evaluate pedestrian detectors in the wild. We introduce an improved
Faster R-CNN and the vanilla RetinaNet to serve as baselines for the new pedestrian detection benchmark.
Several experiments are conducted on previous datasets including Caltech-USA and CityPersons
to analyze the generalization capabilities of the proposed dataset and we achieve state-of-the-art
performances on these previous datasets without bells and whistles. Finally, we analyze common
failure cases and find the classification ability of pedestrian detector needs to be improved to
reduce false alarm and miss detection rates. The proposed dataset is available at this http URL 