In this paper, we present a theoretical effort to connect the theory of program size to psychology
by implementing a concrete language of thought with Turing-computable Kolmogorov complexity
(LT^2C^2) satisfying the following requirements: 1) to be simple enough so that the complexity
of any given finite binary sequence can be computed, 2) to be based on tangible operations of human
reasoning (printing, repeating,...), 3) to be sufficiently powerful to generate all possible
sequences but not too powerful as to identify regularities which would be invisible to humans. We
first formalize LT^2C^2, giving its syntax and semantics and defining an adequate notion of program
size. Our setting leads to a Kolmogorov complexity function relative to LT^2C^2 which is computable
in polynomial time, and it also induces a prediction algorithm in the spirit of Solomonoff's inductive
inference theory. We then prove the efficacy of this language by investigating regularities in
strings produced by participants attempting to generate random strings. Participants had a profound
understanding of randomness and hence avoided typical misconceptions such as exaggerating the
number of alternations. We reasoned that remaining regularities would express the algorithmic
nature of human thoughts, revealed in the form of specific patterns. Kolmogorov complexity relative
to LT^2C^2 passed three expected tests examined here: 1) human sequences were less complex than
control PRNG sequences, 2) human sequences were not stationary, showing decreasing values of complexity
resulting from fatigue, 3) each individual showed traces of algorithmic stability since fitting
of partial sequences was more effective to predict subsequent sequences than average fits. This
work extends on previous efforts to combine notions of Kolmogorov complexity theory and algorithmic
information theory to psychology, by explicitly ... 