The development of renewable energy generation empowers microgrids to generate electricity to
supply itself and to trade the surplus on energy markets. To minimize the overall cost, a microgrid
must determine how to schedule its energy resources and electrical loads and how to trade with others.
The control decisions are influenced by various factors, such as energy storage, renewable energy
yield, electrical load, and competition from other microgrids. Making the optimal control decision
is challenging, due to the complexity of the interconnected microgrids, the uncertainty of renewable
energy generation and consumption, and the interplay among microgrids. The previous works mainly
adopted the modeling-based approaches for deriving the control decision, yet they relied on the
precise information of future system dynamics, which can be hard to obtain in a complex environment.
This work provides a new perspective of obtaining the optimal control policy for distributed energy
trading and scheduling by directly interacting with the environment, and proposes a multiagent
deep reinforcement learning approach for learning the optimal control policy. Each microgrid
is modeled as an agent, and different agents learn collaboratively for maximizing their rewards.
The agent of each microgrid can make the local scheduling decision without knowing others' information,
which can well maintain the autonomy of each microgrid. We evaluate the performances of our proposed
method using real-world datasets. The experimental results show that our method can significantly
reduce the cost of the microgrids compared with the baseline methods. 