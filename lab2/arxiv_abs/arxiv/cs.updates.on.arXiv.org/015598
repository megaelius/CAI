Machine learning tools have illustrated their potential in many significant sectors such as healthcare
and finance, to aide in deriving useful inferences. The sensitive and confidential nature of the
data, in such sectors, raise natural concerns for the privacy of data. This motivated the area of
Privacy-preserving Machine Learning (PPML) where privacy of the data is guaranteed. Typically,
ML techniques require large computing power, which leads clients with limited infrastructure
to rely on the method of Secure Outsourced Computation (SOC). In SOC setting, the computation is
outsourced to a set of specialized and powerful cloud servers and the service is availed on a pay-per-use
basis. In this work, we explore PPML techniques in the SOC setting for widely used ML algorithms--
Linear Regression, Logistic Regression, and Neural Networks. We propose BLAZE, a blazing fast
PPML framework in the three server setting tolerating one malicious corruption over a ring (\Z{\ell}).
BLAZE achieves the stronger security guarantee of fairness (all honest servers get the output whenever
the corrupt server obtains the same). Leveraging an input-independent preprocessing phase, BLAZE
has a fast input-dependent online phase relying on efficient PPML primitives such as: (i) A dot product
protocol for which the communication in the online phase is independent of the vector size, the first
of its kind in the three server setting; (ii) A method for truncation that shuns evaluating expensive
circuit for Ripple Carry Adders (RCA) and achieves a constant round complexity. This improves over
the truncation method of ABY3 (Mohassel et al., CCS 2018) that uses RCA and consumes a round complexity
that is of the order of the depth of RCA. An extensive benchmarking of BLAZE for the aforementioned
ML algorithms over a 64-bit ring in both WAN and LAN settings shows massive improvements over ABY3.
