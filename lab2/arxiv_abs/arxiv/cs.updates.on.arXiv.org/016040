Deep Neural Networks (DNNs) have revolutionized many aspects of our lives. The use of DNNs is becoming
ubiquitous including in softwares for image recognition, speech recognition, speech synthesis,
language translation, to name a few. he training of DNN architectures however is computationally
expensive. Once the model is created, its use in the intended application - the inference task, is
computationally heavy too and the inference needs to be fast for real time use. For obtaining high
performance today, the code of Deep Learning (DL) primitives optimized for specific architectures
by expert programmers exposed via libraries is the norm. However, given the constant emergence
of new DNN architectures, creating hand optimized code is expensive, slow and is not scalable. To
address this performance-productivity challenge, in this paper we present compiler algorithms
to automatically generate high performance implementations of DL primitives that closely match
the performance of hand optimized libraries. We develop novel data reuse analysis algorithms using
the polyhedral model to derive efficient execution schedules automatically. In addition, because
most DL primitives use some variant of matrix multiplication at their core, we develop a flexible
framework where it is possible to plug in library implementations of the same in lieu of a subset of
the loops. We show that such a hybrid compiler plus a minimal library-use approach results in state-of-the-art
performance. We develop compiler algorithms to also perform operator fusions that reduce data
movement through the memory hierarchy of the computer system. 