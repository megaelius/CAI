A major challenge in obtaining large-scale evaluations, e.g., product or service reviews on online
platforms, labeling images, grading in online courses, etc., is that of eliciting honest responses
from agents in the absence of verifiability. We propose a new reward mechanism with strong incentive
properties applicable in a wide variety of such settings. This mechanism has a simple and intuitive
output agreement structure: an agent gets a reward only if her response for an evaluation matches
that of her peer. But instead of the reward being the same across different answers, it is inversely
proportional to a popularity index of each answer. This index is a second order population statistic
that captures how frequently two agents performing the same evaluation agree on the particular
answer. Rare agreements thus earn a higher reward than agreements that are relatively more common.
In the regime where there are a large number of evaluation tasks, we show that truthful behavior is
a strict Bayes-Nash equilibrium of the game induced by the mechanism. Further, we show that the truthful
equilibrium is approximately optimal in terms of expected payoffs to the agents across all symmetric
equilibria, where the approximation error vanishes in the number of evaluation tasks. Moreover,
under a mild condition on strategy space, we show that any symmetric equilibrium that gives a higher
expected payoff than the truthful equilibrium must be close to being fully informative if the number
of evaluations is large. These last two results are driven by a new notion of an agreement measure
that is shown to be monotonic in information loss. This notion and its properties are of independent
interest. 