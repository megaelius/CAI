The goal of few-shot image recognition (FSIR) is to identify novel categories with a small number
of annotated samples by exploiting transferable knowledge from training data (base categories).
Most current studies assume that the transferable knowledge can be well used to identify novel categories.
However, such transferable capability may be impacted by the dataset bias, and this problem has
rarely been investigated before. Besides, most of few-shot learning methods are biased to different
datasets, which is also an important issue that needs to be investigated in depth. In this paper,
we first investigate the impact of transferable capabilities learned from base categories. Specifically,
we introduce relevance to describe relations of base and novel categories, along with instance
diversity and category diversity to depict distributions of base categories. The FSIR model learns
better transferable knowledge from relative training data. In the relative data, diverse instances
or categories can further enrich the learned knowledge. We conduct experiments on different sub-datasets
of ImagNet, and experimental results demonstrate category relevance and category/instance diversity
can depict transferable bias from distributions of base categories. Second, we investigate performance
differences on different datasets from dataset structures and few-shot learning methods. Specifically,
we introduce image complexity, inner-concept visual consistency, and inter-concept visual similarity
to quantify characteristics of dataset structures. We use these quantitative characteristics
and four few-shot learning methods to analyze performance differences on five different datasets.
Based on experimental analysis, some insightful observations are obtained from the perspective
of both dataset structures and few-shot learning methods. These observations are useful to guide
future FSIR research. 