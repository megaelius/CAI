With the pervasiveness of voice control on smart devices, speaker verification is widely used as
the preferred identity authentication mechanism due to its convenience. However, the task of "in-the-wild"
speaker verification is challenging, considering the speech samples may contain lots of identity-unrelated
information, e.g., background noise, reverberation, emotion, etc. Previous works focus on optimizing
the model to improve verification accuracy, without considering eliminating the impact from the
identity-unrelated information. To solve this problem, we propose SEF-ALDR, a novel Speaker Embedding
Framework via Adversarial Learning based Disentangled Representation, to further improve the
performance of existing models on speaker verification. The key idea is to retrieve as much speaker
identity information as possible from the original speech, i.e., minimizing the impact of identity-unrelated
information on the speaker verification task. We utilize the adversarial learning method to train
an eliminating encoder that produces an embedding without speaker identity representation. Hence,
the speaker identity information can be well preserved in the complementary features that are not
used by the eliminating encoder in the original speech. Experimental results demonstrate that
the proposed framework can significantly improve the performance of the speaker verification
by 20.3% and 28.7% on average over 13 tested baselines on dataset Voxceleb1 and 9 tested baselines
on dataset Voxceleb2 respectively, without adjusting the structure or hyper-parameters of them.
Furthermore, porting an existing model into the proposed framework is straightforward and cost-efficient,
with very little effort from the model owners due to the modular design of the framework. 