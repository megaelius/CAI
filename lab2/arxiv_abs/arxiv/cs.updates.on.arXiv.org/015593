We study the performance of neural network models on random geometric transformations and adversarial
perturbations. Invariance means that the model's prediction remains unchanged when a geometric
transformation is applied to an input. Adversarial robustness means that the model's prediction
remains unchanged after small adversarial perturbations of an input. In this paper, we show a quantitative
trade-off between rotation invariance and robustness. We empirically study the following two
cases: (a) change in adversarial robustness as we improve only the invariance of equivariant models
via training augmentation, (b) change in invariance as we improve only the adversarial robustness
using adversarial training. We observe that the rotation invariance of equivariant models (StdCNNs
and GCNNs) improves by training augmentation with progressively larger random rotations but while
doing so, their adversarial robustness drops progressively, and very significantly on MNIST.
We take adversarially trained LeNet and ResNet models which have good $L_\infty$ adversarial robustness
on MNIST and CIFAR-10, respectively, and observe that adversarial training with progressively
larger perturbations results in a progressive drop in their rotation invariance profiles. Similar
to the trade-off between accuracy and robustness known in previous work, we give a theoretical justification
for the invariance vs. robustness trade-off observed in our experiments. We also give additional
empirical evidence for claim (a). We observe that the average distance of the test points to the decision
boundary reduces when models are trained with larger rotations. 