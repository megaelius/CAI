Although deep RL models have shown a great potential for solving various types of tasks with minimal
supervision, several key challenges remain in terms of learning from limited experience, adapting
to environmental changes, and generalizing learning from a single task. Recent evidence in decision
neuroscience has shown that the human brain has an innate capacity to resolve these issues, leading
to optimism regarding the development of neuroscience-inspired solutions toward sample-efficient,
and generalizable RL algorithms. We show that the computational model combining model-based and
model-free control, which we term the prefrontal RL, reliably encodes the information of high-level
policy that humans learned, and this model can generalize the learned policy to a wide range of tasks.
First, we trained the prefrontal RL, and deep RL algorithms on 82 subjects' data, collected while
human participants were performing two-stage Markov decision tasks, in which we manipulated the
goal, state-transition uncertainty and state-space complexity. In the reliability test, which
includes the latent behavior profile and the parameter recoverability test, we showed that the
prefrontal RL reliably learned the latent policies of the humans, while all the other models failed.
Second, to test the ability to generalize what these models learned from the original task, we situated
them in the context of environmental volatility. Specifically, we ran large-scale simulations
with 10 Markov decision tasks, in which latent context variables change over time. Our information-theoretic
analysis showed that the prefrontal RL showed the highest level of adaptability and episodic encoding
efficacy. This is the first attempt to formally test the possibility that computational models
mimicking the way the brain solves general problems can lead to practical solutions to key challenges
in machine learning. 