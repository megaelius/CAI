Machine learning has proven to be an extremely useful tool for solving complex problems in many application
domains. This prevalence makes it an attractive target for malicious actors. Adversarial machine
learning is a well-studied field of research in which an adversary seeks to cause predicable errors
in a machine learning algorithm through careful manipulation of the input. In response, numerous
techniques have been proposed to harden machine learning algorithms and mitigate the effect of
adversarial attacks. Of these techniques, adversarial training, which augments the training
data with adversarial inputs, has proven to be an effective defensive technique. However, adversarial
training is computationally expensive and the improvements in adversarial performance are limited
to a single model. In this paper, we propose Adversarially-Trained Autoencoder Augmentation,
the first transferable adversarial defense that is robust to certain adaptive adversaries. We
disentangle adversarial robustness from the classification pipeline by adversarially training
an autoencoder with respect to the classification loss. We show that our approach achieves comparable
results to state-of-the-art adversarially trained models on the MNIST, Fashion-MNIST, and CIFAR-10
datasets. Furthermore, we can transfer our approach to other vulnerable models and improve their
adversarial performance without additional training. Finally, we combine our defense with ensemble
methods and parallelize adversarial training across multiple vulnerable pre-trained models.
In a single adversarial training session, the autoencoder can achieve adversarial performance
on the vulnerable models that is comparable or better than standard adversarial training. 