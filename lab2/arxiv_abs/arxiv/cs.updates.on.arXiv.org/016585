The problem of catastrophic forgetting can be traced back to the 1980s, but it has not been completely
solved. Since human brains are good at continual lifelong learning, brain-inspired methods may
provide solutions to this problem. The end result of learning different objects in different categories
is the formation of concepts in the brain. Experiments showed that concepts are likely encoded by
concept cells in the medial temporal lobe (MTL) of the human brain. Furthermore, concept cells encode
concepts sparsely and are responsive to multi-modal stimuli. However, it is unknown how concepts
are formed in the MTL. Here we assume that the integration of audio and visual perceptual information
in the MTL during learning is a crucial step to form concepts and make continual learning possible,
and we propose a biological plausible audio-visual integration model (AVIM), which is a spiking
neural network with multi-compartmental neuron model and a calcium based synaptic tagging and
capture plasticity model, as a possible mechanism of concept formation. We then build such a model
and run on different datasets to test its ability of continual learning. Our simulation results
show that the AVIM not only achieves state-of-the-art performance compared with other advanced
methods but also the output of AVIM for each concept has stable representations during the continual
learning process. These results support our assumption that concept formation is essential for
continuous lifelong learning, and suggest the AVIM we propose here is a possible mechanism of concept
formation, and hence is a brain-like solution to the problem of catastrophic forgetting. 