Organizations cannot address demographic disparities that they cannot see. Recent research on
machine learning and fairness has emphasized that awareness of sensitive attributes, such as race
and sex, is critical to the development of interventions. However, on the ground, the existence
of these data cannot be taken for granted. This paper uses the domains of employment, credit, and
healthcare in the United States to surface conditions that have shaped the availability of sensitive
attribute data. For each domain, we describe how and when private companies collect or infer sensitive
attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies
are required by law to collect sensitive attribute data, while others are prohibited from doing
so. Still others, in the absence of legal mandates, have determined that collection and imputation
of these data are appropriate to address disparities. This story has important implications for
fairness research and its future applications. If companies that mediate access to life opportunities
are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to
detect and mitigate bias in machine learning models might never be implemented outside the lab.
We conclude that today's legal requirements and corporate practices, while highly inconsistent
across domains, offer lessons for how to approach the collection and inference of sensitive data
in appropriate circumstances. We urge stakeholders, including machine learning practitioners,
to actively help chart a path forward that takes both policy goals and technical needs into account.
