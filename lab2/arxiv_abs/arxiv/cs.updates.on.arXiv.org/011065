An effective person re-identification (re-ID) model should learn feature representations that
are both discriminative, for distinguishing similar-looking people, and generalisable, for
deployment across datasets without any adaptation. In this paper, we develop novel CNN architectures
to address both challenges. First, we present a re-ID CNN termed omni-scale network (OSNet) to learn
features that not only capture different spatial scales but also encapsulate a synergistic combination
of multiple scales, namely omni-scale features. The basic building block consists of multiple
convolutional streams, each detecting features at a certain scale. For omni-scale feature learning,
a unified aggregation gate is introduced to dynamically fuse multi-scale features with channel-wise
weights. OSNet is lightweight as its building blocks comprise factorised convolutions. Second,
to improve generalisable feature learning, we introduce instance normalisation (IN) layers into
OSNet to cope with cross-dataset discrepancies. Further, to determine the optimal placements
of these IN layers in the architecture, we formulate an efficient differentiable architecture
search algorithm. Extensive experiments show that, in the conventional same-dataset setting,
OSNet achieves state-of-the-art performance, despite being much smaller than existing re-ID
models. In the more challenging yet practical cross-dataset setting, OSNet beats most recent unsupervised
domain adaptation methods without requiring any target data for model adaptation. Our code and
models are released at \texttt{https://github.com/KaiyangZhou/deep-person-reid}. 