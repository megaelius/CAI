It is difficult to trust decisions made by Black-box Artificial Intelligence (AI) methods since
their inner working and decision logic is hidden from the user. Explainable Artificial Intelligence
(XAI) refers to systems that try to explain how a black-box AI model produces its outcomes. Post-hoc
XAI methods approximate the behavior of a black-box by extracting relationships between feature
values and the predictions. Some post-hoc explanators randomly perturb data records and build
local linear models to explain individual predictions. Other type of explanators use frequent
itemsets to extract feature values that frequently appear in samples belonging to a particular
class. However, the above methods have some limitations. Random perturbations do not take into
account the distribution of feature values in different subspaces, leading to misleading approximations.
Frequent itemsets only pay attention to frequently appearing feature values and miss many important
correlations between features and class labels that could accurately represent decision boundaries
of the model. In this paper, we address the above challenges by proposing an explanation method named
Confident Itemsets Explanation (CIE). We introduce confident itemsets, a set of feature values
that are highly correlated to a specific class label. CIE utilizes confident itemsets to discretize
the whole decision space of a model to smaller subspaces. Extracting important correlations between
the features and the outcomes of the black-box in different subspaces, CIE produces instance-wise
and class-wise explanations that accurately approximate the behavior of the target black-box
classifier. 