Dynamic texture (DT) exhibits statistical stationarity in the spatial domain and stochastic repetitiveness
in the temporal dimension, indicating that different frames of DT possess high similarity correlation.
However, there are no DT synthesis methods to consider the similarity prior for representing DT
instead, which can explicitly capture the homogeneous and heterogeneous correlation between
different frames of DT. In this paper, we propose a novel DT synthesis method (named Similarity-DT),
which embeds the similarity prior into the representation of DT. Specifically, we first raise two
hypotheses: the content of texture video frames varies over time-to-time, while the more closed
frames should be more similar; the transition between frame-to-frame could be modeled as a linear
or nonlinear function to capture the similarity correlation. Then, our proposed Similarity-DT
integrates kernel learning and extreme learning machine (ELM) into a powerful unified synthesis
model to learn kernel similarity embedding to represent the spatial-temporal transition among
frame-to-frame of DTs. Extensive experiments on DT videos collected from internet and two benchmark
datasets, i.e., Gatech Graphcut Textures and Dyntex, demonstrate that the learned kernel similarity
embedding effectively exhibits the discriminative representation for DTs. Hence our method is
capable of preserving long-term temporal continuity of the synthesized DT sequences with excellent
sustainability and generalization. We also show that our method effectively generates realistic
DT videos with fast speed and low computation, compared with the state-of-the-art approaches.
