The attention-based encoder-decoder framework has recently achieved impressive results for
scene text recognition, and many variants have emerged with improvements in recognition quality.
However, it performs poorly on contextless texts (e.g., random character sequences) which is unacceptable
in most of real application scenarios. In this paper, we first deeply investigate the decoding process
of the decoder. We empirically find that a representative character-level sequence decoder utilizes
not only context information but also positional information. Contextual information, which
the existing approaches heavily rely on, causes the problem of attention drift. To suppress such
side-effect, we propose a novel position enhancement branch, and dynamically fuse its outputs
with those of the decoder attention module for scene text recognition. Specifically, it contains
a position aware module to enable the encoder to output feature vectors encoding their own spatial
positions, and an attention module to estimate glimpses using the positional clue (i.e., the current
decoding time step) only. The dynamic fusion is conducted for more robust feature via an element-wise
gate mechanism. Theoretically, our proposed method, dubbed \emph{RobustScanner}, decodes individual
characters with dynamic ratio between context and positional clues, and utilizes more positional
ones when the decoding sequences with scarce context, and thus is robust and practical. Empirically,
it has achieved new state-of-the-art results on popular regular and irregular text recognition
benchmarks while without much performance drop on contextless benchmarks, validating its robustness
in both contextual and contextless application scenarios. 