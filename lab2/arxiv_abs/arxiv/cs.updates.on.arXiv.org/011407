Spiking neural network is an important family of models to emulate the brain, which has been widely
adopted by neuromorphic platforms. In the meantime, it is well-known that the huge memory and compute
costs of neural networks greatly hinder the execution with high efficiency, especially on edge
devices. To this end, model compression is proposed as a promising technique to improve the running
efficiency via parameter and operation reduction. Therefore, it is interesting to investigate
how much an SNN model can be compressed without compromising much functionality. However, this
is quite challenging because SNNs usually behave distinctly from deep learning models. Specifically,
i) the accuracy of spike-coded SNNs is usually sensitive to any network change; ii) the computation
of SNNs is event-driven rather than static. Here we present a comprehensive SNN compression through
three steps. First, we formulate the connection pruning and the weight quantization as a supervised
learning-based constrained optimization problem. Second, we combine the emerging spatio-temporal
backpropagation and the powerful alternating direction method of multipliers to solve the problem
with minimum accuracy loss. Third, we further propose an activity regularization to reduce the
spike events for fewer active operations. We define several quantitative metrics to evaluation
the compression performance for SNNs and validate our methodology in pattern recognition tasks
over MNIST, N-MNIST, and CIFAR10 datasets. Extensive comparisons between different compression
strategies, the corresponding result analysis, and some interesting insights are provided. To
our best knowledge, this is the first work that studies SNN compression in a comprehensive manner
by exploiting all possible compression ways and achieves better results. Our work offers a promising
solution to pursue ultra-efficient neuromorphic systems. 