Deep learning methods are successfully used in applications pertaining to ubiquitous computing,
health, and well-being. Specifically, the area of human activity recognition (HAR) is primarily
transformed by the convolutional and recurrent neural networks, thanks to their ability to learn
semantic representations from raw input. However, to extract generalizable features, massive
amounts of well-curated data are required, which is a notoriously challenging task; hindered by
privacy issues, and annotation costs. Therefore, unsupervised representation learning is of
prime importance to leverage the vast amount of unlabeled data produced by smart devices. In this
work, we propose a novel self-supervised technique for feature learning from sensory data that
does not require access to any form of semantic labels. We learn a multi-task temporal convolutional
network to recognize transformations applied on an input signal. By exploiting these transformations,
we demonstrate that simple auxiliary tasks of the binary classification result in a strong supervisory
signal for extracting useful features for the downstream task. We extensively evaluate the proposed
approach on several publicly available datasets for smartphone-based HAR in unsupervised, semi-supervised,
and transfer learning settings. Our method achieves performance levels superior to or comparable
with fully-supervised networks, and it performs significantly better than autoencoders. Notably,
for the semi-supervised case, the self-supervised features substantially boost the detection
rate by attaining a kappa score between 0.7-0.8 with only 10 labeled examples per class. We get similar
impressive performance even if the features are transferred from a different data source. While
this paper focuses on HAR as the application domain, the proposed technique is general and could
be applied to a wide variety of problems in other areas. 