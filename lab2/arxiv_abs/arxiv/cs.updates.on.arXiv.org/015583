Acceptance testing is a validation activity performed to ensure the conformance of software systems
with respect to their functional requirements. In safety critical systems, it plays a crucial role
since it is enforced by software standards. Test engineers need to identify all the representative
test execution scenarios from requirements, determine the runtime conditions that trigger these
scenarios, and finally provide the input data that satisfy these conditions. Given that requirements
specifications are typically large and often provided in natural language, the generation of acceptance
test cases tends to be expensive and error-prone. In this paper, we present UMTG, an approach that
supports the generation of executable, system-level, acceptance test cases from requirements
specifications in natural language, with the goal of reducing the manual effort required to generate
test cases and ensuring requirements coverage. More specifically, UMTG automates the generation
of acceptance test cases based on use case specifications and a domain model for the system under
test, which are commonly produced in many development environments. Unlike existing approaches,
it does not impose strong restrictions on the expressiveness of use case specifications. We rely
on recent advances in natural language processing to automatically identify test scenarios and
to generate formal constraints that capture conditions triggering the execution of the scenarios,
thus enabling the generation of test data. In two industrial case studies, UMTG automatically and
correctly translated 95% of the use case specification steps into formal constraints required
for test data generation; furthermore, it generated test cases that exercise not only all the test
scenarios manually implemented by experts, but also some critical scenarios not previously considered.
