Historically, the pursuit of efficient inference has been one of the driving forces behind research
into new deep learning architectures and building blocks. Some recent examples include: the squeeze-and-excitation
module, depthwise separable convolutions in Xception, and the inverted bottleneck in MobileNet
v2. Notably, in all of these cases, the resulting building blocks enabled not only higher efficiency,
but also higher accuracy, and found wide adoption in the field. In this work, we further expand the
arsenal of efficient building blocks for neural network architectures; but instead of combining
standard primitives (such as convolution), we advocate for the replacement of these dense primitives
with their sparse counterparts. While the idea of using sparsity to decrease the parameter count
is not new, the conventional wisdom is that this reduction in theoretical FLOPs does not translate
into real-world efficiency gains. We aim to correct this misconception by introducing a family
of efficient sparse kernels for ARM and WebAssembly, which we open-source for the benefit of the
community as part of the XNNPACK library. Equipped with our efficient implementation of sparse
primitives, we show that sparse versions of MobileNet v1, MobileNet v2 and EfficientNet architectures
substantially outperform strong dense baselines on the efficiency-accuracy curve. On Snapdragon
835 our sparse networks outperform their dense equivalents by $1.3-2.4\times$ -- equivalent to
approximately one entire generation of MobileNet-family improvement. We hope that our findings
will facilitate wider adoption of sparsity as a tool for creating efficient and accurate deep learning
architectures. 