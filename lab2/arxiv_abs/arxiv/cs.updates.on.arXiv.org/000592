Software defect prediction plays a crucial role in estimating the most defect-prone components
of software, and a large number of studies have pursued improving prediction accuracy within a project
or across projects. However, the rules for making an appropriate decision between within- and cross-project
defect prediction when available historical data are insufficient remain unclear. The objective
of this work is to validate the feasibility of the predictor built with a simplified metric set for
software defect prediction in different scenarios, and to investigate practical guidelines for
the choice of training data, classifier and metric subset of a given project. First, based on six
typical classifiers, we constructed three types of predictors using the size of software metric
set in three scenarios. Then, we validated the acceptable performance of the predictor based on
Top-k metrics in terms of statistical methods. Finally, we attempted to minimize the Top-k metric
subset by removing redundant metrics, and we tested the stability of such a minimum metric subset
with one-way ANOVA tests. The experimental results indicate that (1) the choice of training data
should depend on the specific requirement of prediction accuracy; (2) the predictor built with
a simplified metric set works well and is very useful in case limited resources are supplied; (3)
simple classifiers (e.g., Naive Bayes) also tend to perform well when using a simplified metric
set for defect prediction; (4) in several cases, the minimum metric subset can be identified to facilitate
the procedure of general defect prediction with acceptable loss of prediction precision in practice.
The guideline for choosing a suitable simplified metric set in different scenarios is presented
in Table 10. 