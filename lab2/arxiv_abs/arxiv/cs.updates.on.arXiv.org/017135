Edge computing offers an additional layer of compute infrastructure closer to the data source before
raw data from privacy-sensitive and performance-critical applications is transferred to a cloud
data center. Deep Neural Networks (DNNs) are one class of applications that are reported to benefit
from collaboratively computing between the edge and the cloud. A DNN is partitioned such that specific
layers of the DNN are deployed onto the edge and the cloud to meet performance and privacy objectives.
However, there is limited understanding of: (a) whether and how evolving operational conditions
(increased CPU and memory utilization at the edge or reduced data transfer rates between the edge
and the cloud) affect the performance of already deployed DNNs, and (b) whether a new partition configuration
is required to maximize performance. A DNN that adapts to changing operational conditions is referred
to as an `adaptive DNN'. This paper investigates whether there is a case for adaptive DNNs in edge
computing by considering three questions: (i) Are DNNs sensitive to operational conditions? (ii)
How sensitive are DNNs to operational conditions? (iii) Do individual or a combination of operational
conditions equally affect DNNs? The exploration is carried out in the context of 8 pre-trained DNN
models and the results presented are from analyzing nearly 2 million data points. The results highlight
that network conditions affects DNN performance more than CPU or memory related operational conditions.
Repartitioning is noted to provide a performance gain in a number of cases, thus demonstrating the
need for adaptive DNNs. 