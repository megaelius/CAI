Automatic understanding of human affect using visual signals is of great importance in everyday
human-machine interactions. Appraising human emotional states, behaviors and reactions displayed
in real-world settings, can be accomplished using latent continuous dimensions (e.g., the circumplex
model of affect). Valence (i.e., how positive or negative is an emotion) & arousal (i.e., power of
the activation of the emotion) constitute popular and effective affect representations. Nevertheless,
the majority of collected datasets this far, although containing naturalistic emotional states,
have been captured in highly controlled recording conditions. In this paper, we introduce the Aff-Wild
benchmark for training and evaluating affect recognition algorithms. We also report on the results
of the First Affect-in-the-wild Challenge that was organized in conjunction with CVPR 2017 on the
Aff-Wild database and was the first ever challenge on the estimation of valence and arousal in-the-wild.
Furthermore, we design and extensively train an end-to-end deep neural architecture which performs
prediction of continuous emotion dimensions based on visual cues. The proposed deep learning architecture,
AffWildNet, includes convolutional & recurrent neural network layers, exploiting the invariant
properties of convolutional features, while also modeling temporal dynamics that arise in human
behavior via the recurrent layers. The AffWildNet produced state-of-the-art results on the Aff-Wild
Challenge. We then exploit the AffWild database for learning features, which can be used as priors
for achieving best performances both for dimensional, as well as categorical emotion recognition,
using the RECOLA, AFEW-VA and EmotiW datasets, compared to all other methods designed for the same
goal. The database and emotion recognition models are available at this http URL 