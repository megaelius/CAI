Binary logistic regression with a sparsity constraint on the solution plays a vital role in many
high dimensional machine learning applications. In some cases, the features can be grouped together,
so that entire subsets of features can be selected or zeroed out. In many applications, however,
this can be very restrictive. In this paper, we are interested in a less restrictive form of structured
sparse feature selection: we assume that while features can be grouped according to some notion
of similarity, not all features in a group need be selected for the task at hand. This is sometimes
referred to as a "sparse group" lasso procedure, and it allows for more flexibility than traditional
group lasso methods. Our framework generalizes conventional sparse group lasso further by allowing
for overlapping groups, an additional flexibility that presents further challenges. The main
contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex
optimization program that automatically selects similar features for learning in high dimensions.
We establish consistency results for the SOSlasso for classification problems using the logistic
regression setting, which specializes to results for the lasso and the group lasso, some known and
some new. In particular, SOSlasso is motivated by multi-subject fMRI studies in which functional
activity is classified using brain voxels as features, source localization problems in Magnetoencephalography
(MEG), and analyzing gene activation patterns in microarray data analysis. Experiments with real
and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso.
