Deep learning has become the standard methodology to approach computer vision tasks when large
amounts of labeled data are available. One area where traditional deep learning approaches fail
to perform is one-shot learning tasks where a model must correctly classify a new category after
seeing only one example. One such domain is animal re-identification, an application of computer
vision which can be used globally as a method to automate species population estimates from camera
trap images. Our work demonstrates both the application of similarity comparison networks to animal
re-identification, as well as the capabilities of deep convolutional neural networks to generalize
across domains. Few studies have considered animal re-identification methods across species.
Here, we compare two similarity comparison methodologies: Siamese and Triplet-Loss, based on
the AlexNet, VGG-19, DenseNet201, MobileNetV2, and InceptionV3 architectures considering mean
average precision (mAP)@1 and mAP@5. We consider five data sets corresponding to five different
species: humans, chimpanzees, humpback whales, fruit flies, and Siberian tigers, each with their
own unique set of challenges. We demonstrate that Triplet Loss outperformed its Siamese counterpart
for all species. Without any species-specific modifications, our results demonstrate that similarity
comparison networks can reach a performance level beyond that of humans for the task of animal re-identification.
The ability for researchers to re-identify an animal individual upon re-encounter is fundamental
for addressing a broad range of questions in the study of population dynamics and community/behavioural
ecology. Our expectation is that similarity comparison networks are the beginning of a major trend
that could stand to revolutionize animal re-identification from camera trap data. 