Rapid technological advancements in AI as well as the growing deployment of intelligent technologies
in new application domains are currently driving the competition between businesses, nations
and regions. This race for technological supremacy creates a complex ecology of choices that may
lead to negative consequences, in particular, when ethical and safety procedures are underestimated
or even ignored. As a consequence, different actors are urging to consider both the normative and
social impact of these technological advancements. As there is no easy access to data describing
this AI race, theoretical models are necessary to understand its dynamics, allowing for the identification
of when, how and which procedures need to be put in place to favour outcomes beneficial for all. We
show that, next to the risks of setbacks and being reprimanded for unsafe behaviour, the time-scale
in which AI supremacy can be achieved plays a crucial role. When this supremacy can be achieved in
a short term, those who completely ignore the safety precautions are bound to win the race but at a
cost to society, apparently requiring regulatory actions. Our analysis reveals that blindly imposing
regulations may not have anticipated effect as only for specific conditions a dilemma arises between
what individually preferred and globally beneficial. Similar observations can be made for the
long-term development case. Yet different from the short term situation, certain conditions require
the promotion of risk-taking as opposed to compliance to safety regulations in order to improve
social welfare. These results remain robust when two or several actors are involved in the race and
when collective rather than individual setbacks are produced by risk-taking behaviour. When defining
codes of conduct and regulatory policies for AI, a clear understanding about the time-scale of the
race is required. 