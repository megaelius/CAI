Even though convolutional neural networks (CNNs) are driving progress in medical image segmentation,
standard models still have some drawbacks. First, the use of multi-scale approaches, i.e., encoder-decoder
architectures, leads to a redundant use of information, where similar low-level features are extracted
multiple times at multiple scales. Second, long-range feature dependencies are not efficiently
modeled, resulting in non-optimal discriminative feature representations associated with each
semantic class. In this paper we attempt to overcome these limitations with the proposed architecture,
by capturing richer contextual dependencies based on the use of guided self-attention mechanisms.
This approach is able to integrate local features with their corresponding global dependencies,
as well as highlight interdependent channel maps in an adaptive manner. Further, the additional
loss between different modules guides the attention mechanisms to remove the noise and focus on
more discriminant regions of the image by emphasizing relevant feature associations. We evaluate
the proposed model in the context of abdominal organ segmentation on magnetic resonance imaging
(MRI). A series of ablation experiments support the importance of these attention modules in the
proposed architecture. In addition, compared to other state-of-the-art segmentation networks
our model yields better segmentation performance, increasing the accuracy of the predictions
while reducing the standard deviation. This demonstrates the efficiency of our approach to generate
precise and reliable automatic segmentations of medical images. Our code and the trained model
are made publicly available at: https://github.com/sinAshish/Multi-Scale-Attention 