There are two main lines of research on visual reasoning: neural module network (NMN) with \emph{explicit}
multi-hop reasoning through handcrafted neural modules, and monolithic network with \emph{implicit}
reasoning in the latent feature space. The former excels in interpretability and compositionality,
while the latter usually achieves better performance due to model flexibility and parameter efficiency.
In order to bridge the gap of the two, we present Meta Module Network (MMN), a novel hybrid approach
that can efficiently utilize a Meta Module to perform versatile functionalities, while preserving
compositionality and interpretability through modularized design. The proposed model first
parses an input question into a functional program through a Program Generator. Instead of handcrafting
a task-specific network to represent each function like traditional NMN, we use Recipe Encoder
to translate the functions into their corresponding recipes (specifications), which are used
to dynamically instantiate the Meta Module into Instance Modules. To endow different instance
modules with designated functionality, a Teacher-Student framework is proposed, where a symbolic
teacher pre-executes against the scene graphs to provide guidelines for the instantiated modules
(student) to follow. In a nutshell, MMN adopts the meta module to increase its parameterization
efficiency and uses recipe encoding to improve its generalization ability over NMN. Experiments
conducted on the GQA benchmark demonstrates that: ($i$) MMN achieves significant improvement
over both NMN and monolithic network baselines; ($ii$) MMN is able to generalize to unseen but related
functions. 