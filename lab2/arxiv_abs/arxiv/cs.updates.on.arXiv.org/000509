Random samples are popular summaries which allow queries posed over the data to be approximated
by applying an appropriate estimator to the sample. The effectiveness of sampling, however, hinges
on estimator selection. The choice of estimators is subjected to global requirements, such as unbiasedness
and range restrictions on the estimate value, and ideally, we seek estimators that are both efficient
to derive and apply and optimal in that they are not dominated by other estimators. Nevertheless,
for a given data domain, sampling scheme, and query, there are many applicable estimators, as it
is not generally possible to simultaneously minimize the variance for all data. In this work, and
focusing on coordinated sampling, we aim to understand and leverage the choice of estimators. When
we have information on data patterns, we we can design estimators that perform better on "typical"
data. To control the worst-case, we seek variance competitiveness, where the expectation of the
square is within a factor c from optimum. We present natural unbiased and nonnegative variance optimal
and variance competitive estimators that vastly improve over the state of the art. Our order-optimal
estimators minimize variance according to {\em any} specified priorities over the data domain.
When prioritizing for lower values, we obtain the L* estimator, which is the unique monotone variance-optimal
estimator, is 4-competitive, and dominates the classic Horvitz-Thompson estimator. The U* estimator
prioritizes large values. Finally, we study the {\em universal ratio}, which is the smallest competitive
ratio that can be obtained for all functions and provide an upper bound of 3.375 and a lower bound of
1.44. Our estimators are natural and efficient to compute and suitable for data analysis applications.
