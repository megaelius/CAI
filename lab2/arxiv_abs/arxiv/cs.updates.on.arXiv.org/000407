Representations that are invariant to translation, scale and other transformations, can considerably
reduce the sample complexity of learning, allowing recognition of new object classes from very
few examples - a hallmark of human recognition. Empirical estimates of one-dimensional projections
of the distribution induced by a group of affine transformations are proven to represent a unique
and invariant signature associated with an image. We show how projections yielding invariant signatures
for future images can be learned automatically, and updated continuously, during unsupervised
visual experience. A module performing filtering and pooling, like simple and complex cells as
proposed by Hubel and Wiesel, can compute such estimates. Under this view, a pooling stage estimates
a one-dimensional probability distribution. Invariance from observations through a restricted
window is equivalent to a sparsity property w.r.t. to a transformation, which yields templates
that are a) Gabor for optimal simultaneous invariance to translation and scale or b) very specific
for complex, class-dependent transformations such as rotation in depth of faces. Hierarchical
architectures consisting of this basic Hubel-Wiesel module inherit its properties of invariance,
stability, and discriminability while capturing the compositional organization of the visual
world in terms of wholes and parts, and are invariant to complex transformations that may only be
locally affine. The theory applies to several existing deep learning convolutional architectures
for image and speech recognition. It also suggests that the main computational goal of the ventral
stream of visual cortex is to provide a hierarchical representation of new objects which is invariant
to transformations, stable, and discriminative for recognition - this representation may be learned
in an unsupervised way from natural visual experience. 