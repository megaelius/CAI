Over the years, the Web has shrunk the world, allowing individuals to share viewpoints with many
more people than they are able to in real life. At the same time, however, it has also enabled anti-social
and toxic behavior to occur at an unprecedented scale. Video sharing platforms like YouTube receive
uploads from millions of users, covering a wide variety of topics and allowing others to comment
and interact in response. Unfortunately, these communities are periodically plagued with aggression
and hate attacks. In particular, recent work has showed how these attacks often take place as a result
of "raids," i.e., organized efforts coordinated by ad-hoc mobs from third-party communities.
Despite the increasing relevance of this phenomenon, online services often lack effective countermeasures
to mitigate it. Unlike well-studied problems like spam and phishing, coordinated aggressive behavior
both targets and is perpetrated by humans, making defense mechanisms that look for automated activity
unsuitable. Therefore, the de-facto solution is to reactively rely on user reports and human reviews.
In this paper, we propose an automated solution to identify videos that are likely to be targeted
by coordinated harassers. First, we characterize and model YouTube videos along several axes (metadata,
audio transcripts, thumbnails) based on a ground truth dataset of raid victims. Then, we use an ensemble
of classifiers to determine the likelihood that a video will be raided with high accuracy (AUC up
to 94%). Overall, our work paves the way for providing video platforms like YouTube with proactive
systems to detect and mitigate coordinated hate attacks. 