This paper presents a unified training strategy that enables a novel multi-scale deep neural network
to be trained on multiple partially labeled datasets for multi-organ segmentation. Multi-scale
contextual information is effective for pixel-level label prediction, i.e. image segmentation.
However, such important information is only partially exploited by the existing methods. In this
paper, we propose a new network architecture for multi-scale feature abstraction, which integrates
pyramid feature analysis into an image segmentation model. To bridge the semantic gap caused by
directly merging features from different scales, an equal convolutional depth mechanism is proposed.
In addition, we develop a deep supervision mechanism for refining outputs in different scales.
To fully leverage the segmentation features from different scales, we design an adaptive weighting
layer to fuse the outputs in an automatic fashion. All these features together integrate into a pyramid-input
pyramid-output network for efficient feature extraction. Last but not least, to alleviate the
hunger for fully annotated data in training deep segmentation models, a unified training strategy
is proposed to train one segmentation model on multiple partially labeled datasets for multi-organ
segmentation with a novel target adaptive loss. Our proposed method was evaluated on four publicly
available datasets, including BTCV, LiTS, KiTS and Spleen, where very promising performance has
been achieved. The source code of this work is publicly shared at https://github.com/DIAL-RPI/PIPO-FAN
for others to easily reproduce the work and build their own models with the introduced mechanisms.
