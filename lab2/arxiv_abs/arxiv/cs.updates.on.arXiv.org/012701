In this paper, we introduce a novel human interaction detection approach, based on CALIPSO (Classifying
ALl Interacting Pairs in a Single shOt), a classifier of human-object interactions. This new single-shot
interaction classifier estimates interactions simultaneously for all human-object pairs, regardless
of their number and class. State-of-the-art approaches adopt a multi-shot strategy based on a pairwise
estimate of interactions for a set of human-object candidate pairs, which leads to a complexity
depending, at least, on the number of interactions or, at most, on the number of candidate pairs.
In contrast, the proposed method estimates the interactions on the whole image. Indeed, it simultaneously
estimates all interactions between all human subjects and object targets by performing a single
forward pass throughout the image. Consequently, it leads to a constant complexity and computation
time independent of the number of subjects, objects or interactions in the image. In detail, interaction
classification is achieved on a dense grid of anchors thanks to a joint multi-task network that learns
three complementary tasks simultaneously: (i) prediction of the types of interaction, (ii) estimation
of the presence of a target and (iii) learning of an embedding which maps interacting subject and
target to a same representation, by using a metric learning strategy. In addition, we introduce
an object-centric passive-voice verb estimation which significantly improves results. Evaluations
on the two well-known Human-Object Interaction image datasets, V-COCO and HICO-DET, demonstrate
the competitiveness of the proposed method (2nd place) compared to the state-of-the-art while
having constant computation time regardless of the number of objects and interactions in the image.
