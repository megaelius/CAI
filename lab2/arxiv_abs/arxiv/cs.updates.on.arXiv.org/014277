Acute infection, if not rapidly and accurately detected, can lead to sepsis, organ failure and even
death. Currently, detection of acute infection as well as assessment of a patient's severity of
illness are based on imperfect (and often superficial) measures of patient physiology. Characterization
of a patient's immune response by quantifying expression levels of key genes from blood represents
a potentially more timely and precise means of accomplishing both tasks. Machine learning methods
provide a platform for development of deployment-ready classification models robust to the smaller,
more heterogeneous datasets typical of healthcare. Identification of promising classifiers
is dependent, in part, on hyperparameter optimization (HO), for which a number of approaches including
grid search, random sampling and Bayesian optimization have been shown to be effective. In this
analysis, we compare HO approaches for the development of diagnostic classifiers of acute infection
and in-hospital mortality from gene expression of 29 diagnostic markers. Our comprehensive analysis
of a multi-study patient cohort evaluates HO for three different classifier types and over a range
of different optimization settings. Consistent with previous research, we find that Bayesian
optimization is more efficient than grid search or random sampling-based methods, identifying
promising classifiers with fewer evaluated hyperparameter configurations. However, we also
find evidence of a lack of correspondence between internal and external validation performance
of selected classifiers that complicates model selection for deployment as well as stymies development
of clear-cut, practical guidelines for HO application in healthcare. We highlight the need for
additional considerations about patient heterogeneity, dataset partitioning and optimization
setup when applying HO methods in the healthcare context. 