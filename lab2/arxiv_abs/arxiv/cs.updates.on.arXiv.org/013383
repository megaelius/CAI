Compared with Generative Adversarial Networks (GAN), Energy-Based generative Models (EBMs)
possess two appealing properties: i) they can be directly optimized without requiring an auxiliary
network during the learning and synthesizing; ii) they can better approximate underlying distribution
of the observed data by learning explicitly potential functions. This paper studies a branch of
EBMs, i.e., energy-based Generative ConvNets (GCNs), which minimize their energy function defined
by a bottom-up ConvNet. From the perspective of particle physics, we solve the problem of unstable
energy dissipation that might damage the quality of the synthesized samples during the maximum
likelihood learning. Specifically, we firstly establish a connection between classical FRAME
model [1] and dynamic physics process and generalize the GCN in discrete flow with a certain metric
measure from particle perspective. To address KL-vanishing issue, we then reformulate GCN from
the KL discrete flow with KL divergence measure to a Jordan-Kinderleher-Otto (JKO) discrete flow
with Wasserastein distance metric and derive a Wasserastein GCN (wGCN). Based on these theoretical
studies on GCN, we finally derive a Generalized GCN (GGCN) to further improve the model generalization
and learning capability. GGCN introduces a hidden space mapping strategy by employing a normal
distribution for the reference distribution to address the learning bias issue. Due to MCMC sampling
in GCNs, it still suffers from a serious time-consuming issue when sampling steps increase; thus
a trainable non-linear upsampling function and an amortized learning are proposed to improve the
learning efficiency. Our proposed GGCN is trained in a symmetrical learning manner. Our method
surpass the existing models in both model stability and the quality of generated samples on several
widely-used face and natural image datasets. 