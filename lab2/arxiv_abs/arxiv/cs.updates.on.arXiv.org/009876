This paper is a survey and an analysis of different ways of using deep learning (deep artificial neural
networks) to generate musical content. We propose a methodology based on five dimensions for our
analysis: Objective - What musical content is to be generated? Examples are: melody, polyphony,
accompaniment or counterpoint. - For what destination and for what use? To be performed by a human(s)
(in the case of a musical score), or by a machine (in the case of an audio file). Representation - What
are the concepts to be manipulated? Examples are: waveform, spectrogram, note, chord, meter and
beat. - What format is to be used? Examples are: MIDI, piano roll or text. - How will the representation
be encoded? Examples are: scalar, one-hot or many-hot. Architecture - What type(s) of deep neural
network is (are) to be used? Examples are: feedforward network, recurrent network, autoencoder
or generative adversarial networks. Challenge - What are the limitations and open challenges?
Examples are: variability, interactivity and creativity. Strategy - How do we model and control
the process of generation? Examples are: single-step feedforward, iterative feedforward, sampling
or input manipulation. For each dimension, we conduct a comparative analysis of various models
and techniques and we propose some tentative multidimensional typology. This typology is bottom-up,
based on the analysis of many existing deep-learning based systems for music generation selected
from the relevant literature. These systems are described and are used to exemplify the various
choices of objective, representation, architecture, challenge and strategy. The last section
includes some discussion and some prospects. 