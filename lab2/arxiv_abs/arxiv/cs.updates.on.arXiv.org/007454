The geometric problem of estimating an unknown compact convex set from evaluations of its support
function arises in a range of scientific and engineering applications. Traditional approaches
typically rely on estimators that minimize the error over all possible compact convex sets; in particular,
these methods do not allow for the incorporation of prior structural information about the underlying
set and the resulting estimates become increasingly more complicated to describe as the number
of measurements available grows. We address both of these shortcomings by describing a framework
for estimating tractably specified convex sets from support function evaluations. Building on
the literature in convex optimization, our approach is based on estimators that minimize the error
over structured families of convex sets that are specified as linear images of concisely described
sets -- such as the simplex or the free spectrahedron -- in a higher-dimensional space that is not
much larger than the ambient space. Convex sets parametrized in this manner are significant from
a computational perspective as one can optimize linear functionals over such sets efficiently;
they serve a different purpose in the inferential context of the present paper, namely, that of incorporating
regularization in the reconstruction while still offering considerable expressive power. We
provide a geometric characterization of the asymptotic behavior of our estimators, and our analysis
relies on the property that certain sets which admit semialgebraic descriptions are Vapnik-Chervonenkis
(VC) classes. Our numerical experiments highlight the utility of our framework over previous approaches
in settings in which the measurements available are noisy or small in number as well as those in which
the underlying set to be reconstructed is non-polyhedral. 