In recent scene recognition research images or large image regions are often represented as disorganized
"bags" of features which can then be analyzed using models originally developed to capture co-variation
of word counts in text. However, image feature counts are likely to be constrained in different ways
than word counts in text. For example, as a camera pans upwards from a building entrance over its first
few floors and then further up into the sky Fig. 1, some feature counts in the image drop while others
rise -- only to drop again giving way to features found more often at higher elevations. The space
of all possible feature count combinations is constrained both by the properties of the larger scene
and the size and the location of the window into it. To capture such variation, in this paper we propose
the use of the counting grid model. This generative model is based on a grid of feature counts, considerably
larger than any of the modeled images, and considerably smaller than the real estate needed to tile
the images next to each other tightly. Each modeled image is assumed to have a representative window
in the grid in which the feature counts mimic the feature distribution in the image. We provide a learning
procedure that jointly maps all images in the training set to the counting grid and estimates the
appropriate local counts in it. Experimentally, we demonstrate that the resulting representation
captures the space of feature count combinations more accurately than the traditional models,
not only when the input images come from a panning camera, but even when modeling images of different
scenes from the same category. 