Brain computer interfaces (BCI) enable direct communication with a computer, using neural activity
as the control signal. This neural signal is generally chosen from a variety of well-studied electroencephalogram
(EEG) signals. For a given BCI paradigm, feature extractors and classifiers are tailored to the
distinct characteristics of its expected EEG control signal, limiting its application to that
specific signal. Convolutional Neural Networks (CNNs), which have been used in computer vision
and speech recognition, have successfully been applied to EEG-based BCIs; however, they have mainly
been applied to single BCI paradigms and thus it remains unclear how these architectures generalize
to other paradigms. Here, we ask if we can design a single CNN architecture to accurately classify
EEG signals from different BCI paradigms, while simultaneously being as compact as possible. In
this work we introduce EEGNet, a compact convolutional network for EEG-based BCIs. We introduce
the use of depthwise and separable convolutions to construct an EEG-specific model which encapsulates
well-known EEG feature extraction concepts for BCI. We compare EEGNet to current state-of-the-art
approaches across four BCI paradigms: P300 visual-evoked potentials, error-related negativity
responses (ERN), movement-related cortical potentials (MRCP), and sensory motor rhythms (SMR).
We show that EEGNet generalizes across paradigms better than the reference algorithms when only
limited training data is available. We demonstrate three different approaches to visualize the
contents of a trained EEGNet model to enable interpretation of the learned features. Our results
suggest that EEGNet is robust enough to learn a wide variety of interpretable features over a range
of BCI tasks, suggesting that the observed performances were not due to artifact or noise sources
in the data. 