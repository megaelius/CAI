Bregman divergences $D_\phi$ are a class of divergences parametrized by a convex function $\phi$
and include well known distance functions like $\ell_2^2$ and the Kullback-Leibler divergence.
There has been extensive research on algorithms for problems like clustering and near neighbor
search with respect to Bregman divergences; in all cases, the algorithms depend not just on the data
size $n$ and dimensionality $d$, but also on a structure constant $\mu \ge 1$ that depends solely
on $\phi$ and can grow without bound independently. In this paper, we provide the first evidence
that this dependence on $\mu$ might be intrinsic. We focus on the problem of approximate near neighbor
search for Bregman divergences. We show that under the cell probe model, any non-adaptive data structure
(like locality-sensitive hashing) for $c$-approximate near-neighbor search that admits $r$
probes must use space $\Omega(n^{1 + \frac{\mu}{c r}})$. In contrast, for LSH under $\ell_1$ the
best bound is $\Omega(n^{1+\frac{1}{cr}})$. Our new tool is a directed variant of the standard
boolean noise operator. We show that a generalization of the Bonami-Beckner hypercontractivity
inequality exists ``in expectation'' or upon restriction to certain subsets of the Hamming cube,
and that this is sufficient to prove the desired isoperimetric inequality that we use in our data
structure lower bound. We also present a structural result reducing the Hamming cube to a Bregman
cube. This structure allows us to obtain lower bounds for problems under Bregman divergences from
their $\ell_1$ analog. In particular, we get a (weaker) lower bound for approximate near neighbor
search of the form $\Omega(n^{1 + \frac{1}{cr}})$ for an $r$-query non-adaptive data structure,
and new cell probe lower bounds for a number of other near neighbor questions in Bregman space. 