A major impediment to the application of deep learning to real-world problems is the scarcity of
labeled data. Small training sets are in fact of no use to deep networks as, due to the large number
of trainable parameters, they will very likely be subject to overfitting phenomena. On the other
hand, the increment of the training set size through further manual or semi-automatic labellings
can be costly, if not possible at times. Thus, the standard techniques to address this issue are transfer
learning and data augmentation, which consists of applying some sort of "transformation" to existing
labeled instances to let the training set grow in size. Although this approach works well in applications
such as image classification, where it is relatively simple to design suitable transformation
operators, it is not obvious how to apply it in more structured scenarios. Motivated by the observation
that in virtually all application domains it is easy to obtain unlabeled data, in this paper we take
a different perspective and propose a \emph{label augmentation} approach. We start from a small,
curated labeled dataset and let the labels propagate through a larger set of unlabeled data using
graph transduction techniques. This allows us to naturally use (second-order) similarity information
which resides in the data, a source of information which is typically neglected by standard augmentation
techniques. In particular, we show that by using known game theoretic transductive processes we
can create larger and accurate enough labeled datasets which use results in better trained neural
networks. Preliminary experiments are reported which demonstrate a consistent improvement over
standard image classification datasets. 