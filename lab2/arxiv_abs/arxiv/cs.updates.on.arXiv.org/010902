Channel attention has recently demonstrated to offer great potential in improving the performance
of deep convolutional neural networks (CNNs). However, most existing methods dedicate to developing
more sophisticated attention modules to achieve better performance, inevitably increasing the
computational burden. To overcome the paradox of performance and complexity trade-off, this paper
makes an attempt to investigate an extremely lightweight attention module for boosting the performance
of deep CNNs. In particular, we propose an Efficient Channel Attention (ECA) module, which only
involves $k (k < 9)$ parameters but brings clear performance gain. By revisiting the channel attention
module in SENet, we empirically show avoiding dimensionality reduction and appropriate cross-channel
interaction are important to learn effective channel attention. Therefore, we propose a local
cross-channel interaction strategy without dimension reduction, which can be efficiently implemented
by a fast 1D convolution. Furthermore, we develop a function of channel dimension to adaptively
determine kernel size of 1D convolution, which stands for coverage of local cross-channel interaction.
Our ECA module can be flexibly incorporated into existing CNN architectures, and the resulting
CNNs are named by ECA-Net. We extensively evaluate the proposed ECA-Net on image classification,
object detection and instance segmentation with backbones of ResNets and MobileNetV2. The experimental
results show our ECA-Net is more efficient while performing favorably against its counterparts.
The source code and models can be available at https://github.com/BangguWu/ECANet. 