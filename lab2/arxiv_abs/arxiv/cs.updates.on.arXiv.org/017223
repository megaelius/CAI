In the context of science, the well-known adage "a picture is worth a thousand words" might well be
"a model is worth a thousand datasets." Scientific models, such as Newtonian physics or biological
gene regulatory networks, are human-driven simplifications of complex phenomena that serve as
surrogates for the countless experiments that validated the models. Recently, machine learning
has been able to overcome the inaccuracies of approximate modeling by directly learning the entire
set of nonlinear interactions from data. However, without any predetermined structure from the
scientific basis behind the problem, machine learning approaches are flexible but data-expensive,
requiring large databases of homogeneous labeled training data. A central challenge is reconciling
data that is at odds with simplified models without requiring "big data". In this work we develop
a new methodology, universal differential equations (UDEs), which augments scientific models
with machine-learnable structures for scientifically-based learning. We show how UDEs can be
utilized to discover previously unknown governing equations, accurately extrapolate beyond
the original data, and accelerate model simulation, all in a time and data-efficient manner. This
advance is coupled with open-source software that allows for training UDEs which incorporate physical
constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity
in the model. Our examples show how a diverse set of computationally-difficult modeling issues
across scientific disciplines, from automatically discovering biological mechanisms to accelerating
the training of physics-informed neural networks and large-eddy simulations, can all be transformed
into UDE training problems that are efficiently solved by a single software methodology. 