In this work, we present tensor-based linear and nonlinear models for hyperspectral data classification
and analysis. By exploiting principles of tensor algebra, we introduce new classification architectures,
the weight parameters of which satisfies the {\it rank}-1 canonical decomposition property. Then,
we introduce learning algorithms to train both the linear and the non-linear classifier in a way
to i) to minimize the error over the training samples and ii) the weight coefficients satisfies the
{\it rank}-1 canonical decomposition property. The advantages of the proposed classification
model is that i) it reduces the number of parameters required and thus reduces the respective number
of training samples required to properly train the model, ii) it provides a physical interpretation
regarding the model coefficients on the classification output and iii) it retains the spatial and
spectral coherency of the input samples. To address issues related with linear classification,
characterizing by low capacity, since it can produce rules that are linear in the input space, we
introduce non-linear classification models based on a modification of a feedforward neural network.
We call the proposed architecture {\it rank}-1 Feedfoward Neural Network (FNN), since their weights
satisfy the {\it rank}-1 caconical decomposition property. Appropriate learning algorithms
are also proposed to train the network. Experimental results and comparisons with state of the art
classification methods, either linear (e.g., SVM) and non-linear (e.g., deep learning) indicates
the outperformance of the proposed scheme, especially in cases where a small number of training
samples are available. Furthermore, the proposed tensor-based classfiers are evaluated against
their capabilities in dimensionality reduction. 