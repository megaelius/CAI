As automation proliferates and algorithms become increasingly responsible for high-stakes decision-making,
AI agents face moral dilemmas in fields ranging from market design to robots. Prior approaches to
automated moral decision-making utilize either rules-based game theoretic models or machine
learning models trained on crowd-sourced data. But rules-based systems are difficult to adapt
to new moral dilemmas and data, and sourcing high quality, representative, hand-labeled data for
machine learning is costly and even harmful if the labels are biased. To lower the barrier to training
moral agents, I develop a heuristic-based weak learning approach to moral decision-making. My
approach synthesizes potentially conflicting legal, philosophical, and domain-specific heuristics
to inexpensively and automatically label training data for moral dilemmas. Rather than attempting
to survey a representative sample of users who may be unable to make informed decisions about complex
dilemmas, this approach relies on a smaller sample of domain experts. By writing heuristic functions
over the dataset, these experts efficiently specify ethical principles for technical dilemmas.
Weak learning paves the way to a ubiquitous, transparent method for instilling moral decision-making
in the machine learning pipeline. I test this approach in two case studies for which there is publicly
available data on people's moral preferences: 1) the Moral Machine trolley problem, in which an
autonomous vehicle must choose to save only one group of characters; 2) a kidney exchange, in which
a market clearing algorithm must choose between two potential matches for a donor kidney. I find
that in these domains, heuristic-based weak learning is quicker and easier than fully supervised
learning and achieves comparable performance. 