Reconstruction error bounds in compressed sensing under Gaussian or uniform bounded noise do not
translate easily to the case of Poisson noise. Reasons for this include the signal dependent nature
of Poisson noise, and also the fact that the negative log likelihood (NLL) in case of a Poisson distribution
(which is related to the generalized Kullback-Leibler divergence (GKLD)) is not a metric and does
not obey the triangle inequality. There exist prior theoretical results in the form of provable
error bounds for computationally tractable estimators for compressed sensing problems under
Poisson noise. However, these results do not apply to realistic compressive systems, which must
obey some crucial constraints such as non-negativity and flux preservation. On the other hand,
there exist provable error bounds for such realistic systems in the published literature, but they
are for estimators that are computationally intractable. In this paper, we develop error bounds
for a computationally tractable estimator which also applies to realistic compressive systems
obeying the required constraints. Our technique replaces the GKLD, with an information theoretic
metric - namely the square root of the Jensen-Shannon divergence (JSD), which is related to an approximate,
symmetrized version of the Poisson NLL. We show that this allows for simple proofs of the error bounds.
We propose and prove interesting statistical properties of the square root of JSD and exploit other
known ones. Numerical experiments are performed showing the use of the technique in signal and image
reconstruction from compressed measurements under Poisson noise. Our technique applies to sparse/
compressible signals in any orthonormal basis, works with high probability for any randomly generated
non-negative and flux-preserving sensing matrix and is proposes an estimator whose parameters
are purely statistically motivated. 