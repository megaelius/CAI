Controversies around race and machine learning have sparked debate among computer scientists
over how to design machine learning systems that guarantee fairness. These debates rarely engage
with how racial identity is embedded in our social experience, making for sociological and psychological
complexity. This complexity challenges the paradigm of considering fairness to be a formal property
of supervised learning with respect to protected personal attributes. Racial identity is not simply
a personal subjective quality. For people labeled "Black" it is an ascribed political category
that has consequences for social differentiation embedded in systemic patterns of social inequality
achieved through both social and spatial segregation. In the United States, racial classification
can best be understood as a system of inherently unequal status categories that places whites as
the most privileged category while signifying the Negro/black category as stigmatized. Social
stigma is reinforced through the unequal distribution of societal rewards and goods along racial
lines that is reinforced by state, corporate, and civic institutions and practices. This creates
a dilemma for society and designers: be blind to racial group disparities and thereby reify racialized
social inequality by no longer measuring systemic inequality, or be conscious of racial categories
in a way that itself reifies race. We propose a third option. By preceding group fairness interventions
with unsupervised learning to dynamically detect patterns of segregation, machine learning systems
can mitigate the root cause of social disparities, social segregation and stratification, without
further anchoring status categories of disadvantage. 