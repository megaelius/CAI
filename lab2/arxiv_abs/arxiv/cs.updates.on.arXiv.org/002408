Transfer learning leverages the knowledge in one domain, the source domain, to improve learning
efficiency in another domain, the target domain. Existing transfer learning research is relatively
well-progressed, but only in situations where the feature spaces of the domains are homogeneous
and the target domain contains at least a few labeled instances. However, transfer learning has
not been well-studied in heterogeneous settings with an unlabeled target domain. To contribute
to the research in this emerging field, this paper presents: (1) an unsupervised knowledge transfer
theorem that prevents negative transfer; and (2) a principal angle-based metric to measure the
distance between two pairs of domains. The metric shows the extent to which homogeneous representations
have preserved the information in original source and target domains. The unsupervised knowledge
transfer theorem sets out the transfer conditions necessary to prevent negative transfer. Linear
monotonic maps meet the transfer conditions of the theorem and, hence, are used to construct homogeneous
representations of the heterogeneous domains, which in principle prevents negative transfer.
The metric and the theorem have been implemented in an innovative transfer model, called a Grassmann-LMM-geodesic
flow kernel (GLG), that is specifically designed for knowledge transfer across heterogeneous
domains. The GLG model learns homogeneous representations of heterogeneous domains by minimizing
the proposed metric. Knowledge is transferred through these learned representations via a geodesic
flow kernel. Notably, the theorem presented in this paper provides the sufficient transfer conditions
needed to guarantee that knowledge is transferred from a source domain to an unlabeled target domain
with correctness. 