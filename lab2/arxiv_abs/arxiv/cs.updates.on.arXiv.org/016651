The recent proliferation of fake portrait videos poses direct threats on society, law, and privacy.
Believing the fake video of a politician, distributing fake pornographic content of celebrities,
fabricating impersonated fake videos as evidence in courts are just a few real world consequences
of deep fakes. We present a novel approach to detect synthetic content in portrait videos, as a preventive
solution for the emerging threat of deep fakes. In other words, we introduce a deep fake detector.
We observe that detectors blindly utilizing deep learning are not effective in catching fake content,
as generative models produce formidably realistic results. Our key assertion follows that biological
signals hidden in portrait videos can be used as an implicit descriptor of authenticity, because
they are neither spatially nor temporally preserved in fake content. To prove and exploit this assertion,
we first engage several signal transformations for the pairwise separation problem, achieving
99.39% accuracy. Second, we utilize those findings to formulate a generalized classifier for fake
content, by analyzing proposed signal transformations and corresponding feature sets. Third,
we generate novel signal maps and employ a CNN to improve our traditional classifier for detecting
synthetic content. Lastly, we release an "in the wild" dataset of fake portrait videos that we collected
as a part of our evaluation process. We evaluate FakeCatcher on several datasets, resulting with
96%, 94.65%, 91.50%, and 91.07% accuracies, on Face Forensics, Face Forensics++, CelebDF, and
on our new Deep Fakes Dataset respectively. We also analyze signals from various facial regions,
under image distortions, with varying segment durations, from different generators, against
unseen datasets, and under several dimensionality reduction techniques. 