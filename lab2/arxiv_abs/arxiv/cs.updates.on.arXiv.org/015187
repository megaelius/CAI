Medical image segmentation can provide a reliable basis for further clinical analysis and disease
diagnosis. The performance of medical image segmentation has been significantly advanced with
the convolutional neural networks (CNNs). However, most existing CNNs-based methods often produce
unsatisfactory segmentation mask without accurate object boundaries. This is caused by the limited
context information and inadequate discriminative feature maps after consecutive pooling and
convolution operations. In that the medical image is characterized by the high intra-class variation,
inter-class indistinction and noise, extracting powerful context and aggregating discriminative
features for fine-grained segmentation are still challenging today. In this paper, we formulate
a boundary-aware context neural network (BA-Net) for 2D medical image segmentation to capture
richer context and preserve fine spatial information. BA-Net adopts encoder-decoder architecture.
In each stage of encoder network, pyramid edge extraction module is proposed for obtaining edge
information with multiple granularities firstly. Then we design a mini multi-task learning module
for jointly learning to segment object masks and detect lesion boundaries. In particular, a new
interactive attention is proposed to bridge two tasks for achieving information complementarity
between different tasks, which effectively leverages the boundary information for offering a
strong cue to better segmentation prediction. At last, a cross feature fusion module aims to selectively
aggregate multi-level features from the whole encoder network. By cascaded three modules, richer
context and fine-grain features of each stage are encoded. Extensive experiments on five datasets
show that the proposed BA-Net outperforms state-of-the-art approaches. 