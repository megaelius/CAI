We consider nonlinear convergence acceleration methods for fixed-point iteration $x_{k+1}=q(x_k)$,
including Anderson acceleration (AA), nonlinear GMRES (NGMRES), and Nesterov-type acceleration
(corresponding to AA with window size one). We focus on fixed-point methods that converge asymptotically
linearly with convergence factor $\rho<1$ and that solve an underlying fully smooth and non-convex
optimization problem. It is often observed that AA and NGMRES substantially improve the asymptotic
convergence behavior of the fixed-point iteration, but this improvement has not been quantified
theoretically. We investigate this problem under simplified conditions. First, we consider stationary
versions of AA and NGMRES, and determine coefficients that result in optimal asymptotic convergence
factors, given knowledge of the spectrum of $q'(x)$ at the fixed point $x^*$. This allows us to understand
and quantify the asymptotic convergence improvement that can be provided by nonlinear convergence
acceleration, viewing $x_{k+1}=q(x_k)$ as a nonlinear preconditioner for AA and NGMRES. Second,
for the case of infinite window size, we consider linear asymptotic convergence bounds for GMRES
applied to the fixed-point iteration linearized about $x^*$. Since AA and NGMRES are equivalent
to GMRES in the linear case, one may expect the GMRES convergence factors to be relevant for AA and
NGMRES as $x_k \rightarrow x^*$. Our results are illustrated numerically for a class of test problems
from canonical tensor decomposition, comparing steepest descent and alternating least squares
(ALS) as the fixed-point iterations that are accelerated by AA and NGMRES. Our numerical tests show
that both approaches allow us to estimate asymptotic convergence speed for nonstationary AA and
NGMRES with finite window size. 