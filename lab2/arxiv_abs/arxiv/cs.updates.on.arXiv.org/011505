Combining data from varied sources has considerable potential for knowledge discovery: collaborating
data parties can mine data in an expanded feature space, allowing them to explore a larger range of
scientific questions. However, data sharing among different parties is highly restricted by legal
conditions, ethical concerns, and / or data volume. Fueled by these concerns, the fields of cryptography
and distributed learning have made great progress towards privacy-preserving and distributed
data mining. However, practical implementations have been hampered by the limited scope or computational
complexity of these methods. In this paper, we greatly extend the range of analyses available for
vertically partitioned data, i.e., data collected by separate parties with different features
on the same subjects. To this end, we present a novel approach for privacy-preserving generalized
linear models, a fundamental and powerful framework underlying many prediction and classification
procedures. We base our method on a distributed block coordinate descent algorithm to obtain parameter
estimates, and we develop an extension to compute accurate standard errors without additional
communication cost. We critically evaluate the information transfer for semi-honest collaborators
and show that our protocol is secure against data reconstruction. Through both simulated and real-world
examples we illustrate the functionality of our proposed algorithm. Without leaking information,
our method performs as well on vertically partitioned data as existing methods on combined data
-- all within mere minutes of computation time. We conclude that our method is a viable approach for
vertically partitioned data analysis with a wide range of real-world applications. 