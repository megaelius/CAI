The popularity of mobile devices results in the availability of enormous data and computational
resources at the network edge. To leverage the data and resources, a new machine learning paradigm,
called edge learning, has emerged where learning algorithms are deployed at the edge for providing
fast and intelligent services to mobile users. While computing speeds are advancing rapidly, the
communication latency is becoming the bottleneck of fast edge learning. To address this issue,
this work is focused on designing a low latency multi-access scheme for edge learning. We consider
a popular framework, federated edge learning (FEEL), where edge-server and on-device learning
are synchronized to train a model without violating user-data privacy. It is proposed that model
updates simultaneously transmitted by devices over broadband channels should be analog aggregated
"over-the-air" by exploiting the superposition property of a multi-access channel. Thereby,
"interference" is harnessed to provide fast implementation of the model aggregation. This results
in dramatical latency reduction compared with the traditional orthogonal access (i.e., OFDMA).
In this work, the performance of FEEL is characterized targeting a single-cell random network.
First, due to power alignment between devices as required for aggregation, a fundamental tradeoff
is shown to exist between the update-reliability and the expected update-truncation ratio. This
motivates the design of an opportunistic scheduling scheme for FEEL that selects devices within
a distance threshold. This scheme is shown using real datasets to yield satisfactory learning performance
in the presence of high mobility. Second, both the multi-access latency of the proposed analog aggregation
and the OFDMA scheme are analyzed. Their ratio, which quantifies the latency reduction of the former,
is proved to scale almost linearly with device population. 