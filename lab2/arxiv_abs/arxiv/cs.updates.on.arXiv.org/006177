This article studies the domain adaptation problem in person re-identification (re-ID) under
a "learning via translation" framework, consisting of two components, 1) translating the labeled
images from the source to the target domain in an unsupervised manner, 2) learning a re-ID model using
the translated images. The objective is to preserve the underlying human identity information
after image translation, so that translated images with labels are effective for feature learning
on the target domain. To this end, we propose a similarity preserving generative adversarial network
(SPGAN) and its end-to-end trainable version, eSPGAN. Both aiming at similarity preserving, SPGAN
enforces this property by heuristic constraints, while eSPGAN does so by optimally facilitating
the re-ID model learning. More specifically, SPGAN separately undertakes the two components in
the "learning via translation" framework. It first preserves two types of unsupervised similarity,
namely, self-similarity of an image before and after translation, and domain-dissimilarity of
a translated source image and a target image. It then learns a re-ID model using existing networks.
In comparison, eSPGAN seamlessly integrates image translation and re-ID model learning. During
the end-to-end training of eSPGAN, re-ID learning guides image translation to preserve the underlying
identity information of an image. Meanwhile, image translation improves re-ID learning by providing
identity-preserving training samples of the target domain style. In the experiment, we show that
identities of the fake images generated by SPGAN and eSPGAN are well preserved. Based on this, we
report the new state-of-the-art domain adaptation results on two large-scale person re-ID datasets.
