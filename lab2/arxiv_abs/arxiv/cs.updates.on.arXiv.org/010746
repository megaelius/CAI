The use of lower precision has emerged as a popular technique to optimize the compute and storage
requirements of complex Deep Neural Networks (DNNs). In the quest for lower precision, recent studies
have shown that ternary DNNs, which represent weights and activations by signed ternary values,
represent a promising sweet spot, and achieve accuracy close to full-precision networks on complex
tasks such as language modeling and image classification. We propose TiM-DNN, a programmable,
in-memory accelerator that is specifically designed to execute ternary DNNs. TiM-DNN supports
various ternary representations including unweighted (-1,0,1), symmetric weighted (-a,0,a),
and asymmetric weighted (-a,0,b) ternary systems. TiM-DNN is designed using TiM tiles -- specialized
memory arrays that perform massively parallel signed vector-matrix multiplications on ternary
values with a single access. TiM tiles are in turn composed of Ternary Processing Cells (TPCs), new
bit-cells that function as both ternary storage units and signed scalar multiplication units.
We evaluate an implementation of TiM-DNN in 32nm technology using an architectural simulator calibrated
with SPICE simulations and RTL synthesis. TiM-DNN achieves a peak performance of 114 TOPs/s, consumes
0.9W power, and occupies 1.96mm2 chip area, representing a 300X and 388X improvement in TOPS/W and
TOPS/mm2, respectively, compared to a state-of-the-art NVIDIA Tesla V100 GPU. In comparison to
popular DNN accelerators, TiM-DNN achieves 55.2X-240X and 160X-291X improvement in TOPS/W and
TOPS/mm2, respectively. We compare TiM-DNN with a well-optimized near-memory accelerator for
ternary DNNs across a suite of state-of-the-art DNN benchmarks including both deep convolutional
and recurrent neural networks, demonstrating 3.9x-4.7x improvement in system-level energy and
3.2x-4.2x speedup. 