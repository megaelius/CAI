We study the problem of learning personalized decision policies from observational data while
accounting for possible unobserved confounding in the data-generating process. Previous approaches,
which assume unconfoundedness, i.e., that no unobserved confounders affect both the treatment
assignment as well as outcome, can lead to policies that actually introduce significant harm rather
then benefit due to overeager intervention when some unobserved confounding is present, as is actually
the case in most applications dealing with observational data. Instead, we calibrate policy learning
for realistic violations of this unverifiable assumption with uncertainty sets motivated by sensitivity
analysis in causal inference. Our framework for confounding-robust policy improvement optimizes
the minimax regret of a candidate policy against a baseline standard-of-care policy over an uncertainty
set for propensity weights. We prove that if the uncertainty set is well-specified, our robust policy,
when applied in practice, will do no worse than the baseline and improve upon it if possible. We characterize
the adversarial optimization subproblem and use efficient algorithmic solutions to optimize
over parametrized spaces of decision policies such as logistic treatment assignment and decision
trees. We assess our methods on synthetic data and on a large clinical trial of acute ischaemic stroke
treatment, demonstrating that hidden confounding can hinder existing policy learning approaches
and lead to unwarranted harm, while our robust approach guarantees safety and focuses on well-evidenced
improvement, a necessity for making personalized treatment policies learned from observational
data reliable in practice. 