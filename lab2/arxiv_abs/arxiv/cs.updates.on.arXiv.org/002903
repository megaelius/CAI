DNN is presenting human-level performance for many complex intelligent tasks in real-world applications.
However, it also introduces ever-increasing security concerns. For example, the emerging adversarial
attacks indicate that even very small and often imperceptible adversarial input perturbations
can easily mislead the cognitive function of deep learning systems (DLS). Existing DNN adversarial
studies are narrowly performed on the ideal software-level DNN models with a focus on single uncertainty
factor, i.e. input perturbations, however, the impact of DNN model reshaping on adversarial attacks,
which is introduced by various hardware-favorable techniques such as hash-based weight compression
during modern DNN hardware implementation, has never been discussed. In this work, we for the first
time investigate the multi-factor adversarial attack problem in practical model optimized deep
learning systems by jointly considering the DNN model-reshaping (e.g. HashNet based deep compression)
and the input perturbations. We first augment adversarial example generating method dedicated
to the compressed DNN models by incorporating the software-based approaches and mathematical
modeled DNN reshaping. We then conduct a comprehensive robustness and vulnerability analysis
of deep compressed DNN models under derived adversarial attacks. A defense technique named "gradient
inhibition" is further developed to ease the generating of adversarial examples thus to effectively
mitigate adversarial attacks towards both software and hardware-oriented DNNs. Simulation results
show that "gradient inhibition" can decrease the average success rate of adversarial attacks from
87.99% to 4.77% (from 86.74% to 4.64%) on MNIST (CIFAR-10) benchmark with marginal accuracy degradation
across various DNNs. 