In this paper, we propose an end-to-end post-filter method with deep attention fusion features
for monaural speaker-independent speech separation. At first, a time-frequency domain speech
separation method is applied as the pre-separation stage. The aim of pre-separation stage is to
separate the mixture preliminarily. Although this stage can separate the mixture, it still contains
the residual interference. In order to enhance the pre-separated speech and improve the separation
performance further, the end-to-end post-filter (E2EPF) with deep attention fusion features
is proposed. The E2EPF can make full use of the prior knowledge of the pre-separated speech, which
contributes to speech separation. It is a fully convolutional speech separation network and uses
the waveform as the input features. Firstly, the 1-D convolutional layer is utilized to extract
the deep representation features for the mixture and pre-separated signals in the time domain.
Secondly, to pay more attention to the outputs of the pre-separation stage, an attention module
is applied to acquire deep attention fusion features, which are extracted by computing the similarity
between the mixture and the pre-separated speech. These deep attention fusion features are conducive
to reduce the interference and enhance the pre-separated speech. Finally, these features are sent
to the post-filter to estimate each target signals. Experimental results on the WSJ0-2mix dataset
show that the proposed method outperforms the state-of-the-art speech separation method. Compared
with the pre-separation method, our proposed method can acquire 64.1%, 60.2%, 25.6% and 7.5% relative
improvements in scale-invariant source-to-noise ratio (SI-SNR), the signal-to-distortion
ratio (SDR), the perceptual evaluation of speech quality (PESQ) and the short-time objective intelligibility
(STOI) measures, respectively. 