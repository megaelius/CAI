Recently, neural networks trained as optimizers under the "learning to learn" or meta-learning
framework have been shown to be effective for a broad range of optimization tasks including derivative-free
black-box function optimization. Recurrent neural networks (RNNs) trained to optimize a diverse
set of synthetic non-convex differentiable functions via gradient descent have been effective
at optimizing derivative-free black-box functions. In this work, we propose RNN-Opt: an approach
for learning RNN-based optimizers for optimizing real-parameter single-objective continuous
functions under limited budget constraints. Existing approaches utilize an observed improvement
based meta-learning loss function for training such models. We propose training RNN-Opt by using
synthetic non-convex functions with known (approximate) optimal values by directly using discounted
regret as our meta-learning loss function. We hypothesize that a regret-based loss function mimics
typical testing scenarios, and would therefore lead to better optimizers compared to optimizers
trained only to propose queries that improve over previous queries. Further, RNN-Opt incorporates
simple yet effective enhancements during training and inference procedures to deal with the following
practical challenges: i) Unknown range of possible values for the black-box function to be optimized,
and ii) Practical and domain-knowledge based constraints on the input parameters. We demonstrate
the efficacy of RNN-Opt in comparison to existing methods on several synthetic as well as standard
benchmark black-box functions along with an anonymized industrial constrained optimization
problem. 