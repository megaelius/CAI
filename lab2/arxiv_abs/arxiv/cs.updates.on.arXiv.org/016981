Decision making in crucial applications such as lending, hiring, and college admissions has witnessed
increasing use of algorithmic models and techniques as a result of a confluence of factors such as
ubiquitous connectivity, ability to collect, aggregate, and process large amounts of fine-grained
data using cloud computing, and ease of access to applying sophisticated machine learning models.
Quite often, such applications are powered by search and recommendation systems, which in turn
make use of personalized ranking algorithms. At the same time, there is increasing awareness about
the ethical and legal challenges posed by the use of such data-driven systems. Researchers and practitioners
from different disciplines have recently highlighted the potential for such systems to discriminate
against certain population groups, due to biases in the datasets utilized for learning their underlying
recommendation models. We present a study of fairness in online personalization settings involving
the ranking of individuals. Starting from a fair warm-start machine-learned model, we first demonstrate
that online personalization can cause the model to learn to act in an unfair manner if the user is biased
in his/her responses. For this purpose, we construct a stylized model for generating training data
with potentially biased features as well as potentially biased labels and quantify the extent of
bias that is learned by the model when the user responds in a biased manner as in many real-world scenarios.
We then formulate the problem of learning personalized models under fairness constraints and present
a regularization based approach for mitigating biases in machine learning. We demonstrate the
efficacy of our approach through extensive simulations with different parameter settings. 