The hard problem in artificial intelligence asks how the shuffling of syntactical symbols in a program
can lead to systems which experience semantics and qualia. We address this question in three stages.
First, we introduce a new class of human semantic symbols which appears when unexpected and drastic
environmental change causes humans to become surprised, confused, uncertain, and in extreme cases,
unresponsive, passive and dysfunctional. For this class of symbols, pre-learned programs become
inoperative so these syntactical programs cannot be the source of experienced qualia. Second,
we model the dysfunctional human response to a radically changed environment as being the natural
response of any learning machine facing novel inputs from well outside its previous training set.
In this situation, learning machines are unable to extract information from their input and will
typically enter a dynamical state characterized by null outputs and a lack of response. This state
immediately predicts and explains the characteristics of the semantic experiences of humans in
similar circumstances. In the third stage, we consider learning machines trained to implement
multiple functions in simple sequential programs using environmental data to specify subroutine
names, control flow instructions, memory calls, and so on. Drastic change in any of these environmental
inputs can again lead to inoperative programs. By examining changes specific to people or locations
we can model human cognitive symbols featuring these dependencies, such as attachment and grief.
Our approach links known dynamical machines states with human qualia and thus offers new insight
into the hard problem of artificial intelligence. 