Typically a classifier trained on a given dataset (source domain) does not performs well if it is
tested on data acquired in a different setting (target domain). This is the problem that domain adaptation
(DA) tries to overcome and, while it is a well explored topic in computer vision, it is largely ignored
in robotic vision where usually visual classification methods are trained and tested in the same
domain. Robots should be able to deal with unknown environments, recognize objects and use them
in the correct way, so it is important to explore the domain adaptation scenario also in this context.
The goal of the project is to define a benchmark and a protocol for multi-modal domain adaptation
that is valuable for the robot vision community. With this purpose some of the state-of-the-art
DA methods are selected: Deep Adaptation Network (DAN), Domain Adversarial Training of Neural
Network (DANN), Automatic Domain Alignment Layers (AutoDIAL) and Adversarial Discriminative
Domain Adaptation (ADDA). Evaluations have been done using different data types: RGB only, depth
only and RGB-D over the following datasets, designed for the robotic community: RGB-D Object Dataset
(ROD), Web Object Dataset (WOD), Autonomous Robot Indoor Dataset (ARID), Big Berkeley Instance
Recognition Dataset (BigBIRD) and Active Vision Dataset. Although progresses have been made on
the formulation of effective adaptation algorithms and more realistic object datasets are available,
the results obtained show that, training a sufficiently good object classifier, especially in
the domain adaptation scenario, is still an unsolved problem. Also the best way to combine depth
with RGB informations to improve the performance is a point that needs to be investigated more. 