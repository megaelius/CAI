There are a number of studies about extraction of bottleneck (BN) features from deep neural networks
(DNNs)trained to discriminate speakers, pass-phrases and triphone states for improving the performance
of text-dependent speaker verification (TD-SV). However, a moderate success has been achieved.
A recent study [1] presented a time contrastive learning (TCL) concept to explore the non-stationarity
of brain signals for classification of brain states. Speech signals have similar non-stationarity
property, and TCL further has the advantage of having no need for labeled data. We therefore present
a TCL based BN feature extraction method. The method uniformly partitions each speech utterance
in a training dataset into a predefined number of multi-frame segments. Each segment in an utterance
corresponds to one class, and class labels are shared across utterances. DNNs are then trained to
discriminate all speech frames among the classes to exploit the temporal structure of speech. In
addition, we propose a segment-based unsupervised clustering algorithm to re-assign class labels
to the segments. TD-SV experiments were conducted on the RedDots challenge database. The TCL-DNNs
were trained using speech data of fixed pass-phrases that were excluded from the TD-SV evaluation
set, so the learned features can be considered phrase-independent. We compare the performance
of the proposed TCL bottleneck (BN) feature with those of short-time cepstral features and BN features
extracted from DNNs discriminating speakers, pass-phrases, speaker+pass-phrase, as well as
monophones whose labels and boundaries are generated by three different automatic speech recognition
(ASR) systems. Experimental results show that the proposed TCL-BN outperforms cepstral features
and speaker+pass-phrase discriminant BN features, and its performance is on par with those of ASR
derived BN features. Moreover,.... 