We present a novel approach to learning a point-wise, meaningful embedding for point-clouds in
an unsupervised manner, through the use of neural-networks. The domain of point-cloud processing
via neural-networks is rapidly evolving, with novel architectures and applications frequently
emerging. Within this field of research, the availability and plethora of unlabeled point-clouds
as well as their possible applications make finding ways of characterizing this type of data appealing.
Though significant advancement was achieved in the realm of unsupervised learning, its adaptation
to the point-cloud representation is not trivial. Previous research focuses on the embedding of
entire point-clouds representing an object in a meaningful manner. We present a deep learning framework
to learn point-wise description from a set of shapes without supervision. Our approach leverages
self-supervision to define a relevant loss function to learn rich per-point features. We train
a neural-network with objectives based on context derived directly from the raw data, with no added
annotation. We use local structures of point-clouds to incorporate geometric information into
each point's latent representation. In addition to using local geometric information, we encourage
adjacent points to have similar representations and vice-versa, creating a smoother, more descriptive
representation. We demonstrate the ability of our method to capture meaningful point-wise features
through three applications. By clustering the learned embedding space, we perform unsupervised
part-segmentation on point clouds. By calculating euclidean distance in the latent space we derive
semantic point-analogies. Finally, by retrieving nearest-neighbors in our learned latent space
we present meaningful point-correspondence within and among point-clouds. 