Semantic segmentation in high resolution remote sensing images is a fundamental and challenging
task. Convolutional neural networks (CNNs), such as fully convolutional network (FCN) and SegNet,
have shown outstanding performance in many segmentation tasks. One key pillar of these successes
is mining useful information from features in convolutional layers for producing high resolution
segmentation maps. For example, FCN nonlinearly combines high-level features extracted from
last convolutional layers; whereas SegNet utilizes a deconvolutional network which takes as input
only coarse, high-level feature maps of the last convolutional layer. However, how to better fuse
multi-level convolutional feature maps for semantic segmentation of remote sensing images is
underexplored. In this work, we propose a novel bidirectional network called recurrent network
in fully convolutional network (RiFCN), which is end-to-end trainable. It has a forward stream
and a backward stream. The former is a classification CNN architecture for feature extraction,
which takes an input image and produces multi-level convolutional feature maps from shallow to
deep; while in the later, to achieve accurate boundary inference and semantic segmentation, boundary-aware
high resolution feature maps in shallower layers and high-level but low-resolution features are
recursively embedded into the learning framework (from deep to shallow) to generate a fused feature
representation that draws a holistic picture of not only high-level semantic information but also
low-level fine-grained details. Experimental results on two widely-used high resolution remote
sensing data sets for semantic segmentation tasks, ISPRS Potsdam and Inria Aerial Image Labeling
Data Set, demonstrate competitive performance obtained by the proposed methodology compared
to other studied approaches. 