Machine Learning techniques have become pervasive across a range of different applications, and
are now widely used in areas as disparate as recidivism prediction, consumer credit-risk analysis
and insurance pricing. The prevalence of machine learning techniques has raised concerns about
the potential for learned algorithms to become biased against certain groups. Many definitions
have been proposed in the literature, but the fundamental task of reasoning about probabilistic
events is a challenging one, owing to the intractability of inference. The focus of this paper is
taking steps towards the application of tractable models to fairness. Tractable probabilistic
models have emerged that guarantee that conditional marginal can be computed in time linear in the
size of the model. In particular, we show that sum product networks (SPNs) enable an effective technique
for determining the statistical relationships between protected attributes and other training
variables. If a subset of these training variables are found by the SPN to be independent of the training
attribute then they can be considered `safe' variables, from which we can train a classification
model without concern that the resulting classifier will result in disparate outcomes for different
demographic groups. Our initial experiments on the `German Credit' data set indicate that this
processing technique significantly reduces disparate treatment of male and female credit applicants,
with a small reduction in classification accuracy compared to state of the art. We will also motivate
the concept of "fairness through percentile equivalence", a new definition predicated on the notion
that individuals at the same percentile of their respective distributions should be treated equivalently,
and this prevents unfair penalisation of those individuals who lie at the extremities of their respective
distributions. 