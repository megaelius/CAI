Specialized hardware architectures promise a major step in performance and energy efficiency
over the traditional load/store devices currently employed in large scale computing systems.
The adoption of high-level synthesis (HLS) from languages such as C/C++ and OpenCL has greatly increased
programmer productivity when designing for such platforms. While this has enabled a wider audience
to target specialized hardware, the optimization principles known from traditional software
design are no longer sufficient to implement high-performance codes. Fast and efficient codes
for reconfigurable platforms are thus still challenging to design. To alleviate this, we present
a set of optimizing transformations for HLS, targeting scalable and efficient architectures for
high-performance computing (HPC) applications. Our work provides a toolbox for developers, where
we systematically identify classes of transformations, the characteristics of their effect on
the HLS code and the resulting hardware (e.g., increases data reuse or resource consumption), and
the objectives that each transformation can target (e.g., resolve interface contention, or increase
parallelism). We show how these can be used to efficiently exploit pipelining, on-chip distributed
fast memory, and on-chip streaming dataflow, allowing for massively parallel architectures.
To quantify the effect of our transformations, we use them to optimize a set of throughput-oriented
FPGA kernels, demonstrating that our enhancements are sufficient to scale up parallelism within
the hardware constraints. With the transformations covered, we hope to establish a common framework
for performance engineers, compiler developers, and hardware developers, to tap into the performance
potential offered by specialized hardware architectures using HLS. 