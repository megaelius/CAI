Deep neural networks (DNNs), as the basis of object detection, will play a key role in the development
of future autonomous systems with full autonomy. The autonomous systems have special requirements
of real-time, energy-efficient implementations of DNNs on a power-constrained system. Two research
thrusts are dedicated to performance and energy efficiency enhancement of the inference phase
of DNNs. The first one is model compression techniques while the second is efficient hardware implementation.
Recent works on extremely-low-bit CNNs such as the binary neural network (BNN) and XNOR-Net replace
the traditional floating-point operations with binary bit operations which significantly reduces
the memory bandwidth and storage requirement. However, it suffers from non-negligible accuracy
loss and underutilized digital signal processing (DSP) blocks of FPGAs. To overcome these limitations,
this paper proposes REQ-YOLO, a resource-aware, systematic weight quantization framework for
object detection, considering both algorithm and hardware resource aspects in object detection.
We adopt the block-circulant matrix method and propose a heterogeneous weight quantization using
the Alternating Direction Method of Multipliers (ADMM), an effective optimization technique
for general, non-convex optimization problems. To achieve real-time, highly-efficient implementations
on FPGA, we present the detailed hardware implementation of block circulant matrices on CONV layers
and develop an efficient processing element (PE) structure supporting the heterogeneous weight
quantization, CONV dataflow and pipelining techniques, design optimization, and a template-based
automatic synthesis framework to optimally exploit hardware resource. Experimental results
show that our proposed REQ-YOLO framework can significantly compress the YOLO model while introducing
very small accuracy degradation. 