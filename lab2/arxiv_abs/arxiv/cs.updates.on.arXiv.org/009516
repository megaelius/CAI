Disclaimer: This paper is concerned with violent online harassment. To describe the subject at
an adequate level of realism, examples of our collected tweets involve violent, threatening, vulgar
and hateful speech language in the context of racial, sexual, political, appearance and intellectual
harassment. The presence of a significant amount of harassment in user-generated content and its
negative impact calls for robust automatic detection approaches. This requires that we can identify
different forms or types of harassment. Earlier work has classified harassing language in terms
of hurtfulness, abusiveness, sentiment, and profanity. However, to identify and understand harassment
more accurately, it is essential to determine the context that represents the interrelated conditions
in which they occur. In this paper, we introduce the notion of contextual type to harassment involving
five categories: (i) sexual, (ii) racial, (iii) appearance-related, (iv) intellectual and (v)
political. We utilize an annotated corpus from Twitter distinguishing these types of harassment.
To study the context for each type that sheds light on the linguistic meaning, interpretation, and
distribution, we conduct two lines of investigation: an extensive linguistic analysis, and a statistical
distribution of unigrams. We then build type-ware classifiers to automate the identification
of type-specific harassment. Our experiments demonstrate that these classifiers provide competitive
accuracy for identifying and analyzing harassment on social media. We present extensive discussion
and major observations about the effectiveness of type-aware classifiers using a detailed comparison
setup providing insight into the role of type-dependent features. 