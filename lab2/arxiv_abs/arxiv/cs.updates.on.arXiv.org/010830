Graph representation learning, aiming to learn low-dimensional representations which capture
the geometric dependencies between nodes in the original graph, has gained increasing popularity
in a variety of graph analysis tasks, including node classification and link prediction. Existing
representation learning methods based on graph neural networks and their variants rely on the aggregation
of neighborhood information, which makes it sensitive to noises in the graph. In this paper, we propose
Graph Denoising Policy Network (short for GDPNet) to learn robust representations from noisy graph
data through reinforcement learning. GDPNet first selects signal neighborhoods for each node,
and then aggregates the information from the selected neighborhoods to learn node representations
for the down-stream tasks. Specifically, in the signal neighborhood selection phase, GDPNet optimizes
the neighborhood for each target node by formulating the process of removing noisy neighborhoods
as a Markov decision process and learning a policy with task-specific rewards received from the
representation learning phase. In the representation learning phase, GDPNet aggregates features
from signal neighbors to generate node representations for down-stream tasks, and provides task-specific
rewards to the signal neighbor selection phase. These two phases are jointly trained to select optimal
sets of neighbors for target nodes with maximum cumulative task-specific rewards, and to learn
robust representations for nodes. Experimental results on node classification task demonstrate
the effectiveness of GDNet, outperforming the state-of-the-art graph representation learning
methods on several well-studied datasets. Additionally, GDPNet is mathematically equivalent
to solving the submodular maximizing problem, which theoretically guarantees the best approximation
to the optimal solution with GDPNet. 