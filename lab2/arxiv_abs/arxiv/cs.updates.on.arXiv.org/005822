Recently, with convolutional neural networks gaining significant achievements in many challenging
machine learning fields, hand-crafted neural networks no longer satisfy our requirements as designing
a network will cost a lot, and automatically generating architectures has attracted increasingly
more attention and focus. Some research on auto-generated networks has achieved promising results.
However, they mainly aim at picking a series of single layers such as convolution or pooling layers
one by one. There are many elegant and creative designs in the carefully hand-crafted neural networks,
such as Inception-block in GoogLeNet, residual block in residual network and dense block in dense
convolutional network. Based on reinforcement learning and taking advantages of the superiority
of these networks, we propose a novel automatic process to design a multi-block neural network,
whose architecture contains multiple types of blocks mentioned above, with the purpose to do structure
learning of deep neural networks and explore the possibility whether different blocks can be composed
together to form a well-behaved neural network. The optimal network is created by the Q-learning
agent who is trained to sequentially pick different types of blocks. To verify the validity of our
proposed method, we use the auto-generated multi-block neural network to conduct experiments
on image benchmark datasets MNIST, SVHN and CIFAR-10 image classification task with restricted
computational resources. The results demonstrate that our method is very effective, achieving
comparable or better performance than hand-crafted networks and advanced auto-generated neural
networks. 