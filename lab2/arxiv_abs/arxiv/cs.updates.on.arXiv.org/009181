To combat the growing demands for efficient processing of large scale graph-structured datasets,
many distributed graph computing systems have been developed recently. As these systems require
many messages to be exchanged among computing machines at each step of the computation, 'communication
bandwidth' has been observed to be a major performance bottleneck. We present a 'coded computing'
framework that systematically injects redundancy in the computation phase to enable coding opportunities
in the communication phase; thus reducing the communication load substantially. Specifically,
we propose a coded scheme that enables an (asymptotically) inverse-linear trade-off between 'computation
load' and 'average communication load' for Erdos-Renyi graphs. The proposed scheme is shown to
be optimal asymptotically as the graph size increases. For finite-size graphs, we demonstrate
via numerical analysis that for a given computation load $r$, (i.e. when each graph vertex is carefully
stored at $r$ servers), the proposed scheme slashes the average communication load by (nearly)
a multiplicative factor of $r$. Furthermore, we generalize our results to three other random graph
models -- random bi-partite model, stochastic block model and power law model. In particular, we
prove that our schemes asymptotically enable an inverse-linear trade-off between computation
and communication loads in distributed graph processing for these popular graph models as well.
Additionally, we provide converses for bi-partite as well as stochastic block models. Furthermore,
we carry out experiments over Amazon EC2 clusters to practically demonstrate the impact of our coded
schemes, using artificial as well as real-world datasets, demonstrating gains of up to $50.8\%$
in comparison to the baseline approach. 