Due to urbanization and the increase of individual mobility, in most metropolitan areas around
the world congestion and inefficient traffic management occur. Highly necessary intelligent
traffic control systems, which are able to reduce congestion, rely on measurements of traffic situations
in urban road networks and freeways. Unfortunately, the instrumentation for accurate traffic
measurement is expensive and not widely implemented. This thesis addresses this problem, where
relatively inexpensive and easy to install loop-detectors are used by a geometric deep learning
algorithm, which uses loop-detector data in a spatial context of a road network, to estimate queue
length in front of signalized intersections, which can be then used for following traffic control
tasks. Therefore, in the first part of this work a conventional estimation method for queue length
(which does not use machine learning techniques) based on second-by-second loop-detector data
is implemented, which uses detected shockwaves in queues to estimate the length and point of time
for the maximum queue. The method is later used as reference but also as additional input information
for the geometric deep learning approach. In the second part the geometric deep learning algorithm
is developed, which uses spatial correlations in the road network but also temporal correlations
in detector data time sequences by new attention mechanisms, to overcome the limitations of conventional
methods like excess traffic demand, lane changing and stop-and-go traffic. Therefore, it is necessary
to abstract the topology of the road network in a graph. Both approaches are compared regarding their
performance, reliability as well as limitations and validated by usage of the traffic simulation
software SUMO (Simulation of Urban MObility). Finally, the results are discussed in the conclusions
and further investigations are suggested. 