Existing RGB-D salient object detection (SOD) models usually treat RGB and depth as independent
information and design separate networks for feature extraction from each. Such schemes can easily
be constrained by a limited amount of training data or over-reliance on an elaborately designed
training process. Inspired by the observation that RGB and depth modalities actually present certain
commonality in distinguishing salient objects, a novel joint learning and densely cooperative
fusion (JL-DCF) architecture is designed to learn from both RGB and depth inputs through a shared
network backbone, known as the Siamese architecture. In this paper, we propose two effective components:
joint learning (JL), and densely cooperative fusion (DCF). The JL module provides robust saliency
feature learning by exploiting cross-modal commonality via a Siamese network, while the DCF module
is introduced for complementary feature discovery. Comprehensive experiments using five popular
metrics show that the designed framework yields a robust RGB-D saliency detector with good generalization.
As a result, JL-DCF significantly advances the state-of-the-art models by an average of ~2.0% (F-measure)
across seven challenging datasets. In addition, we show that JL-DCF is readily applicable to other
related multi-modal detection tasks, including RGB-T (thermal infrared) SOD and video SOD (VSOD),
achieving comparable or even better performance against state-of-the-art methods. This further
confirms that the proposed framework could offer a potential solution for various applications
and provide more insight into the cross-modal complementarity task. The code will be available
at https://github.com/kerenfu/JLDCF/ 