Despite the recent success of deep neural networks in natural language processing (NLP), their
interpretability remains a challenge. We analyze the representations learned by neural machine
translation models at various levels of granularity and evaluate their quality through relevant
extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately
is word-structure captured within the learned representations, an important aspect in translating
morphologically-rich languages? (ii) Do the representations capture long-range dependencies,
and effectively handle syntactically divergent languages? (iii) Do the representations capture
lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers
in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation
unit (word, character, or subword unit) impact the linguistic properties captured by the underlying
representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do
the representations learned by multilingual NMT models capture the same amount of linguistic information
as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important
aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep
NMT models learn a non-trivial amount of linguistic information. Notable findings include: i)
Word morphology and part-of-speech information are captured at the lower layers of the model; (ii)
In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented
at the higher layers; (iii) Representations learned using characters are more informed about wordmorphology
compared to those learned using subword units; and (iv) Representations learned by multilingual
models are richer compared to bilingual models. 