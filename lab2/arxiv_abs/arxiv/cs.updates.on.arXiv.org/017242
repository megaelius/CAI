Modern day engineering problems are ubiquitously characterized by sophisticated computer codes
that map parameters or inputs to an underlying physical process. In other situations, experimental
setups are used to model the physical process in a laboratory, ensuring high precision while being
costly in materials and logistics. In both scenarios, only limited amount of data can be generated
by querying the expensive information source at a finite number of inputs or designs. This problem
is compounded further in the presence of a high-dimensional input space. State-of-the-art parameter
space dimension reduction methods, such as active subspace, aim to identify a subspace of the original
input space that is sufficient to explain the output response. These methods are restricted by their
reliance on gradient evaluations or copious data, making them inadequate to expensive problems
without direct access to gradients. The proposed methodology is gradient-free and fully Bayesian,
as it quantifies uncertainty in both the low-dimensional subspace and the surrogate model parameters.
This enables a full quantification of epistemic uncertainty and robustness to limited data availability.
It is validated on multiple datasets from engineering and science and compared to two other state-of-the-art
methods based on four aspects: a) recovery of the active subspace, b) deterministic prediction
accuracy, c) probabilistic prediction accuracy, and d) training time. The comparison shows that
the proposed method improves the active subspace recovery and predictive accuracy, in both the
deterministic and probabilistic sense, when only few model observations are available for training,
at the cost of increased training time. 