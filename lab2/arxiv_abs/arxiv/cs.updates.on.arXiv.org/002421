With more and more household objects built on planned obsolescence and consumed by a fast-growing
population, hazardous waste recycling has become a critical challenge. Given the large variability
of household waste, current recycling platforms mostly rely on human operators to analyze the scene,
typically composed of many object instances piled up in bulk. Helping them by robotizing the unitary
extraction is a key challenge to speed up this tedious process. Whereas supervised deep learning
has proven very efficient for such object-level scene understanding, e.g., generic object detection
and segmentation in everyday scenes, it however requires large sets of per-pixel labeled images,
that are hardly available for numerous application contexts, including industrial robotics.
We thus propose a step towards a practical interactive application for generating an object-oriented
robotic grasp, requiring as inputs only one depth map of the scene and one user click on the next object
to extract. More precisely, we address in this paper the middle issue of object seg-mentation in
top views of piles of bulk objects given a pixel location, namely seed, provided interactively by
a human operator. We propose a twofold framework for generating edge-driven instance segments.
First, we repurpose a state-of-the-art fully convolutional object contour detector for seed-based
instance segmentation by introducing the notion of edge-mask duality with a novel patch-free and
contour-oriented loss function. Second, we train one model using only synthetic scenes, instead
of manually labeled training data. Our experimental results show that considering edge-mask duality
for training an encoder-decoder network, as we suggest, outperforms a state-of-the-art patch-based
network in the present application context. 