Scientific applications are complex, large, and often exhibit irregular and stochastic behavior.
The use of efficient loop scheduling techniques in computationally-intensive applications is
crucial for improving their performance on high-performance computing (HPC) platforms. A number
of dynamic loop scheduling (DLS) techniques have been proposed between the late 1980s and early
2000s, and efficiently used in scientific applications. In most cases, the computing systems on
which they have been tested and validated are no longer available. This work is concerned with the
minimization of the sources of uncertainty in the implementation of DLS techniques to avoid unnecessary
influences on the performance of scientific applications. Therefore, it is important to ensure
that the DLS techniques employed in scientific applications today adhere to their original design
goals and specifications. The goal of this work is to attain and increase the trust in the implementation
of DLS techniques in present studies. To achieve this goal, the performance of a selection of scheduling
experiments from the 1992 original work that introduced factoring is reproduced and predicted
via both, simulative and native experimentation. The experiments show that the simulation reproduces
the performance achieved on the past computing platform and accurately predicts the performance
achieved on the present computing platform. The performance reproduction and prediction confirm
that the present implementation of the DLS techniques considered both, in simulation and natively,
adheres to their original description. The results confirm the hypothesis that reproducing experiments
of identical scheduling scenarios on past and modern hardware leads to an entirely different behavior
from expected. 