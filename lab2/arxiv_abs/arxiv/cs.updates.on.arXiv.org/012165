Deep learning-based computer vision is usually data-hungry. Many researchers attempt to augment
datasets with synthesized data to improve model robustness. However, the augmentation of popular
pedestrian datasets, such as Caltech and Citypersons, can be extremely challenging because real
pedestrians are commonly in low quality. Due to the factors like occlusions, blurs, and low-resolution,
it is significantly difficult for existing augmentation approaches, which generally synthesize
data using 3D engines or generative adversarial networks (GANs), to generate realistic-looking
pedestrians. Alternatively, to access much more natural-looking pedestrians, we propose to augment
pedestrian detection datasets by transforming real pedestrians from the same dataset into different
shapes. Accordingly, we propose the Shape Transformation-based Dataset Augmentation (STDA)
framework. The proposed framework is composed of two subsequent modules, i.e. the shape-guided
deformation and the environment adaptation. In the first module, we introduce a shape-guided warping
field to help deform the shape of a real pedestrian into a different shape. Then, in the second stage,
we propose an environment-aware blending map to better adapt the deformed pedestrians into surrounding
environments, obtaining more realistic-looking pedestrians and more beneficial augmentation
results for pedestrian detection. Extensive empirical studies on different pedestrian detection
benchmarks show that the proposed STDA framework consistently produces much better augmentation
results than other pedestrian synthesis approaches using low-quality pedestrians. By augmenting
the original datasets, our proposed framework also improves the baseline pedestrian detector
by up to 38% on the evaluated benchmarks, achieving state-of-the-art performance. 