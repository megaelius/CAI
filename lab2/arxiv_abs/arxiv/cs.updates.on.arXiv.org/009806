Background: Unsupervised machine learners have been increasingly applied to software defect
prediction. It is an approach that may be valuable for software practitioners because it reduces
the need for labeled training data. Objective: Investigate the use and performance of unsupervised
learning techniques in software defect prediction. Method: We conducted a systematic literature
review that identified 48 studies containing 2348 individual experimental results, which satisfied
our inclusion criteria published between January 2000 and March 2018. In order to compare prediction
performance across these studies in a consistent way, we (re-)computed the confusion matrices
and employed Matthew's correlation coefficient (MCC) as our main performance measure. Results:
Our meta-analysis shows that unsupervised models are comparable with supervised models for both
within-project and cross-project prediction. Among 21 unsupervised models, Fuzzy CMeans (FCM)
and Fuzzy SOMs (FSOMs) perform best. In addition, where we were able to check, we found that almost
11% (262/2348) of published results (contained in 16 papers) were internally inconsistent and
a further 30% (705/2348) provided insufficient details for us to check. Conclusion: Although many
factors impact the performance of a classifier, e.g., dataset characteristics, broadly speaking,
unsupervised classifiers do not seem to perform worse than the supervised classifiers in our review.
However, we note a worrying prevalence of (i) demonstrably erroneous experimental results, (ii)
undemanding benchmarks and (iii) incomplete reporting. We particularly encourage researchers
to be comprehensive in their reporting. 