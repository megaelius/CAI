Choosing an encoding over binary strings for input/output to/by a Turing Machine is usually straightforward
and/or inessential for discrete data (like graphs), but delicate -- heavily affecting computability
and even more computational complexity -- already regarding real numbers, not to mention more advanced
(e.g. Sobolev) spaces. For a general theory of computational complexity over continuous data we
introduce and justify QUANTITATIVE admissibility as requirement for sensible encodings of arbitrary
compact metric spaces, a refinement of qualitative 'admissibility' due to [Kreitz&Weihrauch'85]:
An admissible representation of a T0 space $X$ is a (i) continuous partial surjective mapping from
the Cantor space of infinite binary sequences which is (ii) maximal w.r.t. continuous reduction.
By the Kreitz-Weihrauch (aka "Main") Theorem of computability over continuous data, for fixed
spaces $X,Y$ equipped with admissible representations, a function $f:X\to Y$ is continuous iff
it admits continuous a code-translating mapping on Cantor space, a so-called REALIZER. We define
a QUANTITATIVELY admissible representation of a compact metric space $X$ to have (i) asymptotically
optimal modulus of continuity, namely close to the entropy of $X$, and (ii) be maximal w.r.t. reduction
having optimal modulus of continuity in a similar sense. Careful constructions show the category
of such representations to be Cartesian closed, and non-empty: every compact $X$ admits a linearly-admissible
representation. Moreover such representations give rise to a tight quantitative correspondence
between the modulus of continuity of a function $f:X\to Y$ on the one hand and on the other hand that
of its realizer: the MAIN THEOREM of computational complexity. This suggests (how) to take into
account the entropies of the spaces under consideration when measuring algorithmic cost over continuous
data. 