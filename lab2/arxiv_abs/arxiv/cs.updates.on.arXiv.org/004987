We propose a novel attention based hierarchical LSTM model to classify discourse act sequences
in social media conversations, aimed at mining data from online discussion using textual meanings
beyond sentence level. The very uniqueness of the task is the complete categorization of possible
pragmatic roles in informal textual discussions, contrary to extraction of question-answers,
stance detection or sarcasm identification which are very much role specific tasks. Early attempt
was made on a Reddit discussion dataset. We train our model on the same data, and present test results
on two different datasets, one from Reddit and one from Facebook. Our proposed model outperformed
the previous one in terms of domain independence; without using platform-dependent structural
features, our hierarchical LSTM with word relevance attention mechanism achieved F1-scores of
71\% and 66\% respectively to predict discourse roles of comments in Reddit and Facebook discussions.
Efficiency of recurrent and convolutional architectures in order to learn discursive representation
on the same task has been presented and analyzed, with different word and comment embedding schemes.
Our attention mechanism enables us to inquire into relevance ordering of text segments according
to their roles in discourse. We present a human annotator experiment to unveil important observations
about modeling and data annotation. Equipped with our text-based discourse identification model,
we inquire into how heterogeneous non-textual features like location, time, leaning of information
etc. play their roles in charaterizing online discussions on Facebook. 