Offline Signature Verification (OSV) is a challenging pattern recognition task, especially in
presence of skilled forgeries that are not available during training. This study aims to tackle
its challenges and meet the substantial need for generalization for OSV by examining different
loss functions for Convolutional Neural Network (CNN). We adopt our new approach to OSV by asking
two questions: 1. which classification loss provides more generalization for feature learning
in OSV? , and 2. How integration of different losses into a unified multi-loss function lead to an
improved learning framework? These questions are studied based on analysis of three loss functions,
including cross entropy, Cauchy-Schwarz divergence, and hinge loss. According to complementary
features of these losses, we combine them into a dynamic multi-loss function and propose a novel
ensemble framework for simultaneous use of them in CNN. Our proposed Multi-Loss Snapshot Ensemble
(MLSE) consists of several sequential trials. In each trial, a dominant loss function is selected
from the multi-loss set, and the remaining losses act as a regularizer. Different trials learn diverse
representations for each input based on signature identification task. This multi-representation
set is then employed for the verification task. An ensemble of SVMs is trained on these representations,
and their decisions are finally combined according to the selection of most generalizable SVM for
each user. We conducted two sets of experiments based on two different protocols of OSV, i.e., writer-dependent
and writer-independent on three signature datasets: GPDS-Synthetic, MCYT, and UT-SIG. Based
on the writer-dependent OSV protocol, we achieved substantial improvements over the best EERs
in the literature. The results of the second set of experiments also confirmed the robustness to
the arrival of new users enrolled in the OSV system. 