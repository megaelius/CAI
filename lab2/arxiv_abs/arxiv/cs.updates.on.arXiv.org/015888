Recent years have witnessed an explosion of user-generated content (UGC) videos shared and streamed
over the Internet, thanks to the evolution of affordable and reliable consumer capture devices,
and the tremendous popularity of social media platforms. Accordingly, there is a great need for
accurate video quality assessment (VQA) models for UGC/consumer videos to monitor, control, and
optimize this vast content. Blind quality prediction of in-the-wild videos is quite challenging,
since the quality degradations of UGC content are unpredictable, complicated, and often commingled.
Here we contribute to advancing the UGC-VQA problem by conducting a comprehensive evaluation of
leading no-reference/blind VQA (BVQA) features and models on a fixed evaluation architecture,
yielding new empirical insights on both subjective video quality studies and VQA model design.
By employing a feature selection strategy on top of leading VQA model features, we are able to extract
60 of the 763 statistical features used by the leading models to create a new fusion-based BVQA model,
which we dub the \textbf{VID}eo quality \textbf{EVAL}uator (VIDEVAL), that effectively balances
the trade-off between VQA performance and efficiency. Our experimental results show that VIDEVAL
achieves state-of-the-art performance at considerably lower computational cost than other leading
models. Our study protocol also defines a reliable benchmark for the UGC-VQA problem, which we believe
will facilitate further research on deep learning-based VQA modeling, as well as perceptually-optimized
efficient UGC video processing, transcoding, and streaming. To promote reproducible research
and public evaluation, an implementation of VIDEVAL has been made available online: \url{https://github.com/tu184044109/VIDEVAL_release}.
