Controllable image-to-image translation, i.e., transferring an image from a source domain to
a target one guided by controllable structures, has attracted much attention in both academia and
industry. In this paper, we propose a unified Generative Adversarial Network (GAN) framework for
controllable image-to-image translation. In addition to conditioning on a reference image, we
show how the model can generate images conditioned on controllable structures, e.g., class labels,
object keypoints, human skeletons and scene semantic maps. The proposed GAN framework consists
of a single generator and a discriminator taking a conditional image and the target controllable
structure as input. In this way, the conditional image can provide appearance information and the
controllable structure can provide the structure information for generating the target result.
Moreover, the proposed GAN learns the image-to-image mapping through three novel losses, i.e.,
color loss, controllable structure-guided cycle-consistency loss and controllable structure-guided
self-identity preserving loss. Note that the proposed color loss handles the issue of "channel
pollution" when back-propagating the gradients. In addition, we present the Fr\'echet ResNet
Distance (FRD) to evaluate the quality of generated images. Extensive qualitative and quantitative
experiments on two challenging image translation tasks with four different datasets demonstrate
that the proposed GAN model generates convincing results, and significantly outperforms other
state-of-the-art methods on both tasks. Meanwhile, the proposed GAN framework is a unified solution,
thus it can be applied to solving other controllable structure-guided image-to-image translation
tasks, such as landmark-guided facial expression translation and keypoint-guided person image
generation. 