Graph representation learning (GRL) is a powerful technique for learning low-dimensional vector
representation of high-dimensional and often sparse graphs. Most studies explore the structure
and metadata associated with the graph using random walks and employ an unsupervised or semi-supervised
learning schemes. Learning in these methods is context-free, resulting in only a single representation
per node. Recently studies have argued on the adequacy of a single representation and proposed context-sensitive
approaches, which are capable of extracting multiple node representations for different contexts.
This proved to be highly effective in applications such as link prediction and ranking. However,
most of these methods rely on additional textual features that require complex and expensive RNNs
or CNNs to capture high-level features or rely on a community detection algorithm to identify multiple
contexts of a node. In this study we show that in-order to extract high-quality context-sensitive
node representations it is not needed to rely on supplementary node features, nor to employ computationally
heavy and complex models. We propose GOAT, a context-sensitive algorithm inspired by gossip communication
and a mutual attention mechanism simply over the structure of the graph. We show the efficacy of GOAT
using 6 real-world datasets on link prediction and node clustering tasks and compare it against
12 popular and state-of-the-art (SOTA) baselines. GOAT consistently outperforms them and achieves
up to 12% and 19% gain over the best performing methods on link prediction and clustering tasks, respectively.
