There has been growing interest, both theoretical and practical, in utilizing tensor networks
(TNs) for the analysis and design of machine learning systems. TNs have been shown to be able to handle
both dense data (e.g., standard regression or classification tasks) and sparse data (e.g., recommender
systems and one-hot encoded categorical features), unlike support vector machines and traditional
deep learning techniques. Such a tensor-based framework can be interpreted as an application of
local feature mappings to the data, which, through the outer product operator, allows for modelling
all interactions of functions of the features; the corresponding weights are represented as a tensor
network for computational tractability. In this paper, we derive efficient prediction and learning
algorithms for supervised learning using the Canonical Polyadic (CP) decomposition, including
suitable regularization and initialization schemes. We empirically demonstrate that the CP-based
model performs at least on par with the existing models based on the Tensor Train (TT) decomposition
on standard non-sequential tasks, while it outperforms TT-based models on MovieLens 100K dataset.
Furthermore, in contrast to previous works which apply two-dimensional local feature maps to the
data, we generalize the framework to handle arbitrarily high-dimensional maps, which equips the
tensor-based models with enhanced expressiveness. We also propose a normalized version of the
feature maps to address the stability and generalization capabilities. Our experiments show that
this leads to dramatic improvements over the existing unnormalized and/or two-dimensional maps,
as well as to a level of performance on non-sequential supervised learning tasks which is comparable
with other machine learning models, including fully-connected neural networks. 