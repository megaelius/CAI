This paper presents a Deep convolutional network model for Identity-Aware Transfer (DIAT) of facial
attributes. Given the source input image and the reference attribute, DIAT aims to generate a facial
image that owns the reference attribute as well as keeps the same or similar identity to the input
image. In general, our model consists of a mask network and an attribute transform network which
work in synergy to generate a photo-realistic facial image with the reference attribute. Considering
that the reference attribute may be only related to some parts of the image, the mask network is introduced
to avoid the incorrect editing on attribute irrelevant region. Then the estimated mask is adopted
to combine the input and transformed image for producing the transfer result. For joint training
of transform network and mask network, we incorporate the adversarial attribute loss, identity-aware
adaptive perceptual loss, and VGG-FACE based identity loss. Furthermore, a denoising network
is presented to serve for perceptual regularization to suppress the artifacts in transfer result,
while an attribute ratio regularization is introduced to constrain the size of attribute relevant
region. Our DIAT can provide a unified solution for several representative facial attribute transfer
tasks, e.g., expression transfer, accessory removal, age progression, and gender transfer, and
can be extended for other face enhancement tasks such as face hallucination. The experimental results
validate the effectiveness of the proposed method. Even for the identity-related attribute (e.g.,
gender), our DIAT can obtain visually impressive results by changing the attribute while retaining
most identity-aware features. 