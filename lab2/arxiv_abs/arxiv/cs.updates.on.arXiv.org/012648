Patients increasingly turn to search engines and online content before, or in place of, talking
with a health professional. Low quality health information, which is common on the internet, presents
risks to the patient in the form of misinformation and a possibly poorer relationship with their
physician. To address this, the DISCERN criteria (developed at University of Oxford) are used to
evaluate the quality of online health information. However, patients are unlikely to take the time
to apply these criteria to the health websites they visit. We built an automated implementation
of the DISCERN instrument (Brief version) using machine learning models. We compared the performance
of a traditional model (Random Forest) with that of a hierarchical encoder attention-based neural
network (HEA) model using two language embeddings, BERT and BioBERT. The HEA BERT and BioBERT models
achieved average F1-macro scores across all criteria of 0.75 and 0.74, respectively, outperforming
the Random Forest model (average F1-macro = 0.69). Similarly, as measured by F-micro, HEA BERT and
BioBERT scored on average 0.80 and 0.81 vs. 0.76 for the Random Forest model. Overall, the neural
network based models achieved 81% and 86% average accuracy at 100% and 80% coverage, respectively,
compared to 94% manual rating accuracy. The attention mechanism implemented in the HEA architectures
provided 'model explainability' by identifying reasonable supporting sentences for the documents
fulfilling the Brief DISCERN criteria. Our research suggests that it is feasible to automate online
health information quality assessment, which is an important step towards empowering patients
to become informed partners in the healthcare process. 