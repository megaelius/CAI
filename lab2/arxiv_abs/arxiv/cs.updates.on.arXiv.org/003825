With emerging storage-class memory (SCM) nearing commercialization, there is evidence that it
will deliver the much-anticipated high density and access latencies within only a few factors of
DRAM. Nevertheless, the latency-sensitive nature of memory-resident services makes seamless
integration of SCM in servers questionable. In this paper, we ask the question of how best to introduce
SCM for such servers to improve overall performance/cost over existing DRAM-only architectures.
We first show that even with the most optimistic latency projections for SCM, the higher memory access
latency results in prohibitive performance degradation. However, we find that deployment of a
modestly sized high-bandwidth 3D stacked DRAM cache makes the performance of an SCM-mostly memory
system competitive. The high degree of spatial locality that memory-resident services exhibit
not only simplifies the DRAM cache's design as page-based, but also enables the amortization of
increased SCM access latencies and the mitigation of SCM's read/write latency disparity. We identify
the set of memory hierarchy design parameters that plays a key role in the performance and cost of
a memory system combining an SCM technology and a 3D stacked DRAM cache. We then introduce a methodology
to drive provisioning for each of these design parameters under a target performance/cost goal.
Finally, we use our methodology to derive concrete results for specific SCM technologies. With
PCM as a case study, we show that a two bits/cell technology hits the performance/cost sweet spot,
reducing the memory subsystem cost by 40% while keeping performance within 3% of the best performing
DRAM-only system, whereas single-level and triple-level cell organizations are impractical
for use as memory replacements. 