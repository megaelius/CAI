Hyper-heuristics are a novel tool. They deal with complex optimization problems where standalone
solvers exhibit varied performance. Among such a tool reside selection hyper-heuristics. By combining
the strengths of each solver, this kind of hyper-heuristic offers a more robust tool. However, their
effectiveness is highly dependent on the 'features' used to link them with the problem that is being
solved. Aiming at enhancing selection hyper-heuristics, in this paper we propose two types of transformation:
explicit and implicit. The first one directly changes the distribution of critical points within
the feature domain while using a Euclidean distance to measure proximity. The second one operates
indirectly by preserving the distribution of critical points but changing the distance metric
through a kernel function. We focus on analyzing the effect of each kind of transformation, and of
their combinations. We test our ideas in the domain of constraint satisfaction problems because
of their popularity and many practical applications. In this work, we compare the performance of
our proposals against those of previously published data. Furthermore, we expand on previous research
by increasing the number of analyzed features. We found that, by incorporating transformations
into the model of selection hyper-heuristics, overall performance can be improved, yielding more
stable results. However, combining implicit and explicit transformations was not as fruitful.
Additionally, we ran some confirmatory tests on the domain of knapsack problems. Again, we observed
improved stability, leading to the generation of hyper-heuristics whose profit had a standard
deviation between 20% and 30% smaller. 