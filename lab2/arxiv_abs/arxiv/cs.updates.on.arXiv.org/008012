In the beautifully simple-to-state problem of trace reconstruction, the goal is to reconstruct
an unknown binary string $x$ given random "traces" of $x$ where each trace is generated by deleting
each coordinate of $x$ independently with probability $p<1$. The problem is well studied both when
the unknown string is arbitrary and when it is chosen uniformly at random. For both settings, there
is still an exponential gap between upper and lower sample complexity bounds and our understanding
of the problem is still surprisingly limited. In this paper, we consider natural parameterizations
and generalizations of this problem in an effort to attain a deeper and more comprehensive understanding.
We prove that $\exp(O(n^{1/4} \sqrt{\log n}))$ traces suffice for reconstructing arbitrary matrices.
In the matrix version of the problem, each row and column of an unknown $\sqrt{n}\times \sqrt{n}$
matrix is deleted independently with probability $p$. Our results contrasts with the best known
results for sequence reconstruction where the best known upper bound is $\exp(O(n^{1/3}))$. An
optimal result for random matrix reconstruction: we show that $\Theta(\log n)$ traces are necessary
and sufficient. This is in contrast to the problem for random sequences where there is a super-logarithmic
lower bound and the best known upper bound is $\exp({O}(\log^{1/3} n))$. We show that $\exp(O(k^{1/3}\log^{2/3}
n))$ traces suffice to reconstruct $k$-sparse strings, providing an improvement over the best
known sequence reconstruction results when $k = o(n/\log^2 n)$. We show that $\textrm{poly}(n)$
traces suffice if $x$ is $k$-sparse and we additionally have a "separation" promise, specifically
that the indices of 1's in $x$ all differ by $\Omega(k \log n)$. 