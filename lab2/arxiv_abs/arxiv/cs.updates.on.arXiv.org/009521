Sound event detection (SED) is to recognize the presence of sound events in the segment of audio and
detect their onset as well as offset. SED can be regarded as a supervised learning task when strong
annotations (timestamps) are available during learning. However, due to the high cost of manual
strong labeling data, it becomes crucial to introduce weakly supervised learning to SED, in which
only weak annotations (clip-level annotations without timestamps) are available during learning.
In this paper, we approach SED as a multiple instance learning (MIL) problem and utilize a neural
network framework with an embedding-level pooling module to solve it. The pooling module, which
aggregates a sequence of high-level features generated by the neural network feature encoder into
a single contextual feature representation, enables the model to learn with only weak annotations.
We explore the self-learning ability of different pooling modules on finer information and propose
a specialized decision surface (SDS) for class-wise attention pooling (cATP) module. We analyze
and explained why a cATP module with SDS is better than other typical pooling modules from the perspective
of feature space. According to the co-occurrence of several categories in the multi-label classification
task, we also propose a disentangled feature (DF) to reduce interference between categories, which
optimizes the high-level feature space by disentangling it based on class-wise identifiable information
in the training set and obtaining multiple different subspaces. Experiments show that our approach
achieves state-of-art performance on Task4 of the DCASE2018 challenge. 