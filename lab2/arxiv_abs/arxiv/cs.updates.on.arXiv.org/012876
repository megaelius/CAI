There are several benefits from learning disentangled representations, including interpretability,
compositionality and generalisation to new tasks. Disentanglement could be done by imposing an
inductive bias based on prior knowledge. Different priors make different structural assumptions
about the representations. For instance, priors with granularity can lead to representation to
describe data at different scales. A notable example is in the visual domain where there are multiple
scales of the variation in the data. Hence, learning representation at different scales will be
useful for different tasks. In this work, we propose a framework, called SPLIT, which allows us to
disentangle local and global information into two separate sets of latent variables within the
variational autoencoder (VAE) framework. Our framework adds an extra generative assumption to
the VAE by requiring a subset of the latent variables to generate an auxiliary set of observable data.
This set of data, which contains only local information, can be obtained via a transformation of
the original data that removes global information. Three different flavours of VAE with different
generative assumptions were examined in our experiments. We show that the framework can be effectively
used to disentangle local and global information within these models. Benefits of the framework
are demonstrated through multiple downstream representation learning problems. The framework
can unlock the potential of these VAE models in the tasks of style transfer, deep clustering and unsupervised
object detection with a simple modification to existing VAE models. Finally, we review cognitive
neuroscience literature regarding disentanglement in human visual perception. The code for our
experiments can be found at https://github.com/51616/split-vae. 