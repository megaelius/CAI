There is much debate in machine ethics about the most appropriate way to introduce ethical reasoning
capabilities into intelligent autonomous machines. Recent incidents involving autonomous vehicles
in which humans have been killed or injured have raised questions about how we ensure that such vehicles
have an ethical dimension to their behaviour and are therefore trustworthy. The main problem is
that hardwiring such machines with rules not to cause harm or damage is not consistent with the notion
of autonomy and intelligence. Also, such ethical hardwiring does not leave intelligent autonomous
machines with any course of action if they encounter situations or dilemmas for which they are not
programmed or where some harm is caused no matter what course of action is taken. Teaching machines
so that they learn ethics may also be problematic given recent findings in machine learning that
machines pick up the prejudices and biases embedded in their learning algorithms or data. This paper
describes a fuzzy reasoning approach to machine ethics. The paper shows how it is possible for an
ethics architecture to reason when taking over from a human driver is morally justified. The design
behind such an ethical reasoner is also applied to an ethical dilemma resolution case. One major
advantage of the approach is that the ethical reasoner can generate its own data for learning moral
rules (hence, autometric) and thereby reduce the possibility of picking up human biases and prejudices.
The results show that a new type of metric-based ethics appropriate for autonomous intelligent
machines is feasible and that our current concept of ethical reasoning being largely qualitative
in nature may need revising if want to construct future autonomous machines that have an ethical
dimension to their reasoning so that they become moral machines. 