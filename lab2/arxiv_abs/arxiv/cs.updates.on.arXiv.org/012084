The computing wall and data movement challenges of deep neural networks (DNNs) have exposed the
limitations of conventional CMOS-based DNN accelerators. Furthermore, the deep structure and
large model size will make DNNs prohibitive to embedded systems and IoT devices, where low power
consumption are required. To address these challenges, spin orbit torque magnetic random-access
memory (SOT-MRAM) and SOT-MRAM based Processing-In-Memory (PIM) engines have been used to reduce
the power consumption of DNNs since SOT-MRAM has the characteristic of near-zero standby power,
high density, none-volatile. However, the drawbacks of SOT-MRAM based PIM engines such as high
writing latency and requiring low bit-width data decrease its popularity as a favorable energy
efficient DNN accelerator. To mitigate these drawbacks, we propose an ultra energy efficient framework
by using model compression techniques including weight pruning and quantization from the software
level considering the architecture of SOT-MRAM PIM. And we incorporate the alternating direction
method of multipliers (ADMM) into the training phase to further guarantee the solution feasibility
and satisfy SOT-MRAM hardware constraints. Thus, the footprint and power consumption of SOT-MRAM
PIM can be reduced, while increasing the overall system throughput at the meantime, making our proposed
ADMM-based SOT-MRAM PIM more energy efficiency and suitable for embedded systems or IoT devices.
Our experimental results show the accuracy and compression rate of our proposed framework is consistently
outperforming the reference works, while the efficiency (area \& power) and throughput of SOT-MRAM
PIM engine is significantly improved. 