In recent years, deep learning algorithms have become increasingly more prominent for their unparalleled
ability to automatically learn discriminant features from large amounts of data. However, within
the field of electromyography-based gesture recognition, deep learning algorithms are seldom
employed as they require an unreasonable amount of effort from a single person, to generate tens
of thousands of examples. This work's hypothesis is that general, informative features can be learned
from the large amounts of data generated by aggregating the signals of multiple users, thus reducing
the recording burden while enhancing gesture recognition. Consequently, this paper proposes
applying transfer learning on aggregated data from multiple users, while leveraging the capacity
of deep learning algorithms to learn discriminant features from large datasets. Two datasets comprised
of 19 and 17 able-bodied participants respectively (the first one is employed for pre-training)
were recorded for this work, using the Myo Armband. A third Myo Armband dataset was taken from the
NinaPro database and is comprised of 10 able-bodied participants. Three different deep learning
networks employing three different modalities as input (raw EMG, Spectrograms and Continuous
Wavelet Transform (CWT)) are tested on the second and third dataset. The proposed transfer learning
scheme is shown to systematically and significantly enhance the performance for all three networks
on the two datasets, achieving an offline accuracy of 98.31% for 7 gestures over 17 participants
for the CWT-based ConvNet and 68.98% for 18 gestures over 10 participants for the raw EMG-based ConvNet.
Finally, a use-case study employing eight able-bodied participants suggests that real-time feedback
allows users to adapt their muscle activation strategy which reduces the degradation in accuracy
normally experienced over time. 