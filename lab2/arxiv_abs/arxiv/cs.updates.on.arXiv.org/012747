Natural muscles provide mobility in response to nerve impulses. Electromyography (EMG) measures
the electrical activity of muscles in response to a nerve's stimulation. In the past few decades,
EMG signals have been used extensively in the identification of user intention to potentially control
assistive devices such as smart wheelchairs, exoskeletons, and prosthetic devices. In the design
of conventional assistive devices, developers optimize multiple subsystems independently.
Feature extraction and feature description are essential subsystems of this approach. Therefore,
researchers proposed various hand-crafted features to interpret EMG signals. However, the performance
of conventional assistive devices is still unsatisfactory. In this paper, we propose a deep learning
approach to control prosthetic hands with raw EMG signals. We use a novel deep convolutional neural
network to eschew the feature-engineering step. Removing the feature extraction and feature description
is an important step toward the paradigm of end-to-end optimization. Fine-tuning and personalization
are additional advantages of our approach. The proposed approach is implemented in Python with
TensorFlow deep learning library, and it runs in real-time in general-purpose graphics processing
units of NVIDIA Jetson TX2 developer kit. Our results demonstrate the ability of our system to predict
fingers position from raw EMG signals. We anticipate our EMG-based control system to be a starting
point to design more sophisticated prosthetic hands. For example, a pressure measurement unit
can be added to transfer the perception of the environment to the user. Furthermore, our system can
be modified for other prosthetic devices. 