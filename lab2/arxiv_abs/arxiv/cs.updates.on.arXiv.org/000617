A low-rank transformation learning framework for subspace clustering and classification is here
proposed. Many high-dimensional data, such as face images and motion sequences, approximately
lie in a union of low-dimensional subspaces. The corresponding subspace clustering problem has
been extensively studied in the literature to partition such high-dimensional data into clusters
corresponding to their underlying low-dimensional subspaces. However, low-dimensional intrinsic
structures are often violated for real-world observations, as they can be corrupted by errors or
deviate from ideal models. We propose to address this by learning a linear transformation on subspaces
using matrix rank, via its convex surrogate nuclear norm, as the optimization criteria. The learned
linear transformation restores a low-rank structure for data from the same subspace, and, at the
same time, forces a a maximally separated structure for data from different subspaces. In this way,
we reduce variations within subspaces, and increase separation between subspaces for a more robust
subspace clustering. This proposed learned robust subspace clustering framework significantly
enhances the performance of existing subspace clustering methods. Basic theoretical results
here presented help to further support the underlying framework. To exploit the low-rank structures
of the transformed subspaces, we further introduce a fast subspace clustering technique, which
efficiently combines robust PCA with sparse modeling. When class labels are present at the training
stage, we show this low-rank transformation framework also significantly enhances classification
performance. Extensive experiments using public datasets are presented, showing that the proposed
approach significantly outperforms state-of-the-art methods for subspace clustering and classification.
