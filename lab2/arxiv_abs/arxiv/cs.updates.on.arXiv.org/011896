The main challenges of age estimation from facial expression videos lie not only in the modeling
of the static facial appearance, but also in the capturing of the temporal facial dynamics. Traditional
techniques to this problem focus on constructing handcrafted features to explore the discriminative
information contained in facial appearance and dynamics separately. This relies on sophisticated
feature-refinement and framework-design. In this paper, we present an end-to-end architecture
for age estimation, called Spatially-Indexed Attention Model (SIAM), which is able to simultaneously
learn both the appearance and dynamics of age from raw videos of facial expressions. Specifically,
we employ convolutional neural networks to extract effective latent appearance representations
and feed them into recurrent networks to model the temporal dynamics. More importantly, we propose
to leverage attention models for salience detection in both the spatial domain for each single image
and the temporal domain for the whole video as well. We design a specific spatially-indexed attention
mechanism among the convolutional layers to extract the salient facial regions in each individual
image, and a temporal attention layer to assign attention weights to each frame. This two-pronged
approach not only improves the performance by allowing the model to focus on informative frames
and facial areas, but it also offers an interpretable correspondence between the spatial facial
regions as well as temporal frames, and the task of age estimation. We demonstrate the strong performance
of our model in experiments on a large, gender-balanced database with 400 subjects with ages spanning
from 8 to 76 years. Experiments reveal that our model exhibits significant superiority over the
state-of-the-art methods given sufficient training data. 