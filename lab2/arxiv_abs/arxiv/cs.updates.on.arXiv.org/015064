Real-time apple detection in orchards is one of the most effective ways of estimating apple yields,
which helps in managing apple supplies more effectively. Traditional detection methods used highly
computational machine learning algorithms with intensive hardware set up, which are not suitable
for infield real-time apple detection due to their weight and power constraints. In this study,
a real-time embedded solution inspired from "Edge AI" is proposed for apple detection with the implementation
of YOLOv3-tiny algorithm on various embedded platforms such as Raspberry Pi 3 B+ in combination
with Intel Movidius Neural Computing Stick (NCS), Nvidia's Jetson Nano and Jetson AGX Xavier. Data
set for training were compiled using acquired images during field survey of apple orchard situated
in the north region of Italy, and images used for testing were taken from widely used google data set
by filtering out the images containing apples in different scenes to ensure the robustness of the
algorithm. The proposed study adapts YOLOv3-tiny architecture to detect small objects. It shows
the feasibility of deployment of the customized model on cheap and power-efficient embedded hardware
without compromising mean average detection accuracy (83.64%) and achieved frame rate up to 30
fps even for the difficult scenarios such as overlapping apples, complex background, less exposure
of apple due to leaves and branches. Furthermore, the proposed embedded solution can be deployed
on the unmanned ground vehicles to detect, count, and measure the size of the apples in real-time
to help the farmers and agronomists in their decision making and management skills. 