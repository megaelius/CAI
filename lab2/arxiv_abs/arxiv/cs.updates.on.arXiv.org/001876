(1) We will provide a straightforward methodology to express the nonlinearities as affine spline
functions. The linear part being a degenerated case of spline function, we can rewrite any given
DNN topology as succession of such functionals making the network itself a piecewise linear spline.
This formulation provides a universal piecewise linear expression of the input-output mapping
of DNNs, clarifying the role of its internal components. (2)For any given architecture, we provide
a measure of risk to adversarial attacks. (3) Recently, the deep learning community has focused
on the reminiscent theory of flat and sharp minima to provide generalization guarantees. We will
motivate a novel regularization technique pushing the learning of DNNs towards flat minima, maximizing
generalization performances. (4) From (1) we will reinterpret DNNs as template matching algorithms.
When coupled with insights derived from (2), we will integrate unlabeled data information into
the network during learning. To do so, we will propose to guide DNNs templates towards their input
via a scheme assimilated as a reconstruction formula for DNNs. This inversion can be computed efficiently
by back- propagation leading to no computational overhead. From this, any semi-supervised technique
can be used out-of-the-box with current DNNs where we provide state-of-the-art results. Unsupervised
tasks would also become reachable to DNNs, a task considered as the keystone of learning for the neuro-science
community. To date, those problematics have been studied independently leading to over-specialized
solutions generally topology specific and cumbersome to incorporate into a pre-existing pipeline.
On the other hand, all the proposed solutions necessitate negligible software updates, suited
for efficient large-scale deployment. 