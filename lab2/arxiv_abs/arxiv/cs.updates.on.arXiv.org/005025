Understanding the geometry and pose of objects in 2D images is a fundamental necessity for a wide
range of real world applications. Driven by deep neural networks, recent methods have brought significant
improvements to object pose estimation. However, they suffer due to scarcity of keypoint/pose-annotated
real images and hence can not exploit the object's 3D structural information effectively. In this
work, we propose a data-efficient method which utilizes the geometric regularity of intraclass
objects for pose estimation. First, we learn pose-invariant local descriptors of object parts
from simple 2D RGB images. These descriptors, along with keypoints obtained from renders of a fixed
3D template model are then used to generate keypoint correspondence maps for a given monocular real
image. Finally, a pose estimation network predicts 3D pose of the object using these correspondence
maps. This pipeline is further extended to a multi-view approach, which assimilates keypoint information
from correspondence sets generated from multiple views of the 3D template model. Fusion of multi-view
information significantly improves geometric comprehension of the system which in turn enhances
the pose estimation performance. Furthermore, use of correspondence framework responsible for
the learning of pose invariant keypoint descriptor also allows us to effectively alleviate the
data-scarcity problem. This enables our method to achieve state-of-the-art performance on multiple
real-image viewpoint estimation datasets, such as Pascal3D+ and ObjectNet3D. To encourage reproducible
research, we have released the codes for our proposed approach. 