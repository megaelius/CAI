Current benchmarks for optical flow algorithms evaluate the estimation either directly by comparing
the predicted flow fields with the ground truth or indirectly by using the predicted flow fields
for frame interpolation and then comparing the interpolated frames with the actual frames. In the
latter case, objective quality measures such as the mean squared error are typically employed.
However, it is well known that for image quality assessment, the actual quality experienced by the
user cannot be fully deduced from such simple measures. Hence, we conducted a subjective quality
assessment crowdscouring study for the interpolated frames provided by one of the optical flow
benchmarks, the Middlebury benchmark. We collected forced-choice paired comparisons between
interpolated images and corresponding ground truth. To increase the sensitivity of observers
when judging minute difference in paired comparisons we introduced a new method to the field of full-reference
quality assessment, called artefact amplification. From the crowdsourcing data, we reconstructed
absolute quality scale values according to Thurstone's model. As a result, we obtained a re-ranking
of the 155 participating algorithms w.r.t. the visual quality of the interpolated frames. This
re-ranking not only shows the necessity of visual quality assessment as another evaluation metric
for optical flow and frame interpolation benchmarks, the results also provide the ground truth
for designing novel image quality assessment (IQA) methods dedicated to perceptual quality of
interpolated images. As a first step, we proposed such a new full-reference method, called WAE-IQA.
By weighing the local differences between an interpolated image and its ground truth WAE-IQA performed
slightly better than the currently best FR-IQA approach from the literature. 