In the spectrum of vision-based autonomous driving, vanilla end-to-end models are not interpretable
and suboptimal in performance, while mediated perception models require additional intermediate
representations such as segmentation masks or detection bounding boxes, whose annotation can
be prohibitively expensive as we move to a larger scale. More critically, all prior works fail to
deal with the notorious domain shift if we were to merge data collected from different sources, which
greatly hinders the model generalization ability. In this work, we address the above limitations
by taking advantage of virtual data collected from driving simulators, and present DU-drive, an
unsupervised real-to-virtual domain unification framework for end-to-end autonomous driving.
It first transforms real driving data to its less complex counterpart in the virtual domain and then
predicts vehicle control commands from the generated virtual image. Our framework has three unique
advantages: 1) it maps driving data collected from a variety of source distributions into a unified
domain, effectively eliminating domain shift; 2) the learned virtual representation is simpler
than the input real image and closer in form to the "minimum sufficient statistic" for the prediction
task, which relieves the burden of the compression phase while optimizing the information bottleneck
tradeoff and leads to superior prediction performance; 3) it takes advantage of annotated virtual
data which is unlimited and free to obtain. Extensive experiments on two public driving datasets
and two driving simulators demonstrate the performance superiority and interpretive capability
of DU-drive. 