Deep learning (DL) has been applied extensively in many computational imaging problems, often
leading to superior performance over traditional iterative approaches. However, two important
questions remain largely unanswered: first, how well can the trained neural network generalize
to objects very different from the ones in training? This is particularly important in practice,
since large-scale annotated examples similar to those of interest are often not available during
training. Second, has the trained neural network learnt the underlying (inverse) physics model,
or has it merely done something trivial, such as memorizing the examples or point-wise pattern matching?
This pertains to the interpretability of machine-learning based algorithms. In this work, we use
the Phase Extraction Neural Network (PhENN), a deep neural network (DNN) for quantitative phase
retrieval in a lensless phase imaging system as the standard platform and show that the two questions
are related and share a common crux: the choice of the training examples. Moreover, we connect the
strength of the regularization effect imposed by a training set to the training process with the
Shannon entropy of images in the dataset. That is, the higher the entropy of the training images,
the weaker the regularization effect can be imposed. We also discover that weaker regularization
effect leads to better learning of the underlying propagation model, i.e. the weak object transfer
function, applicable for weakly scattering objects under the weak object approximation. Finally,
simulation and experimental results show that better cross-domain generalization performance
can be achieved if DNN is trained on a higher-entropy database, e.g. the ImageNet, than if the same
DNN is trained on a lower-entropy database, e.g. MNIST, as the former allows the underlying physics
model be learned better than the latter. 