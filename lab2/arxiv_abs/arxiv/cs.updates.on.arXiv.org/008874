Every minute, hundreds of hours of video are uploaded to social media sites and the Internet from
around the world. This material creates a visual record of the experiences of a significant percentage
of humanity and can help illuminate how we live in the present moment. When properly analyzed, this
video can also help analysts to reconstruct events of interest, including war crimes, human rights
violations, and terrorist acts. In this technical report, we describe a newly developed tool, the
Video Event Reconstruction and Analysis (VERA) system, that enables the localization of a shooter
from just a few videos that include the sound of gunshots using established machine learning techniques
and straightforward physics models. This tool relies on other tools we have already developed including
video synchronization and geolocation to order unstructured videos lacking metadata over time
and space, and sound detection algorithms. Both this gunshot localization tool and the previous
systems it incorporates are run through a web interface that enables human-in-the-loop verification
to ensure accurate estimations. To demonstrate the efficacy of this suite of tools, we present the
results of estimating the shooter's location of the Las Vegas Shooting in 2017 and show that VERA
accurately predicts the shooter's location using only the first few gunshots. We then point out
future directions that can help improve the system and further reduce unnecessary human labor in
the process. All relevant source code, including the web interface and machine learning models,
are freely available in Github. We do so in the hope that this tool can be used by anyone who needs it
to protect and promote human rights and public safety. We also hope that researchers and software
developers will be inspired to improve and expand this system moving forward to better meet the needs
of public safety and human rights. 