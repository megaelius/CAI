While meta-learning approaches that utilize neural network representations have made progress
in few-shot image classification, reinforcement learning, and, more recently, image semantic
segmentation, the training algorithms and model architectures have become increasingly specialized
to the few-shot domain. A natural question that arises is how to develop learning systems that scale
from few-shot to many-shot settings while yielding competitive performance in both. One scalable
potential approach that does not require ensembling many models nor the computational costs of
relation networks, is to meta-learn an initialization. In this work, we study first-order meta-learning
of initializations for deep neural networks that must produce dense, structured predictions given
an arbitrary amount of training data for a new task. Our primary contributions include (1), an extension
and experimental analysis of first-order model agnostic meta-learning algorithms (including
FOMAML and Reptile) to image segmentation, (2) a novel neural network architecture built for parameter
efficiency and fast learning which we call EfficientLab, (3) a formalization of the generalization
error of meta-learning algorithms, which we leverage to decrease error on unseen tasks, and (4)
a small benchmark dataset, FP-k, for the empirical study of how meta-learning systems perform in
both few- and many-shot settings. We show that meta-learned initializations for image segmentation
provide value for both canonical few-shot learning problems and larger datasets, outperforming
ImageNet-trained initializations for up to 400 densely labeled examples. We find that our network,
with an empirically estimated optimal update procedure, yields state of the art results on the FSS-1000
dataset while only requiring one forward pass through a single model at evaluation time. 