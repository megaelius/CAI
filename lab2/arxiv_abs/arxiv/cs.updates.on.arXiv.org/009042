We establish the fundamental limits of lossless analog compression by considering the recovery
of arbitrary m-dimensional real random vectors x from the noiseless linear measurements y=Ax with
n x m measurement matrix A. Our theory is inspired by the groundbreaking work of Wu and Verdu (2010)
on almost lossless analog compression, but applies to the nonasymptotic, i.e., fixed-m case, and
considers zero error probability. Specifically, our achievability result states that, for almost
all A, the random vector x can be recovered with zero error probability provided that n > K(x), where
K(x) is given by the infimum of the lower modified Minkowski dimensions over all support sets U of
x. We then particularize this achievability result to the class of s-rectifiable random vectors
as introduced in Koliander et al. (2016); these are random vectors of absolutely continuous distribution---with
respect to the s-dimensional Hausdorff measure---supported on countable unions of s-dimensional
differentiable manifolds. Countable unions of differentiable manifolds include essentially
all signal models used in compressed sensing theory, in spectrum-blind sampling, and in the matrix
completion problem. Specifically, we prove that, for almost all A, s-rectifiable random vectors
x can be recovered with zero error probability from n>s linear measurements. This threshold is,
however, found not to be tight as exemplified by the construction of an s-rectifiable random vector
that can be recovered with zero error probability from n<s linear measurements. This leads us to
the introduction of the new class of s-analytic random vectors, which admit a strong converse in
the sense of n greater than or equal to s being necessary for recovery with probability of error smaller
than one. The central conceptual tools in the development of our theory are geometric measure theory
and the theory of real analytic functions. 