The attention-based encoder-decoder framework has recently achieved impressive results for
scene text recognition, and many variants have emerged with improvements in recognition quality.
However, it performs poorly on contextless texts (e.g., random character sequences) which is unacceptable
in most of real application scenarios. In this paper, we first deeply investigate the decoding process
of the decoder. We empirically find that a representative character-level sequence decoder utilizes
not only context information but also positional information. The existing approaches heavily
relying on contextual information causes the problem of attention drift. To suppress the side-effect
of the attention drift, we propose one novel position enhancement branch, and dynamically fuse
its outputs with those of the decoder attention module for scene text recognition. Specifically,
it contains a position aware module to make the encoder output feature vectors encoding their own
spatial positions, and an attention module to estimate glimpses using the positional clue (i.e.,
the current decoding time step) only. The dynamic fusion is conducted for more robust feature via
an element-wise gate mechanism. Theoretically, our proposed method, dubbed RobustScanner, decodes
individual characters with dynamic ratio between context and positional clues, and utilizes more
positional ones when the decoding sequences with scarce context, and thus is robust and practical.
Empirically, it has achieved new state-of-the-art results on popular regular and irregular text
recognition benchmarks while without much performance drop on contextless benchmarks, validating
its robustness in both context and contextless application scenarios. 