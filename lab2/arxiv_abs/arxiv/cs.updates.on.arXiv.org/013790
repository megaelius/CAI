Light Field (LF) offers unique advantages such as post-capture refocusing and depth estimation,
but low-light conditions limit these capabilities. To restore low-light LFs we should harness
the geometric cues present in different LF views, which is not possible using single-frame low-light
enhancement techniques. We, therefore, propose a deep neural network for Low-Light Light Field
(L3F) restoration, which we refer to as L3Fnet. The proposed L3Fnet not only performs the necessary
visual enhancement of each LF view but also preserves the epipolar geometry across views. We achieve
this by adopting a two-stage architecture for L3Fnet. Stage-I looks at all the LF views to encode
the LF geometry. This encoded information is then used in Stage-II to reconstruct each LF view. To
facilitate learning-based techniques for low-light LF imaging, we collected a comprehensive
LF dataset of various scenes. For each scene, we captured four LFs, one with near-optimal exposure
and ISO settings and the others at different levels of low-light conditions varying from low to extreme
low-light settings. The effectiveness of the proposed L3Fnet is supported by both visual and numerical
comparisons on this dataset. To further analyze the performance of low-light reconstruction methods,
we also propose an L3F-wild dataset that contains LF captured late at night with almost zero lux values.
No ground truth is available in this dataset. To perform well on the L3F-wild dataset, any method
must adapt to the light level of the captured scene. To do this we propose a novel pre-processing block
that makes L3Fnet robust to various degrees of low-light conditions. Lastly, we show that L3Fnet
can also be used for low-light enhancement of single-frame images, despite it being engineered
for LF data. We do so by converting the single-frame DSLR image into a form suitable to L3Fnet, which
we call as pseudo-LF. 