Split-Merge MCMC (Monte Carlo Markov Chain) is one of the essential and popular variants of MCMC
for problems when an MCMC state consists of an unknown number of components. It is well known that
state-of-the-art methods for split-merge MCMC do not scale well. Strategies for rapid mixing requires
smart and informative proposals to reduce the rejection rate. However, all known smart proposals
involve expensive operations to suggest informative transitions. As a result, the cost of each
iteration is prohibitive for massive scale datasets. It is further known that uninformative but
computationally efficient proposals, such as random split-merge, leads to extremely slow convergence.
This tradeoff between mixing time and per update cost seems hard to get around. In this paper, we get
around this tradeoff by utilizing simple similarity information, such as cosine similarity, between
the entity vectors to design a proposal distribution. Such information is readily available in
almost all applications. We show that the recent use of locality sensitive hashing for efficient
adaptive sampling can be leveraged to obtain a computationally efficient pseudo-marginal MCMC.
The new split-merge MCMC has cheap proposal which is also informative and needs significantly fewer
iterations than random split-merge. Overall, we obtain a sweet tradeoff between convergence and
per update cost. As a direct consequence, our proposal, named LSHSM, is around 5x faster than the
state-of-the-art sampling methods on both synthetic datasets and two large real datasets KDDCUP
and PubMed with several millions of entities and thousands of clusters. 