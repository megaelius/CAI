Machine learning (ML) is redefining what is possible in data-intensive fields of science and engineering.
However, applying ML to problems in the physical sciences comes with a unique set of challenges:
scientists want physically interpretable models that can (i) generalize to predict previously
unobserved behaviors, (ii) provide effective forecasting predictions (extrapolation), and
(iii) be certifiable. Autonomous systems will necessarily interact with changing and uncertain
environments, motivating the need for models that can accurately extrapolate based on physical
principles (e.g. Newton's universal second law for classical mechanics, F=ma). Standard ML approaches
have shown impressive performance for predicting dynamics in an interpolatory regime, but the
resulting models often lack interpretability and fail to generalize. In this paper, we introduce
a unified sparse optimization framework that learns governing dynamical systems models from data,
selecting relevant terms in the dynamics from a library of possible functions. The resulting models
are parsimonious, have physical interpretations, and can generalize to new parameter regimes.
Our framework allows the use of non-convex sparsity promoting regularization functions and can
be adapted to address key challenges in scientific problems and data sets, including outliers,
parametric dependencies, and physical constraints. We show that the approach discovers parsimonious
dynamical models on several example systems, including a spiking neuron model. This flexible approach
can be tailored to the unique challenges associated with a wide range of applications and data sets,
providing a powerful ML-based framework for learning governing models for physical systems from
data. 