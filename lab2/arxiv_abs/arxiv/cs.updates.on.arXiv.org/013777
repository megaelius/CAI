Suboptimal interaction with patient data and challenges in mastering 3D anatomy based on ill-posed
2D interventional images are essential concerns in image-guided therapies. Augmented reality
(AR) has been introduced in the operating rooms in the last decade; however, in image-guided interventions,
it has often only been considered as a visualization device improving traditional workflows. As
a consequence, the technology is gaining minimum maturity that it requires to redefine new procedures,
user interfaces, and interactions. The main contribution of this paper is to reveal how exemplary
workflows are redefined by taking full advantage of head-mounted displays when entirely co-registered
with the imaging system at all times. The proposed AR landscape is enabled by co-localizing the users
and the imaging devices via the operating room environment and exploiting all involved frustums
to move spatial information between different bodies. The awareness of the system from the geometric
and physical characteristics of X-ray imaging allows the redefinition of different human-machine
interfaces. We demonstrate that this AR paradigm is generic, and can benefit a wide variety of procedures.
Our system achieved an error of $4.76\pm2.91$ mm for placing K-wire in a fracture management procedure,
and yielded errors of $1.57\pm1.16^\circ$ and $1.46\pm1.00^\circ$ in the abduction and anteversion
angles, respectively, for total hip arthroplasty. We hope that our holistic approach towards improving
the interface of surgery not only augments the surgeon's capabilities but also augments the surgical
team's experience in carrying out an effective intervention with reduced complications and provide
novel approaches of documenting procedures for training purposes. 