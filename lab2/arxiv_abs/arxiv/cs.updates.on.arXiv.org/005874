As a non-parametric Bayesian model which produces informative predictive distribution, Gaussian
process (GP) has been widely used in various fields, like regression, classification and optimization.
The cubic complexity of standard GP however leads to poor scalability, which poses challenges in
the era of big data. Hence, various scalable GPs have been developed in the literature in order to
improve the scalability while retaining desirable prediction accuracy. This paper devotes to
investigating the methodological characteristics and performance of representative global
and local scalable GPs including sparse approximations and local aggregations from four main perspectives:
scalability, capability, controllability and robustness. The numerical experiments on two toy
examples and five real-world datasets with up to 250K points offer the following findings. In terms
of scalability, most of the scalable GPs own a time complexity that is linear to the training size.
In terms of capability, the sparse approximations capture the long-term spatial correlations,
the local aggregations capture the local patterns but suffer from over-fitting in some scenarios.
In terms of controllability, we could improve the performance of sparse approximations by simply
increasing the inducing size. But this is not the case for local aggregations. In terms of robustness,
local aggregations are robust to various initializations of hyperparameters due to the local attention
mechanism. Finally, we highlight that the proper hybrid of global and local scalable GPs may be a
promising way to improve both the model capability and scalability for big data. 