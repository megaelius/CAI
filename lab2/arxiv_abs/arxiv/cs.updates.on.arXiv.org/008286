Humans make decisions and act alongside other humans to pursue both short-term and long-term goals.
As a result of ongoing progress in areas such as computing science and automation, humans now also
interact with non-human agents of varying complexity as part of their day-to-day activities; substantial
work is being done to integrate increasingly intelligent machine agents into human work and play.
With increases in the cognitive, sensory, and motor capacity of these agents, intelligent machinery
for human assistance can now reasonably be considered to engage in joint action with humans---i.e.,
two or more agents adapting their behaviour and their understanding of each other so as to progress
in shared objectives or goals. The mechanisms, conditions, and opportunities for skillful joint
action in human-machine partnerships is of great interest to multiple communities. Despite this,
human-machine joint action is as yet under-explored, especially in cases where a human and an intelligent
machine interact in a persistent way during the course of real-time, daily-life experience. In
this work, we contribute a virtual reality environment wherein a human and an agent can adapt their
predictions, their actions, and their communication so as to pursue a simple foraging task. In a
case study with a single participant, we provide an example of human-agent coordination and decision-making
involving prediction learning on the part of the human and the machine agent, and control learning
on the part of the machine agent wherein audio communication signals are used to cue its human partner
in service of acquiring shared reward. These comparisons suggest the utility of studying human-machine
coordination in a virtual reality environment, and identify further research that will expand
our understanding of persistent human-machine joint action. 