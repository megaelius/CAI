Deep Learning (DL) model-based AI services are increasingly offered in a variety of predictive
analytics services such as computer vision, natural language processing, speech recognition.
However, the quality of the DL models can degrade over time due to changes in the input data distribution,
thereby requiring periodic model updates. Although cloud data-centers can meet the computational
requirements of the resource-intensive and time-consuming model update task, transferring data
from the edge devices to the cloud incurs a significant cost in terms of network bandwidth and are
prone to data privacy issues. With the advent of GPU-enabled edge devices, the DL model update can
be performed at the edge in a distributed manner using multiple connected edge devices. However,
efficiently utilizing the edge resources for the model update is a hard problem due to the heterogeneity
among the edge devices and the resource interference caused by the co-location of the DL model update
task with latency-critical tasks running in the background. To overcome these challenges, we present
Deep-Edge, a load- and interference-aware, fault-tolerant resource management framework for
performing model update at the edge that uses distributed training. This paper makes the following
contributions. First, it provides a unified framework for monitoring, profiling, and deploying
the DL model update tasks on heterogeneous edge devices. Second, it presents a scheduler that reduces
the total re-training time by appropriately selecting the edge devices and distributing data among
them such that no latency-critical applications experience deadline violations. Finally, we
present empirical results to validate the efficacy of the framework using a real-world DL model
update case-study based on the Caltech dataset and an edge AI cluster testbed. 