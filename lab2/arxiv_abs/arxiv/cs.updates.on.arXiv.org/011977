To infer information flow in any network of agents, it is important first and foremost to establish
causal temporal relations between the nodes. Practical and automated methods that can infer causality
are difficult to find, and the subject of ongoing research. While Shannon information only detects
correlation, there are several information-theoretic notions of "directed information" that
have successfully detected causality in some systems, in particular in the neuroscience community.
However, recent work has shown that some directed information measures can sometimes inadequately
estimate the extent of causal relations, or even fail to identify existing cause-effect relations
between components of systems, especially if neurons contribute in a cryptographic manner to influence
the effector neuron. Here, we test how often cryptographic logic emerges in an evolutionary process
that generates artificial neural circuits for two fundamental cognitive tasks: motion detection
and sound localization. We also test whether activity time-series recorded from behaving digital
brains can infer information flow using the transfer entropy concept, when compared to a ground-truth
model of causal influence constructed from connectivity and circuit logic. Our results suggest
that transfer entropy will sometimes fail to infer causality when it exists, and sometimes suggest
a causal connection when there is none. However, the extent of incorrect inference strongly depends
on the cognitive task considered. These results emphasize the importance of understanding the
fundamental logic processes that contribute to information flow in cognitive processing, and
quantifying their relevance in any given nervous system. 