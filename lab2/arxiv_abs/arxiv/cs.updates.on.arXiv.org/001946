Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve
state-of-the-art performance in many machine learning tasks, including but not limited to image
classification, text mining, and speech processing. However, recent research on DNNs has indicated
ever-increasing concern on the robustness to adversarial examples, especially for security-critical
tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability
of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human
and machines) adversarial images that lead to misclassification. Furthermore, researchers have
shown that these adversarial images are highly transferable by simply training and attacking a
substitute model built upon the target model, known as a black-box attack to DNNs. Similar to the
setting of training substitute models, in this paper we propose an effective black-box attack that
also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However,
different from leveraging attack transferability from substitute models, we propose zeroth order
optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating
adversarial examples. We use zeroth order stochastic coordinate descent along with dimension
reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box
models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished,
sparing the need for training substitute models and avoiding the loss in attack transferability.
Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective
as the state-of-the-art white-box attack and significantly outperforms existing black-box attacks
via substitute models. 