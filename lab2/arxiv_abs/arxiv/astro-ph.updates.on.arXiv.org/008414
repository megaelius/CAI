We present a framework for modelling the star-formation histories of galaxies as a stochastic process.
We define this stochastic process through a power spectrum density with a functional form of a broken
power-law. Star-formation histories are correlated on short timescales, the strength of this
correlation described by a power-law slope, $\alpha$, and they decorrelate to resemble white noise
over a timescale that is proportional to the timescale of the break in the power spectrum density,
$\tau_{\rm break}$. We use this framework to explore the properties of the stochastic process that,
we assume, gives rise to the log-normal scatter about the relationship between star-formation
rate and stellar mass, the so-called galaxy star-forming main sequence. Specifically, we show
how the measurements of the normalisation and width ($\sigma_{\rm MS}$) of the main sequence, measured
in several passbands that probe different timescales, give a constraint on the parameters of the
underlying power spectrum density. We first derive these results analytically for a simplified
case where we model observations by averaging over the recent star-formation history. We then run
numerical simulations to find results for more realistic observational cases. As a proof of concept,
we use observational estimates of the main sequence scatter at $z\sim0$ and $M_{\star}\approx10^{10}~M_{\odot}$
measured in H$\alpha$, UV+IR and the u-band, and show that combination of these point to $\tau_{\rm
break}=178^{+104}_{-66}$ Myr, when assuming $\alpha=2$. This implies that star-formation histories
of galaxies lose "memory" of their previous activity on a timescale of $\sim200$ Myr, highlighting
the importance of baryonic effects that act over the dynamical timescales of galaxies. 