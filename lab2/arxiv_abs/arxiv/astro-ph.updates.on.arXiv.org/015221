In order to assure a stable series of recorded images of sufficient quality for further scientific
analysis, an objective image quality measure is required. Especially when dealing with ground-based
observations, which are subject to varying seeing conditions and clouds, the quality assessment
has to take multiple effects into account and provide information about the affected regions. In
this study, we develop a deep learning method that is suited to identify anomalies and provide an
image quality assessment of solar full-disk H$\alpha$ filtergrams. The approach is based on the
structural appearance and the true image distribution of high-quality observations. We employ
a neural network with an encoder-decoder architecture to perform an identity transformation of
selected high-quality observations. The encoder network is used to achieve a compressed representation
of the input data, which is reconstructed to the original by the decoder. We use adversarial training
to recover truncated information based on the high-quality image distribution. When images with
reduced quality are transformed, the reconstruction of unknown features (e.g., clouds, contrails,
partial occultation) shows deviations from the original. This difference is used to quantify the
quality of the observations and to identify the affected regions. We apply our method to full-disk
H$\alpha$ filtergrams from Kanzelh\"ohe Observatory recorded during 2012-2019 and demonstrate
its capability to perform a reliable image quality assessment for various atmospheric conditions
and instrumental effects, without the requirement of reference observations. Our quality metric
achieves an accuracy of 98.5% in distinguishing observations with quality-degrading effects
from clear observations and provides a continuous quality measure which is in good agreement with
the human perception. 