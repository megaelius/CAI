The use of luminous red galaxies as cosmic chronometers provides us with an indispensable method
of measuring the universal expansion rate H(z) in a model-independent way. Unlike many probes of
the cosmological history, this approach does not rely on integrated quantities, such as the luminosity
distance, and therefore does not require the pre-assumption of any particular model, which may
bias subsequent interpretations of the data. We employ three statistical tools -- the Akaike, Kullback,
and Bayes Information Criteria (AIC, KIC and BIC) -- to compare the LCDM model and the R_h=ct Universe
with the currently available measurements of H(z), and show that the R_h=ct Universe is favored
by these model selection criteria. The parameters in each model are individually optimized by maximum
likelihood estimation. The R_h=ct Universe fits the data with a reduced chi^2_dof=0.745 for a Hubble
constant H_0=63.2+/-2.5 km/s/Mpc, and H_0 is the sole parameter in this model. By comparison, the
optimal LCDM model, which has three free parameters (including H_0=68.9+/-2.4 km/s/Mpc, Omega_m=0.32,
and a dark-energy equation of state p_de=-rho_de), fits the H(z) data with a reduced chi^2_dof=0.777.
With these chi^2_dof values, the AIC yields a likelihood of about 82 per cent that the distance--redshift
relation of the R_h=ct Universe is closer to the correct cosmology, than is the case for LCDM. If the
alternative BIC criterion is used, the respective Bayesian posterior probabilities are 91.2 per
cent (R_h=ct) versus 8.8 per cent (LCDM). Using the concordance LCDM parameter values, rather than
those obtained by fitting LCDM to the cosmic chronometer data, would further disfavor LCDM. 