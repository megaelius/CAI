Analyzing the Galactic plane CO survey with the Nobeyama 45-m telescope, we compared the spectral
column density (SCD) of H$_2$ calculated for $^{12}$CO line using the current conversion factor
$X_{\rm CO}$ to that for $^{13}$CO line under LTE in M16 and W43 regions. Here, SCD is defined by $dN_{\rm
H_2}/dv$ with $N_{\rm H_2}$ and $v$ being the column density and radial velocity, respectively.
It is found that the $X_{\rm CO}$ method significantly under-estimates the H$_2$ density in a cloud
or region, where SCD exceeds a critical value ($ \sim 3\times 10^{21}\ [{\rm H_2 \ cm^{-2} \ (km \ s^{-1})^{-1}}]$),
but over-estimates in lower SCD regions. We point out that the actual CO-to-H$_2$ conversion factor
varies with the H$_2$ column density or with the CO-line intensity: It increases in the inner and
opaque parts of molecular clouds, whereas it decreases in the low-density envelopes. However,
in so far as the current $X_{\rm CO}$ is used combined with the integrated $^{12}$CO intensity averaged
over an entire cloud, it yields a consistent value with that calculated using the $^{13}$CO intensity
by LTE. Based on the analysis, we propose a new CO-to-\Htwo conversion relation, $N_{\rm H_2}^*
= \int X_{\rm CO}^*(T_{\rm B}) T_{\rm B} dv$, where $X_{\rm CO}^*=(T_{\rm B}/T_{\rm B}^*)^\beta
X_{\rm CO}$ is the modified spectral conversion factor as a function of the brightness temperature,
$T_{\rm B}$, of the ${12}$CO ($J=1-0$) line, and $\beta\sim 1-2$ and $T_{\rm B}^*=12-16$ K are empirical
constants obtained by fitting to the observed data. The formula corrects for the over/under estimation
of the column density at low/high-CO line intensities, and is applicable to molecular clouds with
$T_{\rm B} \ge 1$ K (rms noise in the data) from envelope to cores at sub-parsec scales (resolution).
(Full resolution copy available at this http URL) 