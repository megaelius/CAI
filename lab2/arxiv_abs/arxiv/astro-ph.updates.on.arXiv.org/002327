As first emphasized by Bernard Schutz, there exists a universal distribution of signal-to-noise
ratios for gravitational wave detection. Because gravitational waves (GWs) are almost impossible
to obscure via dust absorption or other astrophysical processes, the strength of the detected signal
is dictated solely by the emission strength and the distance to the source. Assuming that the space
density of an arbitrary population of GW sources does not evolve, we show explicitly that the distribution
of detected signal-to-noise (SNR) values depends solely on the detection threshold; it is independent
of the detector network (interferometer or pulsar timing array), the individual detector noise
curves (initial or Advanced LIGO), the nature of the GW sources (compact binary coalescence, supernova,
or some other discrete source), and the distributions of source variables (only non-spinning neutron
stars of mass exactly $1.4\,M_\odot$ or a complicated distribution of masses and spins). We derive
the SNR distribution for each individual detector within a network as a function of the relative
detector orientations and sensitivities. While most detections will have SNR near the detection
threshold, there will be a tail of events to higher SNR. We derive the SNR distribution of the loudest
(highest SNR) events in any given sample of detections. We find that the median SNR of the loudest
out of the first four events should have an $\mbox{SNR}=22$ (for a threshold of 12, appropriate for
the Advanced LIGO/Virgo network), increasing to a median value for the loudest SNR of 47 for 40 detections.
We expect these loudest events to provide particularly powerful constraints on their source parameters,
and they will play an important role in extracting astrophysics from gravitational wave sources.
These distributions also offer an important internal calibration of the response of the GW detector
networks. 