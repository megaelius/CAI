The ohmic decay of magnetic fields in the crusts of neutron stars is generally believed to be governed
by Hall drift which leads to what is known as a Hall cascade. Here we show that helical and fractionally
helical magnetic fields undergo strong inverse cascading like in magnetohydrodynamics (MHD),
but the magnetic energy decays more slowly with time $t$: $\propto t^{-2/5}$ instead of $\propto
t^{-2/3}$ in MHD. Even for a nonhelical magnetic field there is inverse cascading with an increase
of magnetic energy at the largest possible scale of the system. The strength of this effect depends
on the strength of the magnetic field and disappears for weak fields. The inertial range scaling
with wavenumber $k$ is compatible with earlier findings for the forced Hall cascade, i.e., proportional
to $k^{-7/3}$, but in the decaying case, the subinertial range spectrum steepens to a novel $k^5$
slope instead of the $k^4$ slope in MHD. The energy of the large-scale magnetic field can increase
quadratically in time through inverse cascading. For helical fields, the energy dissipation is
found to be inversely proportional to the large-scale magnetic field and proportional to the fifth
power of the root-mean square (rms) magnetic field. For neutron star conditions with an rms magnetic
field of a few times $10^{14}\,$G, the large-scale magnetic field might only be $10^{11}\,$G, while
still producing magnetic dissipation of $10^{33}\,$erg$\,$s$^{-1}$ for thousands of years,
which could manifest itself through X-ray emission. Finally, it is shown that the conclusions from
local unstratified models agree rather well with those from stratified models with boundaries.
