With large-scale galaxy surveys, we can observe hundreds of thousands of galaxies or more, up to
billions with upcoming experiments such as WFIRST, Euclid and LSST. While such surveys cannot obtain
spectra for all observed galaxies, we have access to the galaxy magnitudes in color filters. This
data set behaves like a high-dimensional nonlinear surface, making it an excellent target for machine
learning methods. In this work, we use a lightcone of semianalytic galaxies tuned to match CANDELS
observations from Lu et al. (2014) to train a set of neural networks on a set of galaxy properties (stellar
mass, metallicity, and average star formation rate) using the truth values from the semianalytic
catalogs. We also demonstrate the effect of adding simulated observational noise to the simulated
magnitudes, and then use neural networks trained on the noisy data to predict stellar masses, metallicities,
and average star formation rates on real CANDELS galaxies, comparing our results to the physical
parameters obtained from SED fitting. On semianalytic galaxies alone, we are nearly competitive
with template-fitting methods. For the observed CANDELS data, our results are not as accurate,
with indications that this inaccuracy is due to a combination of different assumptions in template-fitting
and differences between the semianalytic models and the observed galaxies, particularly in the
noise properties. Our results show that stellar mass, metallicity, and star formation rate can
in principle be measured with neural networks at a competitive degree of accuracy and precision
relative to physically motivated template-fitting methods if an appropriate training set can
be obtained. 