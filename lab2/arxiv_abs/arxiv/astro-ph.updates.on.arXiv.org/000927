Galaxy counts and recent measurements of the luminosity density in the near-infrared (NIR) have
indicated the possibility that the local universe may be under-dense on scales of several hundred
megaparsecs. The presence of a large-scale under-density in the local universe could introduce
significant biases into the interpretation of cosmological observables, and, in particular,
into the inferred effects of dark energy on the expansion rate. Here we measure the K-band luminosity
density as a function of redshift to test for such a local under-density. In this effort, we combine
photometry from UKIDSS and 2MASS with redshifts from the SDSS, 2DFGRS, 6DFGRS, 2MR, and GAMA surveys.
We find that the overall shape of the z=0 rest-frame K-band luminosity function (M* = -22.15 +/- 0.04
and alpha = -1.02 +/- 0.03) appears to be relatively constant as a function of environment and redshift
out to z ~0.2. We find a local (z < 0.07) luminosity density that is in good agreement with previous
studies. Beyond z ~ 0.07 we detect a rising luminosity density that reaches a value ~1.5 times higher
than that measured locally at z>0.1. This suggests that the stellar mass density as a function
of redshift follows a similar trend. Assuming that luminous matter traces the underlying dark matter
distribution, this implies that the local mass density of the universe may be lower than the global
value on a scale and amplitude sufficient to introduce significant biases into the determination
of basic cosmological observables, such as the expansion rate. An under-density of roughly this
scale and amplitude would be sufficient to resolve the apparent tension between direct measurements
of the Hubble constant and those inferred by Planck. 