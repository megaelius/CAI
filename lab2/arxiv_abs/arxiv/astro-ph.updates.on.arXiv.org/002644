New supernova surveys such as the Dark Energy Survey, Pan-STARRS and the LSST will produce an unprecedented
number of photometric supernova candidates, most with no spectroscopic data. Avoiding biases
in cosmological parameters due to the resulting inevitable contamination from non-Ia supernovae
can be achieved with the BEAMS formalism, allowing for fully photometric supernova cosmology studies.
Here we extend BEAMS to deal with the case in which the supernovae are correlated by systematic uncertainties.
The analytical form of the full BEAMS posterior requires evaluating 2^N terms, where N is the number
of supernova candidates. This `exponential catastrophe' is computationally unfeasible even
for N of order 100. We circumvent the exponential catastrophe by marginalising numerically instead
of analytically over the possible supernova types: we augment the cosmological parameters with
nuisance parameters describing the covariance matrix and the types of all the supernovae, \tau_i,
that we include in our MCMC analysis. We show that this method deals well even with large, unknown
systematic uncertainties without a major increase in computational time, whereas ignoring the
correlations can lead to significant biases and incorrect credible contours. We then compare the
numerical marginalisation technique with a perturbative expansion of the posterior based on the
insight that future surveys will have exquisite light curves and hence the probability that a given
candidate is a Type Ia will be close to unity or zero, for most objects. Although this perturbative
approach changes computation of the posterior from a 2^N problem into an N^2 or N^3 one, we show that
it leads to biases in general through a small number of misclassifications, implying that numerical
marginalisation is superior. 