Many statistical models in cosmology can be simulated forwards but have intractable likelihood
functions. Likelihood-free inference methods allow us to perform Bayesian inference from these
models using only forward simulations, free from any likelihood assumptions or approximations.
Likelihood-free inference generically involves simulating mock data and comparing to the observed
data; this comparison in data-space suffers from the curse of dimensionality and requires compression
of the data to a small number of summary statistics to be tractable. In this paper we use massive optimal
data compression to reduce the dimensionality of the data-space to just one number per parameter,
providing a natural and optimal framework for summary statistic choice for likelihood-free inference.
Secondly, we introduce density estimation likelihood-free inference, which learns a parameterized
model for joint distribution of data and parameters, yielding both the parameter posterior and
the model evidence. This approach is conceptually simple, requires less tuning than traditional
Approximate Bayesian Computation approaches to likelihood-free inference and can give high-fidelity
posteriors from orders of magnitude fewer forward simulations. As an additional bonus, it enables
parameter inference and Bayesian model comparison simultaneously. We demonstrate density estimation
likelihood-free inference with massive data compression on an analysis of the joint light-curve
analysis supernova data. We show that high-fidelity posterior inference is possible for full-scale
cosmological data analyses with as few as $\sim 10^4$ simulations, with substantial scope for further
improvement, demonstrating the scalability of likelihood-free inference to large and complex
cosmological datasets. 