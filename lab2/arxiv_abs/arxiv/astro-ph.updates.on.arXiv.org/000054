The disruption of stars by supermassive black holes has been linked to more than a dozen flares in
the cores of galaxies out to redshift $z \sim 0.4$. Modeling these flares properly requires a prediction
of the rate of mass return to the black hole after a disruption. Through hydrodynamical simulation,
we show that aside from the full disruption of a solar mass star at the exact limit where the star is
destroyed, the common assumptions used to estimate $\dot{M}(t)$, the rate of mass return to the
black hole, are largely invalid. While the analytical approximation to tidal disruption predicts
that the least-centrally concentrated stars and the deepest encounters should have more quickly-peaked
flares, we find that the most-centrally concentrated stars have the quickest-peaking flares,
and the trend between the time of peak and the impact parameter for deeply-penetrating encounters
reverses beyond the critical distance at which the star is completely destroyed. We also show that
the most-centrally concentrated stars produced a characteristic drop in $\dot{M}(t)$ shortly
after peak when a star is only partially disrupted, with the power law index $n$ being as extreme as
-4 in the months immediately following the peak of a flare. Additionally, we find that $n$ asymptotes
to $\simeq -2.2$ for both low- and high-mass stars for approximately half of all stellar disruptions.
Both of these results are significantly steeper than the typically assumed $n = -5/3$. As these precipitous
decay rates are only seen for events in which a stellar core survives the disruption, they can be used
to determine if an observed tidal disruption flare produced a surviving remnant. These results
should be taken into consideration when flares arising from tidal disruptions are modeled. [abridged]
