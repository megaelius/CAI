The winds of ionized gas driven by Active Galactic Nuclei (AGN) can be studied through absorption
lines in their X-ray spectra. A recurring feature of these outflows is their broad ionization distribution,
including essentially all ionization levels (e.g., Fe^0+ to Fe^25+). This characteristic feature
can be quantified with the absorption measure distribution (AMD), defined as the distribution
of column density with ionization parameter |dN / dlog xi|. Observed AMDs extend over 0.1 < xi
< 10^4 (cgs), and are remarkably similar in different objects. Power-law fits (|dN /dlog xi|
~ N_1 xi^a) yield N_1 = 3x10^{21} cm^-2 +- 0.4 dex and a = 0 -- 0.4. What is the source of this broad ionization
distribution, and what sets the small range of observed $N_1$ and $a$? A common interpretation is
a multiphase outflow, with a wide range of gas densities in a uniform gas pressure medium. However,
the incident radiation pressure leads to a gas pressure gradient in the photoionized gas, and therefore
to a broad range of ionization states within a single slab. We show that this compression of the gas
by the radiation pressure leads to an AMD with |dN / dlog xi| = 8x10^21 xi^0.03 cm^-2, remarkably similar
to that observed. The calculated values of $N_1$ and $a$ depend weakly on the gas metallicity, the
ionizing spectral slope, the distance from the nucleus, the ambient density, and the total absorber
column. Thus, radiation pressure compression (RPC) of the photoionized gas provides a natural
explanation for the observed AMD. RPC predicts that the gas pressure increases with decreasing
ionization, which can be used to test the validity of RPC in ionized AGN outflows. 