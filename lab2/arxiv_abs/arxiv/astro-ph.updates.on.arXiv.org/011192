In order to separate astrophysical gravitational-wave signals from instrumental noise, which
often contains transient non-Gaussian artifacts, astronomers have traditionally relied on bootstrap
methods such as time slides. Bootstrap methods sample with replacement, comparing single-observatory
data to construct a background distribution, which is used to assign a false-alarm probability
to candidate signals. While bootstrap methods have played an important role establishing the first
gravitational-wave detections, there are limitations. First, as the number of detections increases,
it makes increasingly less sense to treat single-observatory data as bootstrap-estimated noise,
when we know that the data are filled with astrophysical signals, some resolved, some unresolved.
Second, it has been known for a decade that background estimation from time-slides eventually breaks
down due to saturation effects, yielding incorrect estimates of significance. Third, the false
alarm probability cannot be used to weight candidate significance, for example when performing
population inference on a set of candidates. Given recent debate about marginally resolved gravitational-wave
detection claims, the question of significance has practical consequences. We propose a Bayesian
framework for calculating the odds that a signal is of astrophysical origin versus instrumental
noise without bootstrap noise estimation. We show how the astrophysical odds can safely accommodate
glitches. We argue that it is statistically optimal. We demonstrate the method with simulated noise
and provide examples to build intuition about this new approach to significance. 