The Kepler DR25 planet candidate catalog was produced using an automated method of planet candidate
identification based on various tests. These tests were tuned to obtain a reasonable but arbitrary
balance between catalog completeness and reliability. We produce new catalogs with differing
balances of completeness and reliability by varying these tests, and study the impact of these alternative
catalogs on occurrence rates. We find that if there is no correction for reliability, different
catalogs give statistically inconsistent occurrence rates, while if we correct for both completeness
and reliability, we get statistically consistent occurrence rates. This is a strong indication
that correction for completeness and reliability is critical for the accurate computation of occurrence
rates. Additionally, we find that this result is the same whether using Bayesian Poisson likelihood
MCMC or Approximate Bayesian Computation methods. We also examine the use of a Robovetter disposition
score cut as an alternative to reliability correction, and find that while a score cut does increase
the reliability of the catalog, it is not as accurate as performing a full reliability correction.
We get the same result when performing a reliability correction with and without a score cut. Therefore
removing low-score planets removes data without providing any advantage, and should be avoided
when possible. We make our alternative catalogs publicly available, and propose that these should
be used as a test of occurrence rate methods, with the requirement that a method should provide statistically
consistent occurrence rates for all these catalogs. 