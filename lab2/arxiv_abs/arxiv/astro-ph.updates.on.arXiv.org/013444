Observations since the 1970's have revealed the existence of cool-core (CC) clusters, which are
galaxy clusters with a central cooling time much shorter than the age of the universe. Both observations
and theory suggest that the ambient gas at the centers of galaxy clusters is thermally regulated
by a central heating mechanism that suppresses condensation (most likely an active galactic nucleus,
or AGN). Previous analytical work has suggested specific configurations of heating kernels that
may result in thermal balance and a steady state. To test this hypothesis, we simulated idealized
galaxy clusters using the ENZO cosmology code with a spatial heat-input kernel meant to mimic feedback
from a central AGN. Thermal heating as a function of radius was injected according to a range of kernels,
with global thermal balance enforced at all times. We compare our simulation results with observed
entropy profiles from the ACCEPT cluster dataset. Although some heating kernels produced thermally
steady galaxy clusters, no kernel was able to produce a steady cluster with a central entropy as low
as the central entropies typically observed among CC clusters. The general behavior of the simulations
depended on the amount of heating in the inner $10 ~\text{kpc}$, with low central heating leading
to central cooling catastrophes, high central heating creating a central convective zone with
an inverted entropy gradient, and intermediate heating leading to a flat but elevated entropy core.
The simulated clusters enter an unsteady multiphase state on a timescale proportional to the square
of the cooling time of the lowest entropy gas in the simulation, with centrally concentrated heating
resulting in a steady state lasting for a longer period of time. 