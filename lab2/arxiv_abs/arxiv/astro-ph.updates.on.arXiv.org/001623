The modeling of turbulence, whether it be numerical or analytical, is a difficult challenge. Turbulence
is amenable to analysis with linear theory if it is subject to rapid distortions, i.e., motions occurring
on a time scale that is short compared to the time scale for non-linear interactions. Such an approach
could prove useful for understanding aspects of astrophysical turbulence, which is often subject
to rapid distortions, such as supernova explosions or the free-fall associated with gravitational
instability. As a proof of principle, a particularly simple problem is considered here: the evolution
of vorticity due to a planar rarefaction in an ideal gas. Vorticity can either grow or decay in the
wake of a rarefaction front, and there are two competing effects that determine which outcome occurs:
entropy fluctuations couple to the mean pressure gradient to produce vorticity via baroclinic
effects, whereas vorticity is damped due to the conservation of angular momentum as the fluid expands.
In the limit of purely entropic fluctuations in the ambient fluid, a strong rarefaction generates
vorticity with a turbulent Mach number on the order of the root-mean square of the ambient entropy
fluctuations. The analytical results are shown to compare well with results from two- and three-dimensional
numerical simulations. Analytical solutions are also derived in the linear regime of Reynolds-averaged
turbulence models. This highlights an inconsistency in standard turbulence models that prevents
them from accurately capturing the physics of rarefaction-turbulence interaction. Finally,
dimensional analysis of the equations indicates that rapid distortion of turbulence can give rise
to two distinct regimes in the turbulent spectrum: a distortion range at large scales where linear
distortion effects dominate, and an inertial range at small scales where non-linear effects dominate.
