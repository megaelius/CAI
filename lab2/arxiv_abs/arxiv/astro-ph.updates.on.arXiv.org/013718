We show that Compton scattering by electrons of the hot intergalactic gas in galaxy clusters should
lead to peculiar distortions of the cosmic background X-ray and soft gamma-ray radiation - an increase
in its brightness at E<60-100 keV and a drop at higher energies. The distortions allow the most important
cluster parameters to be measured. The spectral shape of the distortions and its dependence on the
gas temperature, optical depth, and surface density distribution law have been studied using Monte
Carlo computations and confirmed by analytical estimations. In the cluster frame the maximum of
the background decrease due to the recoil effect occurs at ~500-600 keV. The photoionization of
H- and He-like iron and nickel ions leads to additional distortions in the background spectrum -
a strong absorption line with the threshold at ~9 keV (and also to an absorption jump at ~2 keV for cold
clusters). The absorption of intrinsic thermal radiation from the cluster gas by these ions also
leads to such lines. In nearby (z<1) clusters the line at ~2 keV is noticeably enhanced by absorption
in the colder (~10^6 K) plasma of their peripheral (~3 Mpc) regions; moreover, the absorption line
at ~1.3 keV splits off from it. The redshift of distant clusters shifts the absorption lines in the
background spectrum (at ~2, ~9, and ~500 keV) to lower energies. Thus, in contrast to the microwave
background scattering effect, this effect depends on the cluster redshift z, but in a very peculiar
way. When observing clusters at z>1, the effect allows one to determine how the X-ray background
evolved and how it was "gathered" with z. To detect the effect, the accuracy of measurements should
reach ~0.1%. We consider the most promising clusters for observing the effect and discuss the techniques
whereby the influence of the thermal gas radiation hindering the detection of background distortions
should be minimal. 