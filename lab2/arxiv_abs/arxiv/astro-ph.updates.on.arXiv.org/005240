Abundant observational evidence indicates that the cooling time t_cool of the hot ambient medium
pervading a massive galaxy does not drop much below 10 times the freefall time t_ff at any radius.
Theoretical models have accounted for this finding by hypothesizing that cold clouds start to condense
out of the ambient medium when t_cool/t_ff < 10 and fuel a strong black-hole feedback response that
reheats the ambient gas, but those models have not yet been able to provide a simple explanation for
the origin of the critical t_cool/t_ff ratio. This paper explores a heuristic model for condensation
that links the critical ratio to turbulent driving of gravity-wave oscillations. In the linear
regime, internal gravity waves are thermally unstable in a thermally balanced medium. Buoyancy
oscillations in a balanced medium with t_cool/t_ff therefore grow until they saturate without
condensing at an amplitude that depends on t_cool/t_ff. However, in a medium with 10 < t_cool/t_ff
< 20, turbulence with a velocity dispersion roughly half the galaxy's stellar velocity dispersion
can drive those oscillations into condensation. Intriguingly, this is indeed the gas-phase velocity
dispersion observed among galaxy-cluster cores that contain multiphase gas. It is therefore possible
that both the critical t_cool/t_ff ratio for condensation of ambient gas and the level of turbulence
in that gas are determined by coupling between condensation, feedback, and turbulence. Such a system
can converge to a well-regulated equilibrium state, as long as the fraction of feedback energy that
goes into turbulence is significantly less than the fraction that goes more directly into heat.
