Photometric variability detection is often considered as a hypothesis testing problem: an object
is variable if the null-hypothesis that its brightness is constant can be ruled out given the measurements
and their uncertainties. Uncorrected systematic errors limit the practical applicability of
this approach to high-amplitude variability and well-behaving data sets. Searching for a new variability
detection technique that would be applicable to a wide range of variability types while being robust
to outliers and underestimated measurement uncertainties, we propose to consider variability
detection as a classification problem that can be approached with machine learning. We compare
several classification algorithms: Logistic Regression (LR), Support Vector Machines (SVM),
k-Nearest Neighbors (kNN) Neural Nets (NN), Random Forests (RF) and Stochastic Gradient Boosting
classifier (SGB) applied to 18 features (variability indices) quantifying scatter and/or correlation
between points in a light curve. We use a subset of OGLE-II Large Magellanic Cloud (LMC) photometry
(30265 light curves) that was searched for variability using traditional methods (168 known variable
objects identified) as the training set and then apply the NN to a new test set of 31798 OGLE-II LMC
light curves. Among 205 candidates selected in the test set, 178 are real variables, 13 low-amplitude
variables are new discoveries. We find that the considered machine learning classifiers are more
efficient (they find more variables and less false candidates) compared to traditional techniques
that consider individual variability indices or their linear combination. The NN, SGB, SVM and
RF show a higher efficiency compared to LR and kNN. 