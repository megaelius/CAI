The rate of tidal disruption events (TDEs), $R_\text{TDE}$, is predicted to depend on stellar conditions
near the super-massive black hole (SMBH), which are on sub-parsec scales that are difficult to measure.
Here we test whether $R_\text{TDE}$ depends on kpc-scale global galaxy properties, which are observable.
We concentrate on stellar surface mass density, $\Sigma_{M_\star}$, and velocity dispersion,
$\sigma_v$. We consider 34 TDE candidates, with and without known X-ray emission. The hosts range
in type from star-forming to quiescent to quiescent with strong Balmer absorption lines. The last
(often with post-starburst, aka "E+A," spectra) are overrepresented in our sample by a factor of
$35^{+22}_{-17}$ or $17^{+8}_{-6}$, depending on the strength of the H$\delta$ absorption line.
The $\Sigma_{M_\star}$ of TDE hosts is higher on average than for a volume-weighted control sample
of SDSS galaxies with similar redshifts and stellar masses. This difference arises because: (1)
most of the TDE hosts here are quiescent galaxies, which tend to have higher $\Sigma_{M_\star}$
than the star-forming galaxies that dominate the control sample, and (2) the star-forming TDE hosts
have higher average $\Sigma_{M_\star}$ than the star-forming control galaxies. There is also
evidence that the velocity dispersions of quiescent TDE hosts are lower than for the quiescent control
galaxies. Assuming that $R_\text{TDE}$ depends on these global properties as $R_{\rm TDE}\propto
\Sigma_{M_\star}^\alpha \times \sigma_v^\beta$, and applying a statistical model to the TDE
hosts and control sample, we estimate $\hat{\alpha}=0.9\pm0.2$ and $\hat{\beta}=-1.1\pm0.7$.
This significant, roughly linear dependence on $\Sigma_{M_\star}$ and inverse linear (though
not significant) dependence on $\sigma_v$ is broadly consistent with the TDE rate being tied to
the dynamical relaxation of stars surrounding the SMBH. 