Galaxy counts and recent measurements of the luminosity density in the near-infrared (NIR) have
indicated the possibility that the local universe may be under-dense on scales of several hundred
megaparsecs. The presence of a large-scale under-density in the local universe could introduce
significant biases into the interpretation of cosmological observables, and, in particular,
into the inferred effects of dark energy on the expansion rate. Here we measure the K-band luminosity
density as a function of redshift to test for such a local under-density. We find that the overall
shape of the z = 0 rest-frame K-band luminosity function (M* = -21.6 +/- 0.04 and alpha = -0.99 +/- 0.03)
appears to be relatively constant as a function of environment and redshift out to z ~0.2. We find
a local (z < 0.07) luminosity density that is in good agreement with previous studies. At z >
0.07 we detect a rising luminosity density, and at z > 0.1, it is roughly ~1.5 times higher than
that measured locally. This suggests that the stellar mass density as a function of redshift follows
a similar trend. Assuming that luminous matter traces the underlying dark matter distribution,
this implies that the local mass density of the universe may be lower than the global value on a scale
and amplitude sufficient to introduce significant biases into the determination of basic cosmological
observables, such as the expansion rate. An under-density on this scale and amplitude, for example,
would be more than sufficient to resolve the apparent tension between direct measurements of the
Hubble constant and those inferred by Planck. 