The dispersion measure (DM) is one of the key attributes of radio pulsars and Fast Radio Bursts (FRBs).
There is a mistaken view that the DM is an accurate measure of the column density of electrons between
the observer and the source. To start with, the DM, unlike a true column density, is not a Lorentz invariant.
Next, the DM also includes contribution from ions and is sensitive to the temperature of the plasma
in the intervening clouds. Separately, the primary observable is the dispersion slope, $\mathcal{D}\equiv
\Delta{(t)}/\Delta{(\nu^{-2}})$, where $t(\nu)$ is the arrival time at frequency, $\nu$. A scaling
factor composed of physical and astronomical constants is needed to convert $\mathcal{D}$ to DM.
In the early days of pulsar astronomy the relevant constants were defined to parts per million (ppm).
As a result, a convention arose in which this conversion factor was fixed. Over time, several such
conventions came about -- recipe for confusion. Meanwhile, over the past several years, the SI system
has been restructured and the parsec is now exactly defined. As a result, the present accuracy of
the conversion factor is below a part per billion -- many orders of magnitude better than the best
measurement errors of $\mathcal{D}$. We are now in an awkward situation wherein the primary "observable",
the DM, has incorrect scaling factor(s). To address these two concerns I propose that astronomers
report the primary measurement, $\mathcal{D}$ (with a suggested normalization of $10^{15}\,$Hz),
and not the DM. Interested users can convert $\mathcal{D}$ to DM without the need to know secret handshakes
of the pulsar timing communities. 