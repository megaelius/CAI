New and forthcoming deep-wide surveys, from instruments like the HSC, LSST and EUCLID, are poised
to revolutionize our understanding of galaxy evolution, by revealing aspects of galaxies that
are largely invisible in past wide-area datasets. These surveys will open up the realm of low-surface-brightness
(LSB) and dwarf galaxies -- which dominate the galaxy number density -- for the first time at cosmological
distances. They will also reveal key, unexplored LSB structures which strongly constrain our structure-formation
paradigm, such as merger-induced tidal features and intra-cluster light. However, exploitation
of these revolutionary new datasets will require us to address several data-analysis challenges.
Data-processing pipelines will have to preserve LSB structures, which are susceptible to sky over-subtraction.
Analysis of the prodigious data volumes will require machine-learning (in particular unsupervised
techniques), to augment or even replace traditional methods. Cosmological simulations, which
are essential for a statistical understanding of the physics of galaxy evolution, will require
mass and spatial resolutions that are high enough to resolve LSB/dwarf galaxies and LSB structures.
And finally, estimation of physical properties (e.g. stellar masses and star formation rates)
will require reliable redshift information. Since it is unlikely that even next-generation spectrographs
will provide complete spectral coverage in the LSB/dwarf regime outside the nearby Universe, photometric
redshifts may drive the science from these datasets. It is necessary, therefore, that the accuracy
of these redshifts is good enough (e.g. < 10 per cent) to enable statistical studies in the LSB/dwarf
regime. I outline the tremendous discovery potential of new/forthcoming deep-wide surveys and
describe techniques which will enable us to solve the data-analysis challenges outlined above.
(Abridged) 