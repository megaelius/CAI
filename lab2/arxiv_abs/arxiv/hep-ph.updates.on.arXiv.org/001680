The complexity of collider data analyses has dramatically increased from early colliders to the
CERN LHC. Reconstruction of the collision products in the particle detectors has reached a point
that requires dedicated publications documenting the techniques, and periodic retuning of the
algorithms themselves. Analysis methods evolved to account for the increased complexity of the
combination of particles required in each collision event (final states) and for the need of squeezing
every last bit of sensitivity from the data; physicists often seek to fully reconstruct the final
state, a process that is mostly relatively easy at lepton colliders but sometimes exceedingly difficult
at hadron colliders to the point of requiring sometimes using advanced statistical techniques
such as machine learning. The need for keeping the publications documenting results to a reasonable
size implies a greater level of compression or even omission of information with respect to publications
from twenty years ago. The need for compression should however not prevent sharing a reasonable
amount of information that is essential to understanding a given analysis. Infrastructures like
Rivet or HepData have been developed to host additional material, but physicists in the experimental
Collaborations often still send an insufficient amount of material to these databases. In this
manuscript I advocate for an increase in the information shared by the Collaborations, and try to
define a minimum standard for acceptable level of information when reporting the results of statistical
procedures in High Energy Physics publications. 