It is interesting to observe that all optical materials with a positive refractive index have a value
of index that is of order unity. Surprisingly, though, a deep understanding of the mechanisms that
lead to this universal behavior seems to be lacking. Moreover, this observation is difficult to
reconcile with the fact that a single, isolated atom is known to have a giant optical response, as
characterized by a resonant scattering cross section that far exceeds its physical size. Here,
we theoretically and numerically investigate the evolution of the optical properties of an ensemble
of ideal atoms as a function of density, starting from the dilute gas limit, including the effects
of multiple scattering and near-field interactions. Interestingly, despite the giant response
of an isolated atom, we find that the maximum index does not indefinitely grow with increasing density,
but rather reaches a limiting value $n\approx 1.7$. We propose an explanation based upon strong-disorder
renormalization group theory, in which the near-field interaction combined with random atomic
positions results in an inhomogeneous broadening of atomic resonance frequencies. This mechanism
ensures that regardless of the physical atomic density, light at any given frequency only interacts
with at most a few near-resonant atoms per cubic wavelength, thus limiting the maximum index attainable.
Our work is a promising first step to understand the limits of refractive index from a bottom-up,
atomic physics perspective, and also introduces renormalization group as a powerful tool to understand
the generally complex problem of multiple scattering of light overall. 