There have been multiple attempts to demonstrate that quantum annealing and, in particular, quantum
annealing on quantum annealing machines, has the potential to outperform current classical optimization
algorithms implemented on CMOS technologies. The benchmarking of these devices has been controversial.
Initially, random spin-glass problems were used, however, these were quickly shown to be not well
suited to detect any quantum speedup. Subsequently, benchmarking shifted to carefully crafted
synthetic problems designed to highlight the quantum nature of the hardware while (often) ensuring
that classical optimization techniques do not perform well on them. Even worse, to date a true sign
of improved scaling with the number problem variables remains elusive when compared to classical
optimization techniques. Here, we analyze the readiness of quantum annealing machines for real-world
application problems. These are typically not random and have an underlying structure that is hard
to capture in synthetic benchmarks, thus posing unexpected challenges for optimization techniques,
both classical and quantum alike. We present a comprehensive computational scaling analysis of
fault diagnosis in digital circuits, considering architectures beyond D-wave quantum annealers.
We find that the instances generated from real data in multiplier circuits are harder than other
representative random spin-glass benchmarks with a comparable number of variables. Although
our results show that transverse-field quantum annealing is outperformed by state-of-the-art
classical optimization algorithms, these benchmark instances are hard and small in the size of
the input, therefore representing the first industrial application ideally suited for near-term
quantum annealers. 