Growth in the complexity and capabilities of quantum information hardware mandates access to practical
techniques for performance verification that function under realistic laboratory conditions.
Here we experimentally characterise the impact of common temporally correlated noise processes
on both randomised benchmarking (RB) and gate-set tomography (GST). We study these using an analytic
toolkit based on a formalism mapping noise to errors for arbitrary sequences of unitary operations.
This analysis highlights the role of sequence structure in enhancing or suppressing the sensitivity
of quantum verification protocols to either slowly or rapidly varying noise, which we treat in the
limiting cases of quasi-DC miscalibration and white noise power spectra. We perform experiments
with a single trapped $^{171}$Yb$^{+}$ ion as a qubit and inject engineered noise ($\propto \sigma^z$)
to probe protocol performance. Experiments on RB validate predictions that the distribution of
measured fidelities over sequences is described by a gamma distribution varying between approximately
Gaussian for rapidly varying noise, and a broad, highly skewed distribution for the slowly varying
case. Similarly we find a strong gate set dependence of GST in the presence of correlated errors,
leading to significant deviations between estimated and calculated diamond distances in the presence
of correlated $\sigma^z$ errors. Numerical simulations demonstrate that expansion of the gate
set to include negative rotations can suppress these discrepancies and increase reported diamond
distances by orders of magnitude for the same error processes. Similar effects do not occur for correlated
$\sigma^x$ or $\sigma^y$ errors or rapidly varying noise processes, highlighting the critical
interplay of selected gate set and the gauge optimisation process on the meaning of the reported
diamond norm in correlated noise environments. 