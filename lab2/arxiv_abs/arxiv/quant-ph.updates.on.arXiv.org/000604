We introduce the Backwards Quantum Propagation of Phase errors (Baqprop) principle, a central
theme upon which we construct multiple universal optimization heuristics for training both parametrized
quantum circuits and classical deep neural networks on a quantum computer. Baqprop encodes error
information in relative phases of a quantum wavefunction defined over the space of network parameters;
it can be thought of as the unification of the phase kickback principle of quantum computation and
of the backpropagation algorithm from classical deep learning. We propose two core heuristics
which leverage Baqprop for quantum-enhanced optimization of network parameters: Quantum Dynamical
Descent (QDD) and Momentum Measurement Gradient Descent (MoMGrad). QDD uses simulated quantum
coherent dynamics for parameter optimization, allowing for quantum tunneling through the hypothesis
space landscape. MoMGrad leverages Baqprop to estimate gradients and thereby perform gradient
descent on the parameter landscape; it can be thought of as the quantum-classical analogue of QDD.
In addition to these core optimization strategies, we propose various methods for parallelization,
regularization, and meta-learning as augmentations to MoMGrad and QDD. We introduce several quantum-coherent
adaptations of canonical classical feedforward neural networks, and study how Baqprop can be used
to optimize such networks. We develop multiple applications of parametric circuit learning for
quantum data, and show how to perform Baqprop in each case. One such application allows for the training
of hybrid quantum-classical neural-circuit networks, via the seamless integration of Baqprop
with classical backpropagation. Finally, for a representative subset of these proposed applications,
we demonstrate the training of these networks via numerical simulations of implementations of
QDD and MoMGrad. 