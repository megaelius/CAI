In an idealistic setting, quantum metrology protocols allow to sense physical parameters with
mean squared error that scales as $1/N^2$ with the number of particles involved - substantially
surpassing the $1/N$-scaling characteristic to classical statistics. A natural question arises,
whether such an impressive enhancement persists when one takes into account the decoherence effects
that are unavoidable in any real-life implementation. In this thesis, we resolve a major part of
this issue by describing general techniques that allow to quantify the attainable precision in
metrological schemes in the presence of uncorrelated noise. We show that the abstract geometrical
structure of a quantum channel describing the noisy evolution of a single particle dictates then
critical bounds on the ultimate quantum enhancement. Our results prove that an infinitesimal amount
of noise is enough to restrict the precision to scale classically in the asymptotic $N$ limit, and
thus constrain the maximal improvement to a constant factor. Although for low numbers of particles
the decoherence may be ignored, for large $N$ the presence of noise heavily alters the form of both
optimal states and measurements attaining the ultimate resolution. However, the established
bounds are then typically achievable with use of techniques natural to current experiments. In
this work, we thoroughly introduce the necessary concepts and mathematical tools lying behind
metrological tasks, including both frequentist and Bayesian estimation theory frameworks. We
provide examples of applications of the methods presented to typical qubit noise models, yet we
also discuss in detail the phase estimation tasks in Mach-Zehnder interferometry both in the classical
and quantum setting - with particular emphasis given to photonic losses while analysing the impact
of decoherence. 