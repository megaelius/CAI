The problem of learning Boolean linear functions from quantum examples w.r.t. the uniform distribution
can be solved on a quantum computer using the Bernstein-Vazirani algorithm. A similar strategy
can be applied in the case of noisy quantum training data, as was observed in arXiv:1702.08255v2
[quant-ph]. However, extensions of these learning algorithms beyond the uniform distribution
have not yet been studied. We employ the biased quantum Fourier transform introduced in arXiv:1802.05690v2
[quant-ph] to develop efficient quantum algorithms for learning Boolean linear functions on $n$
bits from quantum examples w.r.t. a biased product distribution. Our first procedure is applicable
to any (except full) bias and requires $\mathcal{O}(\ln (n))$ quantum examples. The number of quantum
examples used by our second algorithm is independent of $n$, but the strategy is applicable only
for small bias. Moreover, we show that the second procedure is stable w.r.t. noisy training data
and w.r.t. faulty quantum gates. This also enables us to solve a version of the learning problem in
which the underlying distribution is not known in advance. Finally, we prove lower bounds on the
classical and quantum sample complexities of the learning problem. Whereas classically, $\Omega
(n)$ examples are necessary independently of the bias, we are able to establish a quantum sample
complexity lower bound of $\Omega (\ln (n))$ only under an assumption of large bias. Nevertheless,
this allows for a discussion of the performance of our suggested learning algorithms w.r.t. sample
complexity. With our analysis we contribute to a more quantitative understanding of the power and
limitations of quantum training data for learning classical functions. 